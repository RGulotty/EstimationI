{
  "hash": "2ebeb3fb38f4ea8fd0a9d49515833f85",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"5. Efficiency and GLS\"\nsubtitle: \"Weighted least squares, feasible GLS, and the method of moments\"\n---\n\nWhen error variances differ across observations, OLS is still unbiased but no longer efficient. This chapter develops WLS and GLS as the natural response: weight observations by their precision. We build everything from matrix algebra, connect the estimator to the method of moments, and implement feasible GLS when the variance structure must be estimated from data. Along the way we introduce the `sandwich` and `estimatr` packages — the practical tools for robust inference in R.\n\n**Questions this chapter answers:**\n\n1. Why are classical standard errors wrong under heteroskedasticity, and what does the sandwich formula fix?\n2. How do WLS and GLS improve efficiency by weighting observations by their precision?\n3. When should you use FGLS vs. simply reporting robust standard errors?\n4. What is the connection between HC0/HC1/HC2 and the leverage-corrected residual types?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(MASS)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(estimatr)\nlibrary(carData)\ndata(Prestige)\noptions(digits = 3)\ntr <- function(M) sum(diag(M))\n```\n:::\n\n\n## The variance of OLS under non-spherical errors {#sec-sandwich}\n\nRecall the [OLS estimator](ch03-ols.qmd#eq-ols): $\\hat{\\beta} = \\beta + (X'X)^{-1}X'e$. The variance is:\n\n$$\\text{Var}[\\hat{\\beta} \\mid X] = (X'X)^{-1} X' \\text{Var}[e \\mid X] \\, X (X'X)^{-1}$$\n\nUnder **homoskedasticity** ($\\text{Var}[e \\mid X] = \\sigma^2 I$), this simplifies to $\\sigma^2(X'X)^{-1}$. But when $\\text{Var}[e \\mid X] = \\Omega \\neq \\sigma^2 I$, we get the **sandwich formula**:\n\n$$(X'X)^{-1} (X' \\Omega X) (X'X)^{-1}$$ {#eq-sandwich}\n\nThe \"bread\" is $(X'X)^{-1}$ and the \"meat\" is $X'\\Omega X = \\sum_{i=1}^n \\sigma_i^2 x_i x_i'$. Let's see what happens when we ignore heteroskedasticity and use the classical formula anyway.\n\n::: {#def-sandwich}\n## Sandwich Variance Estimator\nUnder heteroskedasticity ($\\text{Var}[e|X] = \\Omega \\neq \\sigma^2 I$), the variance of OLS is $(X'X)^{-1}(X'\\Omega X)(X'X)^{-1}$. The HC estimators replace $\\Omega$ with diagonal matrices of squared residuals, possibly adjusted for leverage.\n:::\n\n### Simulation: when classical standard errors lie\n\nWe design a DGP with strong heteroskedasticity: the error standard deviation grows as $x^2$, so variance ranges from 1 (at $x = 1$) to 10,000 (at $x = 10$). This makes the problem impossible to miss.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nn <- 200\nx <- runif(n, 1, 10)\nX <- cbind(1, x)\n\n# Strongly heteroskedastic DGP: SD = x^2\nsigma_i <- x^2   # variance = x^4, ratio of 10000:1\ny <- 2 + 3 * x + rnorm(n, 0, sigma_i)\nmod <- lm(y ~ x)\n```\n:::\n\n\nFirst, let's build the sandwich by hand to see the matrix algebra:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Classical variance: s^2 * (X'X)^{-1}\ns2 <- sum(resid(mod)^2) / (n - 2)\nV_classical <- s2 * solve(crossprod(X))\n\n# Sandwich variance (HC0): (X'X)^{-1} X' diag(e^2) X (X'X)^{-1}\ne_hat <- resid(mod)\nbread <- solve(crossprod(X))\nmeat <- t(X) %*% diag(e_hat^2) %*% X\nV_HC0 <- bread %*% meat %*% bread\n\n# Compare standard errors for the slope\nc(classical = sqrt(V_classical[2, 2]),\n  HC0       = sqrt(V_HC0[2, 2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassical       HC0 \n     1.31      1.48 \n```\n\n\n:::\n:::\n\n\nNow the practical way — `sandwich::vcovHC()` computes this in one line:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# HC0 (White's original)\nsqrt(diag(vcovHC(mod, type = \"HC0\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)           x \n       5.48        1.48 \n```\n\n\n:::\n\n```{.r .cell-code}\n# HC1 (small-sample correction: multiply by n/(n-k))\nsqrt(diag(vcovHC(mod, type = \"HC1\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)           x \n       5.51        1.49 \n```\n\n\n:::\n\n```{.r .cell-code}\n# HC2 (recommended default — adjusts for leverage)\nsqrt(diag(vcovHC(mod, type = \"HC2\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)           x \n       5.52        1.49 \n```\n\n\n:::\n:::\n\n\nOr even simpler — `estimatr::lm_robust()` fits the model and computes robust SEs in one step:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_robust <- lm_robust(y ~ x, se_type = \"HC2\")\nsummary(mod_robust)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm_robust(formula = y ~ x, se_type = \"HC2\")\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|) CI Lower CI Upper  DF\n(Intercept)     4.07       5.52   0.737    0.462   -6.816    14.95 198\nx               2.30       1.49   1.540    0.125   -0.644     5.24 198\n\nMultiple R-squared:  0.0154 ,\tAdjusted R-squared:  0.0104 \nF-statistic: 2.37 on 1 and 198 DF,  p-value: 0.125\n```\n\n\n:::\n:::\n\n\nCompare the standard errors side by side:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nse_table <- data.frame(\n  Classical = summary(mod)$coefficients[, 2],\n  HC0 = sqrt(diag(vcovHC(mod, type = \"HC0\"))),\n  HC1 = sqrt(diag(vcovHC(mod, type = \"HC1\"))),\n  HC2 = sqrt(diag(vcovHC(mod, type = \"HC2\"))),\n  row.names = c(\"(Intercept)\", \"x\")\n)\nround(se_table, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Classical  HC0  HC1  HC2\n(Intercept)      8.20 5.48 5.51 5.52\nx                1.31 1.48 1.49 1.49\n```\n\n\n:::\n:::\n\n\nThe classical SE for the slope is far too small — it ignores that the high-$x$ observations (which pull the slope) are exactly the noisiest ones.\n\n::: {.callout-warning}\n## Classical Standard Errors Can Be Dangerously Wrong\nUnder heteroskedasticity, classical SEs can be too small by a factor of 2 or more, producing confidence intervals with far below nominal coverage. Always use robust SEs (HC2) as the default for cross-sectional data.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame(x = x, residual = resid(mod))\nggplot(df, aes(x, residual)) +\n  geom_point(alpha = 0.4) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Residuals fan out dramatically with x\",\n       subtitle = \"SD grows as x², so variance ratio is 10000:1\")\n```\n\n::: {.cell-output-display}\n![](ch05-gls_files/figure-html/heteroskedasticity-visual-1.png){width=672}\n:::\n:::\n\n\n### Monte Carlo: coverage of classical vs. robust intervals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nB <- 2000\ncover_classical <- cover_HC0 <- cover_HC2 <- logical(B)\nbeta_true <- 3\n\nfor (b in 1:B) {\n  x_sim <- runif(n, 1, 10)\n  X_sim <- cbind(1, x_sim)\n  sigma_sim <- x_sim^2\n  y_sim <- 2 + beta_true * x_sim + rnorm(n, 0, sigma_sim)\n\n  fit <- lm(y_sim ~ x_sim)\n  b_hat <- coef(fit)[2]\n\n  # Classical CI\n  se_class <- summary(fit)$coefficients[2, 2]\n  ci_class <- b_hat + c(-1, 1) * 1.96 * se_class\n  cover_classical[b] <- ci_class[1] < beta_true & beta_true < ci_class[2]\n\n  # HC0 (White)\n  se_hc0 <- sqrt(vcovHC(fit, type = \"HC0\")[2, 2])\n  ci_hc0 <- b_hat + c(-1, 1) * 1.96 * se_hc0\n  cover_HC0[b] <- ci_hc0[1] < beta_true & beta_true < ci_hc0[2]\n\n  # HC2 (recommended)\n  se_hc2 <- sqrt(vcovHC(fit, type = \"HC2\")[2, 2])\n  ci_hc2 <- b_hat + c(-1, 1) * 1.96 * se_hc2\n  cover_HC2[b] <- ci_hc2[1] < beta_true & beta_true < ci_hc2[2]\n}\n\nc(classical = mean(cover_classical),\n  HC0       = mean(cover_HC0),\n  HC2       = mean(cover_HC2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassical       HC0       HC2 \n    0.894     0.953     0.954 \n```\n\n\n:::\n:::\n\n\nClassical intervals have terrible coverage. HC0 does better. HC2 gets closest to the nominal 95% because it corrects for leverage — observations with high $x$ values both have high variance *and* high leverage. (Chapter 6 develops the HC variants in detail.)\n\n### `lm_robust` vs. `coeftest`: two workflows\n\nIn practice, there are two ways to get robust inference. Use whichever fits your workflow:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Workflow 1: estimatr — one function does everything\nmod_r <- lm_robust(y ~ x, se_type = \"HC2\")\ncoef(summary(mod_r))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Estimate Std. Error t value Pr(>|t|) CI Lower CI Upper  DF\n(Intercept)     4.07       5.52   0.737    0.462   -6.816    14.95 198\nx               2.30       1.49   1.540    0.125   -0.644     5.24 198\n```\n\n\n:::\n\n```{.r .cell-code}\n# Workflow 2: sandwich + lmtest — post-hoc correction to a fitted lm\ncoeftest(mod, vcov = vcovHC(mod, type = \"HC2\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)     4.07       5.52    0.74     0.46\nx               2.30       1.49    1.54     0.13\n```\n\n\n:::\n:::\n\n\nThe `coeftest()` approach is useful when you've already fit a model with `lm()` and want to report robust SEs. `lm_robust()` is cleaner when you know from the start that you want robust inference.\n\n## Weighted least squares\n\nThe sandwich formula tells us what the variance *is*. But can we do better than OLS? Yes — if we know (or can estimate) the variance structure, we should exploit it.\n\n### The idea: weight by precision\n\nIf observation $i$ has variance $\\sigma_i^2$, it carries less information than an observation with variance $\\sigma_j^2 < \\sigma_i^2$. WLS weights each observation by $w_i = 1/\\sigma_i^2$, downweighting noisy observations:\n\n$$\\hat{\\beta}_{WLS} = \\arg\\min_\\beta \\sum_{i=1}^n w_i (y_i - x_i'\\beta)^2 = (X'WX)^{-1} X'Wy$$ {#eq-wls}\n\nwhere $W = \\text{diag}(w_1, \\ldots, w_n)$.\n\n### Two-group example\n\nSuppose we survey two groups: Group A ($n_A = 100$, $\\sigma_A = 10$) and Group B ($n_B = 100$, $\\sigma_B = 1$). OLS gives equal weight to every observation. WLS gives Group A weight $1/100$ and Group B weight $1$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(99)\nB <- 5000\nb_ols <- b_wls <- numeric(B)\n\nfor (i in 1:B) {\n  x_sim <- rnorm(200)\n  sigma_sim <- c(rep(10, 100), rep(1, 100))\n  y_sim <- 1 + 2 * x_sim + rnorm(200, 0, sigma_sim)\n\n  b_ols[i] <- coef(lm(y_sim ~ x_sim))[2]\n  b_wls[i] <- coef(lm(y_sim ~ x_sim, weights = 1 / sigma_sim^2))[2]\n}\n\n# Both unbiased, but WLS has much lower variance\nc(bias_ols = mean(b_ols) - 2, bias_wls = mean(b_wls) - 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nbias_ols bias_wls \n-0.00434 -0.00125 \n```\n\n\n:::\n\n```{.r .cell-code}\nc(sd_ols = sd(b_ols), sd_wls = sd(b_wls))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nsd_ols sd_wls \n 0.507  0.103 \n```\n\n\n:::\n:::\n\n\nBoth estimators are unbiased, but WLS standard errors are dramatically smaller. GLS is just common sense: trust precise observations more.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_sim <- data.frame(\n  estimate = c(b_ols, b_wls),\n  method = rep(c(\"OLS\", \"WLS\"), each = B)\n)\nggplot(df_sim, aes(estimate, fill = method)) +\n  geom_density(alpha = 0.4) +\n  geom_vline(xintercept = 2, linetype = \"dashed\") +\n  labs(title = \"OLS vs. WLS sampling distributions\",\n       subtitle = \"Both centered on truth, but WLS is much tighter\",\n       x = expression(hat(beta)))\n```\n\n::: {.cell-output-display}\n![OLS vs. WLS sampling distributions: both unbiased, but WLS is tighter](ch05-gls_files/figure-html/two-group-density-1.png){width=672}\n:::\n:::\n\n\n### WLS in R: `lm(..., weights = )`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_ols <- lm(prestige ~ education + income + women, data = Prestige)\n\n# Suppose we know variance is proportional to income\n# (higher-income occupations have more variable prestige)\nw <- 1 / Prestige$income\nmod_wls <- lm(prestige ~ education + income + women, data = Prestige, weights = w)\n\n# Compare coefficients\ncbind(OLS = coef(mod_ols), WLS = coef(mod_wls))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 OLS      WLS\n(Intercept) -6.79433 -9.97061\neducation    4.18664  3.18162\nincome       0.00131  0.00303\nwomen       -0.00891  0.07049\n```\n\n\n:::\n:::\n\n\n### WLS as a transformed regression\n\nWLS is equivalent to pre-multiplying the model by $W^{1/2}$:\n\n$$W^{1/2} y = W^{1/2} X \\beta + W^{1/2} e$$\n\nThe transformed errors have variance $W^{1/2} \\Omega W^{1/2} = I$ (if $W = \\Omega^{-1}$), so OLS on the transformed data is efficient. Let's verify:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Manual transformation\nW_half <- diag(sqrt(w))\ny_tilde <- W_half %*% Prestige$prestige\nX_raw <- cbind(1, Prestige$education, Prestige$income, Prestige$women)\nX_tilde <- W_half %*% X_raw\n\n# OLS on transformed data\nbeta_transformed <- solve(crossprod(X_tilde)) %*% crossprod(X_tilde, y_tilde)\n\n# Compare to lm(..., weights = )\nall.equal(as.numeric(beta_transformed), as.numeric(coef(mod_wls)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nThe transformation approach makes clear what `weights` does: it rescales each observation so that the transformed errors are homoskedastic.\n\n**Important note on the intercept.** After transformation, the column of ones becomes $W^{1/2} \\mathbf{1} = (\\sqrt{w_1}, \\ldots, \\sqrt{w_n})'$, which is no longer constant. If you run the transformed regression manually, you must suppress the automatic intercept and include the transformed constant as a regressor:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Transformed data\ndf_t <- data.frame(\n  y = as.numeric(y_tilde),\n  const = as.numeric(W_half %*% rep(1, nrow(Prestige))),\n  education = as.numeric(W_half %*% Prestige$education),\n  income = as.numeric(W_half %*% Prestige$income),\n  women = as.numeric(W_half %*% Prestige$women)\n)\n\n# -1 suppresses R's intercept; 'const' is the transformed intercept\nmod_manual <- lm(y ~ const + education + income + women - 1, data = df_t)\nall.equal(as.numeric(coef(mod_manual)), as.numeric(coef(mod_wls)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n## GLS: The general transformation {#sec-gls}\n\nWLS handles the case where $\\Omega$ is diagonal (heteroskedasticity only). GLS handles the general case where errors may also be correlated. The key idea is the same: find a transformation $\\Omega^{-1/2}$ that sphericizes the errors.\n\n### Eigendecomposition of $\\Omega$\n\nAny positive definite symmetric matrix can be factored as $\\Omega = C \\Lambda C'$, where $C$ is the matrix of eigenvectors and $\\Lambda$ is diagonal with eigenvalues. Then:\n\n$$\\Omega^{-1/2} = C \\Lambda^{-1/2} C'$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A small example: 4x4 covariance matrix with correlation\nn_small <- 4\nrho <- 0.6\nOmega_small <- rho^abs(outer(1:n_small, 1:n_small, \"-\"))  # AR(1) structure\nOmega_small\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1] [,2] [,3]  [,4]\n[1,] 1.000 0.60 0.36 0.216\n[2,] 0.600 1.00 0.60 0.360\n[3,] 0.360 0.60 1.00 0.600\n[4,] 0.216 0.36 0.60 1.000\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\neig <- eigen(Omega_small)\nC <- eig$vectors\nLambda <- diag(eig$values)\n\n# Omega^{-1/2}\nOmega_inv_half <- C %*% diag(1 / sqrt(eig$values)) %*% t(C)\n\n# Verify: Omega^{-1/2} Omega Omega^{-1/2} = I\nall.equal(Omega_inv_half %*% Omega_small %*% Omega_inv_half, diag(n_small),\n          check.attributes = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n### GLS formula\n\nThe GLS estimator pre-multiplies by $\\Omega^{-1/2}$, then applies OLS:\n\n$$\\hat{\\beta}_{GLS} = (X'\\Omega^{-1}X)^{-1} X'\\Omega^{-1}y$$ {#eq-gls}\n\nIts variance is $\\sigma^2(X'\\Omega^{-1}X)^{-1}$, which is the efficiency lower bound — no other linear unbiased estimator can do better.\n\n::: {#thm-gls}\n## GLS Estimator\nThe GLS estimator $\\hat\\beta_{GLS} = (X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}y$ is the best linear unbiased estimator (BLUE) when $\\text{Var}[e|X] = \\sigma^2\\Omega$. Its variance $\\sigma^2(X'\\Omega^{-1}X)^{-1}$ achieves the efficiency lower bound among linear unbiased estimators.\n:::\n\n::: {#thm-gauss-markov}\n## Gauss-Markov Theorem\nUnder the assumptions $\\mathbb{E}[e|X] = 0$ and $\\text{Var}[e|X] = \\sigma^2 I$ (homoskedasticity), OLS is BLUE — the Best Linear Unbiased Estimator. No other linear unbiased estimator has smaller variance. When homoskedasticity fails, GLS replaces OLS as BLUE.\n:::\n\n### Simulation: GLS with correlated errors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(7)\nn <- 80\nrho <- 0.8\n\n# AR(1) correlation matrix\nOmega <- rho^abs(outer(1:n, 1:n, \"-\"))\nOmega_inv <- solve(Omega)\n\n# Cholesky factor for generating correlated errors\nL <- t(chol(Omega))\n\nx <- sort(runif(n, 0, 10))\nX <- cbind(1, x)\nbeta_true <- c(1, 2)\n\nB <- 3000\nb_ols <- b_gls <- matrix(NA, B, 2)\n\nfor (b in 1:B) {\n  e <- L %*% rnorm(n)  # correlated errors\n  y_sim <- X %*% beta_true + e\n\n  b_ols[b, ] <- as.numeric(solve(crossprod(X)) %*% crossprod(X, y_sim))\n  b_gls[b, ] <- as.numeric(solve(t(X) %*% Omega_inv %*% X) %*%\n                              (t(X) %*% Omega_inv %*% y_sim))\n}\n\n# Both unbiased\ncolMeans(b_ols) - beta_true\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  0.01416 -0.00327\n```\n\n\n:::\n\n```{.r .cell-code}\ncolMeans(b_gls) - beta_true\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  0.01215 -0.00282\n```\n\n\n:::\n\n```{.r .cell-code}\n# But GLS is more efficient (lower SD for slope)\nc(sd_ols_slope = sd(b_ols[, 2]), sd_gls_slope = sd(b_gls[, 2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nsd_ols_slope sd_gls_slope \n       0.109        0.101 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_ar <- data.frame(\n  slope = c(b_ols[, 2], b_gls[, 2]),\n  method = rep(c(\"OLS\", \"GLS\"), each = B)\n)\nggplot(df_ar, aes(slope, fill = method)) +\n  geom_density(alpha = 0.4) +\n  geom_vline(xintercept = 2, linetype = \"dashed\") +\n  labs(title = \"OLS vs. GLS with AR(1) errors (ρ = 0.8)\",\n       subtitle = \"GLS recovers the efficiency lost to serial correlation\",\n       x = expression(hat(beta)[1]))\n```\n\n::: {.cell-output-display}\n![](ch05-gls_files/figure-html/gls-ar1-density-1.png){width=672}\n:::\n:::\n\n\n### The GLS projection matrix\n\nIn Chapter 3, we studied $P = X(X'X)^{-1}X'$. The GLS analog replaces the inner product:\n\n$$P_{GLS} = X(X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}$$\n\nUnlike the OLS projection, $P_{GLS}$ is **not symmetric** — but it is still idempotent:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nP_gls <- X %*% solve(t(X) %*% Omega_inv %*% X) %*% t(X) %*% Omega_inv\n\n# Not symmetric\nmax(abs(P_gls - t(P_gls)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.272\n```\n\n\n:::\n\n```{.r .cell-code}\n# But idempotent\nall.equal(P_gls %*% P_gls, P_gls)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n## Feasible GLS\n\nGLS requires knowing $\\Omega$. In practice, we never know the true variance structure. **Feasible GLS** (FGLS) estimates $\\Omega$ from the data in a first step, then applies GLS with $\\hat{\\Omega}$.\n\n### Step-by-step FGLS for heteroskedasticity\n\nThe most common approach assumes a **multiplicative heteroskedasticity** model:\n\n$$\\sigma_i^2 = \\text{Var}[e_i \\mid x_i] = \\exp(\\gamma_0 + \\gamma_1 z_i)$$\n\nwhere $z_i$ is some function of $x_i$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nn <- 300\n\n# DGP: variance depends strongly on x\nx <- runif(n, 1, 10)\nsigma_true <- exp(0.5 + 0.3 * x)  # log-linear variance\ny <- 2 + 3 * x + rnorm(n) * sqrt(sigma_true)\n\n# Step 1: Run OLS, save residuals\nmod_step1 <- lm(y ~ x)\n\n# Step 2: Regress log(e^2) on x to estimate the variance function\nlog_e2 <- log(resid(mod_step1)^2)\nmod_var <- lm(log_e2 ~ x)\n\n# Step 3: Predicted variances -> weights\nsigma2_hat <- exp(fitted(mod_var))\nw_fgls <- 1 / sigma2_hat\n\n# Step 4: Run WLS with estimated weights\nmod_fgls <- lm(y ~ x, weights = w_fgls)\n\ncbind(OLS = coef(mod_step1), FGLS = coef(mod_fgls), Truth = c(2, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             OLS FGLS Truth\n(Intercept) 1.85 2.03     2\nx           3.02 2.98     3\n```\n\n\n:::\n:::\n\n\nCompare standard errors — OLS classical, OLS with HC2, and FGLS:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX_fgls <- cbind(1, x)\n\nse_compare <- data.frame(\n  OLS_classical = summary(mod_step1)$coefficients[, 2],\n  OLS_HC2       = sqrt(diag(vcovHC(mod_step1, type = \"HC2\"))),\n  FGLS          = summary(mod_fgls)$coefficients[, 2],\n  row.names = c(\"(Intercept)\", \"x\")\n)\nround(se_compare, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            OLS_classical OLS_HC2  FGLS\n(Intercept)        0.4559  0.3641 0.255\nx                  0.0753  0.0886 0.062\n```\n\n\n:::\n:::\n\n\nHC2 corrects the SE without changing the point estimate. FGLS changes *both* — it re-estimates $\\hat{\\beta}$ using the variance information, gaining efficiency.\n\n### Implementing FGLS with matrix algebra\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX_mat <- cbind(1, x)\nOmega_hat_inv <- diag(1 / sigma2_hat)\n\n# GLS formula with estimated Omega\nbeta_fgls <- solve(t(X_mat) %*% Omega_hat_inv %*% X_mat) %*%\n             (t(X_mat) %*% Omega_hat_inv %*% y)\n\nall.equal(as.numeric(beta_fgls), as.numeric(coef(mod_fgls)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n### How well does FGLS estimate the variance function?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_var <- data.frame(\n  x = x,\n  log_e2 = log_e2,\n  fitted_log_var = fitted(mod_var),\n  true_log_var = 0.5 + 0.3 * x\n)\n\nggplot(df_var, aes(x)) +\n  geom_point(aes(y = log_e2), alpha = 0.2, size = 1) +\n  geom_line(aes(y = fitted_log_var, color = \"Estimated\"), linewidth = 1) +\n  geom_line(aes(y = true_log_var, color = \"True\"), linewidth = 1, linetype = \"dashed\") +\n  labs(title = \"FGLS variance function estimation\",\n       subtitle = \"Regressing log(ê²) on x recovers the variance structure\",\n       y = \"log(σ²)\", color = NULL)\n```\n\n::: {.cell-output-display}\n![](ch05-gls_files/figure-html/fgls-variance-fit-1.png){width=672}\n:::\n:::\n\n\n### Monte Carlo: OLS vs. FGLS efficiency\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\nB <- 2000\nb_ols_mc <- b_fgls_mc <- numeric(B)\n\nfor (b in 1:B) {\n  x_mc <- runif(n, 1, 10)\n  sigma_mc <- exp(0.5 + 0.3 * x_mc)\n  y_mc <- 2 + 3 * x_mc + rnorm(n) * sqrt(sigma_mc)\n\n  # OLS\n  fit_ols <- lm(y_mc ~ x_mc)\n  b_ols_mc[b] <- coef(fit_ols)[2]\n\n  # FGLS\n  log_e2_mc <- log(resid(fit_ols)^2)\n  sigma2_hat_mc <- exp(fitted(lm(log_e2_mc ~ x_mc)))\n  b_fgls_mc[b] <- coef(lm(y_mc ~ x_mc, weights = 1 / sigma2_hat_mc))[2]\n}\n\nc(sd_ols = sd(b_ols_mc), sd_fgls = sd(b_fgls_mc),\n  efficiency_gain = sd(b_ols_mc) / sd(b_fgls_mc))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         sd_ols         sd_fgls efficiency_gain \n         0.0841          0.0687          1.2255 \n```\n\n\n:::\n:::\n\n\nEven though FGLS must *estimate* the weights, it still substantially improves on OLS when the heteroskedasticity is strong.\n\n::: {.callout-note}\n## FGLS Requires a Correct Variance Model\nFGLS gains efficiency over OLS + robust SEs only when the variance model is correctly specified. If $\\hat\\Omega$ is misspecified, FGLS point estimates are still consistent but may be less efficient than OLS, and its reported SEs may be wrong. Use FGLS only when you have a substantive reason to model the variance.\n:::\n\n## Two strategies for heteroskedasticity\n\nYou have two options when errors may be heteroskedastic:\n\n1. **Robust SEs (agnostic).** Keep the OLS point estimates and correct only the standard errors with the sandwich formula. Requires no model for the variance — always valid.\n2. **WLS/FGLS (model-based).** Specify a model for the variance, estimate it, and re-weight. More efficient *if* the variance model is correct; potentially worse if it's misspecified.\n\nA common older workflow was to first *test* for heteroskedasticity (Breusch-Pagan, White's test), then decide whether to apply WLS. This is **pre-testing** — using the same data to choose the estimator and then to estimate — and it distorts the sampling distribution of the final estimate. The resulting \"test, then decide\" procedure is neither the OLS distribution nor the WLS distribution; its true coverage and size are hard to characterize.\n\nThe modern recommendation: **always report robust SEs** (HC2 by default for cross-sectional data). Use WLS/FGLS only when you have a *substantive* reason to model the variance — for instance, when observations are group averages with known group sizes, or when a theoretical model predicts the variance form. The decision to use WLS should come from domain knowledge, not from a hypothesis test on the same data.\n\n### The Breusch-Pagan test (for understanding, not for pre-testing)\n\nThe Breusch-Pagan test is still useful as a *descriptive* diagnostic — it tells you whether your residuals exhibit systematic patterns in spread. The mechanics are simple: regress $\\hat{e}^2 / \\bar{\\hat{e}^2}$ on $X$ and check whether the $R^2$ is significantly different from zero.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_diag <- lm(y ~ x)\n\n# Breusch-Pagan by hand\ne2 <- resid(mod_diag)^2\np <- e2 / mean(e2)  # normalize by average squared residual\naux <- lm(p ~ x)\n\n# Test statistic: explained sum of squares / 2\nbp_stat <- sum((fitted(aux) - mean(p))^2) / 2\nbp_pval <- 1 - pchisq(bp_stat, df = 1)\nc(BP_statistic = bp_stat, p_value = bp_pval)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBP_statistic      p_value \n         111            0 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Same thing via lmtest (studentized version is robust to non-normal errors)\nbptest(mod_diag)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tstudentized Breusch-Pagan test\n\ndata:  mod_diag\nBP = 66, df = 1, p-value = 4e-16\n```\n\n\n:::\n:::\n\n\nA large test statistic tells you heteroskedasticity is present, which is useful information for understanding your data. But the right response is to always use robust SEs — not to condition your estimator on the test result.\n\n## Residual types\n\nOLS residuals are $\\hat{e} = My$, but they are **not** equal to the true errors. The lecture develops three residual types using projection matrices. Each applies a different diagonal scaling matrix $M^*$ to correct for leverage.\n\n### The matrices\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX_d <- cbind(1, x)\nK <- ncol(X_d)\nP_d <- X_d %*% solve(crossprod(X_d)) %*% t(X_d)\nM_d <- diag(n) - P_d\nh <- diag(P_d)  # leverage values\n\n# M* = diag{(1 - h_ii)^{-1}} — inflates by leverage\nM_star <- diag(1 / (1 - h))\n\n# (M*)^{1/2} = diag{(1 - h_ii)^{-1/2}} — square root scaling\nM_star_half <- diag(1 / sqrt(1 - h))\n```\n:::\n\n\nThe key relationship: $\\text{Var}[\\hat{e} \\mid X] = M \\, \\text{Var}[e \\mid X] \\, M$. Under homoskedasticity this gives $\\text{Var}[\\hat{e}_i \\mid X] = (1 - h_{ii})\\sigma^2$, so residuals are heteroskedastic even when the errors are not. The diagonal of $M$ shows the uneven scaling:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(1 - h)  # ranges from near 0 (high leverage) to near 1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.987   0.991   0.994   0.993   0.996   0.997 \n```\n\n\n:::\n:::\n\n\n### Raw residuals: $\\hat{e} = Me$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ne_raw <- as.numeric(M_d %*% y)\n\n# Same as resid()\nall.equal(e_raw, as.numeric(resid(mod_diag)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n### Prediction errors: $\\tilde{e} = M^* \\hat{e} = M^* M e$\n\nThe prediction error $\\tilde{e}_i = \\hat{e}_i / (1 - h_{ii})$ is the leave-one-out residual from Chapter 4. In matrix form, pre-multiplying by $M^*$ inflates each residual by the inverse of its leverage correction:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ne_pred <- as.numeric(M_star %*% M_d %*% y)\n\n# Equivalently: e_hat / (1 - h)\nall.equal(e_pred, e_raw / (1 - h))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nUnder homoskedasticity, $\\text{Var}[\\tilde{e}_i \\mid X] = (1 - h_{ii})^{-1}\\sigma^2$. These inflate at high-leverage points — the opposite of raw residuals.\n\n### Standardized residuals: $\\bar{e} = (M^*)^{1/2} \\hat{e} / \\hat{\\sigma}$\n\nThe standardized residual applies the square-root scaling to make residuals have (approximately) unit variance under homoskedasticity:\n\n$$\\bar{e} = \\frac{1}{\\hat{\\sigma}}(M^*)^{1/2} M y$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma_hat <- sqrt(sum(e_raw^2) / (n - K))\n\ne_std <- as.numeric(M_star_half %*% M_d %*% y) / sigma_hat\n\n# Same as rstandard()\nall.equal(e_std, as.numeric(rstandard(mod_diag)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n### Comparing the three types\n\nThe three residual types tell different stories. Raw residuals show the fan pattern of heteroskedasticity. Prediction errors *amplify* it — high-leverage observations get inflated further. Standardized residuals divide out the leverage effect, putting everything on a common scale.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_resid <- data.frame(\n  x = rep(x, 3),\n  residual = c(e_raw, e_pred, e_std),\n  type = factor(rep(c(\"Raw: Me\", \"Prediction: M*Me\", \"Standardized: (M*)^½Me / σ̂\"),\n                    each = n),\n                levels = c(\"Raw: Me\", \"Prediction: M*Me\", \"Standardized: (M*)^½Me / σ̂\"))\n)\n\nggplot(df_resid, aes(x, residual)) +\n  geom_point(alpha = 0.3, size = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  facet_wrap(~ type, scales = \"free_y\") +\n  labs(title = \"Three residual types from the same regression\",\n       subtitle = \"Each panel applies a different diagonal scaling matrix to ê = My\",\n       y = \"Residual value\")\n```\n\n::: {.cell-output-display}\n![](ch05-gls_files/figure-html/residual-comparison-1.png){width=864}\n:::\n:::\n\n\nThe fan shape is visible in all three (because the true DGP is heteroskedastic), but the *scale* differs: raw residuals have variance $(1 - h_{ii})\\sigma_i^2$, prediction errors have variance $(1 - h_{ii})^{-1}\\sigma_i^2$, and standardized residuals remove the leverage component, leaving only the heteroskedasticity $\\sigma_i^2 / \\sigma^2$.\n\n**Studentized residuals.** R also provides `rstudent()`, which replaces $\\hat{\\sigma}$ with the leave-one-out estimate $s_{(-i)}$. In practice, studentized and standardized residuals are nearly identical for moderate $n$ — the difference is just one observation's contribution to $\\hat{\\sigma}^2$. The studentized version follows a $t_{n-K-1}$ distribution under normality, which is useful for formal outlier tests:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Studentized ≈ standardized, but uses leave-one-out sigma\nmax(abs(rstudent(mod_diag) - rstandard(mod_diag)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.059\n```\n\n\n:::\n:::\n\n\n## Estimating $\\sigma^2$\n\nThree estimators of the error variance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nK <- ncol(X_d)\n\n# 1. Method of moments (biased)\nsigma2_mm <- sum(resid(mod_diag)^2) / n\n\n# 2. Bias-corrected (used by summary.lm)\nsigma2_s2 <- sum(resid(mod_diag)^2) / (n - K)\n\n# 3. Standardized estimator (unbiased under heteroskedasticity)\nsigma2_bar <- mean(resid(mod_diag)^2 / (1 - h))\n\nc(MM = sigma2_mm, s2 = sigma2_s2, standardized = sigma2_bar)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          MM           s2 standardized \n        11.6         11.7         11.7 \n```\n\n\n:::\n:::\n\n\nUnder homoskedasticity, $s^2$ is unbiased: $E[s^2 \\mid X] = \\sigma^2$. The key identity is $E[\\hat{e}'\\hat{e} \\mid X] = \\text{tr}(M) \\cdot \\sigma^2 = (n - K)\\sigma^2$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The trace trick: E[e'Me] = tr(M * E[ee']) = tr(M) * sigma^2 under homoskedasticity\nc(trace_M = tr(M_d), n_minus_K = n - K)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  trace_M n_minus_K \n      298       298 \n```\n\n\n:::\n:::\n\n\nThe standardized estimator $\\bar{\\sigma}^2 = \\frac{1}{n}\\sum (1-h_{ii})^{-1}\\hat{e}_i^2$ is unbiased even under heteroskedasticity — it corrects each squared residual for its leverage. This is the logic behind HC2 standard errors: replace $\\hat{e}_i^2$ with $\\hat{e}_i^2 / (1 - h_{ii})$ in the sandwich meat.\n\n## Method of moments perspective\n\n### OLS as a method of moments estimator\n\nOLS solves the sample analog of $E[x_i(y_i - x_i'\\beta)] = 0$:\n\n$$\\frac{1}{n} \\sum_{i=1}^n x_i(y_i - x_i'\\hat{\\beta}) = 0 \\quad \\Longleftrightarrow \\quad X'(y - X\\hat{\\beta}) = 0$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The OLS normal equations are moment conditions\nmoment <- t(X_d) %*% resid(mod_diag)\nmoment  # numerically zero\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]\n  2.78e-15\nx 5.62e-13\n```\n\n\n:::\n:::\n\n\n### GLS as efficient method of moments\n\nGLS solves a *weighted* version of the same moment condition:\n\n$$\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{\\sigma_i^2} x_i(y_i - x_i'\\hat{\\beta}_{GLS}) = 0 \\quad \\Longleftrightarrow \\quad X'\\Omega^{-1}(y - X\\hat{\\beta}_{GLS}) = 0$$\n\nThis is a **generalized method of moments** (GMM) estimator with weight matrix $\\Omega^{-1}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# GLS normal equations\nOmega_hat_inv_diag <- diag(1 / sigma2_hat)  # from our FGLS above\nmoment_gls <- t(X_d) %*% Omega_hat_inv_diag %*% resid(mod_fgls)\nmoment_gls  # numerically zero\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]\n   1.11e-14\nx -2.22e-14\n```\n\n\n:::\n:::\n\n\nWhen we have exactly as many moment conditions as parameters ($K$ equations, $K$ unknowns), the GMM estimator reduces to method of moments. The efficiency of GLS comes from choosing the optimal weight matrix.\n\nIn Chapter 14, we'll see that this logic extends to **overidentified** models: when you have *more* moment conditions than parameters, GMM finds the optimal combination. The GLS insight — weight by precision — is the same insight that drives GMM.\n\n## Application: robust inference on the Prestige data\n\nLet's apply the practical workflow to the Prestige dataset. We start with `lm_robust()` as the default — no pre-testing required.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The default workflow: OLS with HC2 robust SEs\nmod_p_robust <- lm_robust(prestige ~ education + income + women,\n                           data = Prestige, se_type = \"HC2\")\nsummary(mod_p_robust)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm_robust(formula = prestige ~ education + income + women, data = Prestige, \n    se_type = \"HC2\")\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  CI Lower CI Upper DF\n(Intercept) -6.79433   3.216715  -2.112 3.72e-02 -13.17780 -0.41087 98\neducation    4.18664   0.446857   9.369 2.83e-15   3.29986  5.07341 98\nincome       0.00131   0.000375   3.504 6.91e-04   0.00057  0.00206 98\nwomen       -0.00891   0.035522  -0.251 8.03e-01  -0.07940  0.06159 98\n\nMultiple R-squared:  0.798 ,\tAdjusted R-squared:  0.792 \nF-statistic:  137 on 3 and 98 DF,  p-value: <2e-16\n```\n\n\n:::\n:::\n\n\nThat's the complete inference in one line. The residual plot is still useful as a *descriptive* tool for understanding your data — it just shouldn't gate your choice of estimator:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_p <- lm(prestige ~ education + income + women, data = Prestige)\ndf_p <- data.frame(income = Prestige$income, resid = resid(mod_p))\nggplot(df_p, aes(income, resid)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_smooth(se = FALSE, color = \"steelblue\", method = \"loess\", formula = y ~ x) +\n  labs(title = \"Prestige: residuals vs. income\",\n       subtitle = \"Useful for understanding the data, not for choosing an estimator\")\n```\n\n::: {.cell-output-display}\n![](ch05-gls_files/figure-html/prestige-resid-plot-1.png){width=672}\n:::\n:::\n\n\nFor comparison, here is what FGLS would give if we had a *substantive* reason to believe variance is proportional to income (e.g., because occupational prestige surveys sample more respondents for common jobs):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# FGLS: only if we have a theoretical reason for the variance model\nlog_e2_p <- log(resid(mod_p)^2)\nmod_var_p <- lm(log_e2_p ~ income, data = Prestige)\nsigma2_hat_p <- exp(fitted(mod_var_p))\n\nmod_fgls_p <- lm(prestige ~ education + income + women,\n                  data = Prestige, weights = 1 / sigma2_hat_p)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Full comparison: coefficients and standard errors\ncoefs <- cbind(\n  OLS  = coef(mod_p),\n  HC2  = coef(mod_p),      # same point estimates\n  FGLS = coef(mod_fgls_p)  # different point estimates\n)\n\nses <- cbind(\n  Classical = summary(mod_p)$coefficients[, 2],\n  HC2       = summary(mod_p_robust)$coefficients[, 2],\n  FGLS      = summary(mod_fgls_p)$coefficients[, 2]\n)\n\ncat(\"Point estimates:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPoint estimates:\n```\n\n\n:::\n\n```{.r .cell-code}\nround(coefs, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                OLS     HC2    FGLS\n(Intercept) -6.7943 -6.7943 -6.6482\neducation    4.1866  4.1866  4.2360\nincome       0.0013  0.0013  0.0012\nwomen       -0.0089 -0.0089 -0.0132\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nStandard errors:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nStandard errors:\n```\n\n\n:::\n\n```{.r .cell-code}\nround(ses, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Classical    HC2   FGLS\n(Intercept)    3.2391 3.2167 3.2241\neducation      0.3887 0.4469 0.3816\nincome         0.0003 0.0004 0.0003\nwomen          0.0304 0.0355 0.0302\n```\n\n\n:::\n:::\n\n\nThe differences here are modest. The pattern illustrates the two strategies:\n\n- **Robust SEs (HC2)**: Keep the OLS point estimates, correct only the standard errors. No assumptions about the variance structure — always valid.\n- **FGLS**: Re-estimate $\\hat{\\beta}$ using the variance structure. More efficient *if* your variance model is correct; potentially worse if it's wrong.\n\nThe default for cross-sectional data is HC2. Use FGLS when you have a substantive reason to model the variance — not because a test told you to.\n\n::: {.callout-tip}\n## HC2 as Default\nFor cross-sectional data, use `lm_robust(y ~ x, se_type = \"HC2\")` or `vcovHC(mod, type = \"HC2\")` as the default. HC2 adjusts for leverage and provides better finite-sample coverage than HC0 or HC1.\n:::\n\n## Summary\n\n| Concept | Matrix formula | R code |\n|---------|---------------|--------|\n| Sandwich variance | $(X'X)^{-1}X'\\Omega X(X'X)^{-1}$ | `vcovHC(mod, type = \"HC2\")` |\n| Robust SEs | $\\sqrt{\\text{diag}(\\hat{V}_{HC})}$ | `lm_robust(y ~ x, se_type = \"HC2\")` |\n| Robust t-test | — | `coeftest(mod, vcov = vcovHC)` |\n| WLS | $(X'WX)^{-1}X'Wy$ | `lm(y ~ x, weights = w)` |\n| GLS | $(X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}y$ | `solve(t(X) %*% Oi %*% X) %*% t(X) %*% Oi %*% y` |\n| Eigendecomposition | $\\Omega = C\\Lambda C'$ | `eigen(Omega)` |\n| $\\Omega^{-1/2}$ | $C\\Lambda^{-1/2}C'$ | `C %*% diag(1/sqrt(lam)) %*% t(C)` |\n| FGLS | Estimate $\\hat{\\Omega}$, then GLS | `lm(log(e^2) ~ z)` then `lm(y ~ x, weights = ...)` |\n| Breusch-Pagan | $nR^2$ from $\\hat{e}^2/\\bar{\\hat{e}^2} \\sim X$ | `bptest(mod)` |\n| Standardized residual | $\\hat{e}_i / (\\hat{\\sigma}\\sqrt{1-h_{ii}})$ | `rstandard(mod)` |\n| Studentized residual | $\\hat{e}_i / (s_{(-i)}\\sqrt{1-h_{ii}})$ | `rstudent(mod)` |\n\n**Key takeaway.** When you know (or can estimate) the variance structure, exploit it: WLS/GLS gives you tighter estimates by trusting precise observations more. When you don't trust your variance model, use `lm_robust()` with HC2 standard errors — it's always valid and requires no assumptions about the form of heteroskedasticity. In either case, the method of moments logic — choosing the right weight matrix — connects directly to GMM (Chapter 14).\n",
    "supporting": [
      "ch05-gls_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}