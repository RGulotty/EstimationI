{
  "hash": "b7b71e67f5905798fd33417b7d553f02",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"1. Probability and Linear Algebra\"\nsubtitle: \"From random variables to the OLS estimator\"\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(MASS)\noptions(digits = 3)\n```\n:::\n\n\nThis chapter builds up the machinery behind OLS from scratch. We start with expectation and variance, show how the conditional expectation function emerges from a joint distribution, and then shift to linear algebra — matrices, projection, and the geometry that makes regression work.\n\n**Questions this chapter answers:**\n\n1. What is the best predictor of a random variable, and why does the mean minimize squared error?\n2. How does conditioning on information improve prediction through the CEF?\n3. How does the design matrix $\\mathbf{X}$ encode a regression problem?\n4. Why is OLS a geometric projection, and what do eigenvectors of the hat matrix tell us?\n\n## Expectation as a best guess\n\nIf you had to predict a random variable $Y$ with a single number $\\mu$, and your penalty for being wrong is squared error, you'd choose the mean. Let's verify this computationally.\n\nSuppose $Y$ takes values $\\{1, 2, 3, 4, 5\\}$ with probabilities $\\{0.1, 0.2, 0.3, 0.25, 0.15\\}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvalues <- 1:5\nprobs <- c(0.1, 0.2, 0.3, 0.25, 0.15)\n\n# The expected value\nmu <- sum(values * probs)\nmu\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.15\n```\n\n\n:::\n\n```{.r .cell-code}\n# Mean squared error as a function of the guess\nmse <- function(guess) {\n  sum(probs * (values - guess)^2)\n}\n\n# Evaluate over a grid\nguesses <- seq(1, 5, by = 0.01)\nerrors <- sapply(guesses, mse)\n\nggplot(data.frame(guess = guesses, mse = errors), aes(guess, mse)) +\n  geom_line(linewidth = 0.8) +\n  geom_vline(xintercept = mu, linetype = \"dashed\", color = \"steelblue\") +\n  annotate(\"text\", x = mu + 0.3, y = max(errors) * 0.9,\n           label = paste(\"E[Y] =\", mu), color = \"steelblue\") +\n  labs(x = \"Guess (mu)\", y = \"Mean Squared Error\",\n       title = \"The expectation minimizes mean squared prediction error\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![The expectation minimizes mean squared prediction error](ch01-review_files/figure-html/expectation-minimizes-mse-1.png){width=672}\n:::\n:::\n\n\nThe minimum of the parabola lands exactly at $\\mathbb{E}[Y]$. This is the simplest version of a result that recurs throughout the course: the conditional expectation function minimizes MSE given information.\n\n::: {#thm-mse-minimizer}\n## E[Y] Minimizes MSE\nAmong all constants $\\mu$, the expected value $\\mathbb{E}[Y]$ uniquely minimizes the mean squared error $\\mathbb{E}[(Y - \\mu)^2]$.\n:::\n\n## Variance and the deviation identity\n\nVariance measures the average squared distance from the mean:\n\n$$\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$$ {#eq-mse}\n\nLet's verify both formulas give the same answer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nEY <- sum(values * probs)\nEY2 <- sum(values^2 * probs)\n\n# Definition form\nvar_def <- sum(probs * (values - EY)^2)\n\n# Shortcut form\nvar_short <- EY2 - EY^2\n\nc(definition = var_def, shortcut = var_short)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndefinition   shortcut \n      1.43       1.43 \n```\n\n\n:::\n:::\n\n\nIn a sample, we can also write the variance as $\\frac{1}{n}\\sum(x_i - \\bar{x})x_i$. This works because deviations from the mean always sum to zero: $\\sum(x_i - \\bar{x}) = 0$, so shifting the second factor by any constant changes nothing. This same algebra produces the OLS normal equations later.\n\nThe interactive visualization below makes this concrete using the sample $x = \\{1, 5, 6, 8\\}$ with mean $\\bar{x} = 5$. Think of each term as the signed area of a rectangle with width $(x_i - \\bar{x})$ and height $(x_i - c)$. Slide $c$ from $\\bar{x}$ down to $0$: the rectangles change shape, but the total signed area stays constant.\n\n```{ojs}\n//| echo: false\n\nviewof ref = Inputs.range([0, 5], {value: 5, step: 0.1, label: \"Reference point (c)\"})\n```\n\n```{ojs}\n//| echo: false\n\n{\n  const width = 500;\n  const height = 420;\n  const margin = {top: 30, right: 40, bottom: 50, left: 50};\n\n  const data = [1, 5, 6, 8];\n  const xbar = 5;\n  const c = ref;\n\n  const svg = d3.create(\"svg\")\n    .attr(\"viewBox\", [0, 0, width, height])\n    .attr(\"width\", width)\n    .attr(\"height\", height)\n    .style(\"font-family\", \"sans-serif\");\n\n  const xScale = d3.scaleLinear()\n    .domain([-1, 9.5])\n    .range([margin.left, width - margin.right]);\n\n  const yScale = d3.scaleLinear()\n    .domain([-1, 9.5])\n    .range([height - margin.bottom, margin.top]);\n\n  // Axes\n  svg.append(\"g\")\n    .attr(\"transform\", `translate(0,${yScale(0)})`)\n    .call(d3.axisBottom(xScale).ticks(10))\n    .append(\"text\")\n    .attr(\"x\", width - margin.right)\n    .attr(\"y\", -10)\n    .attr(\"fill\", \"black\")\n    .attr(\"text-anchor\", \"end\")\n    .text(\"x\");\n\n  svg.append(\"g\")\n    .attr(\"transform\", `translate(${xScale(0)},0)`)\n    .call(d3.axisLeft(yScale).ticks(10));\n\n  // Draw x-bar vertical line\n  svg.append(\"line\")\n    .attr(\"x1\", xScale(xbar)).attr(\"x2\", xScale(xbar))\n    .attr(\"y1\", yScale(-0.5)).attr(\"y2\", yScale(9))\n    .attr(\"stroke\", \"steelblue\").attr(\"stroke-dasharray\", \"4,4\").attr(\"stroke-width\", 1.5);\n\n  svg.append(\"text\")\n    .attr(\"x\", xScale(xbar)).attr(\"y\", yScale(-0.8))\n    .attr(\"text-anchor\", \"middle\").attr(\"fill\", \"steelblue\").attr(\"font-size\", 13)\n    .text(\"x̄ = 5\");\n\n  // Draw x-bar horizontal line\n  svg.append(\"line\")\n    .attr(\"x1\", xScale(-0.5)).attr(\"x2\", xScale(9))\n    .attr(\"y1\", yScale(xbar)).attr(\"y2\", yScale(xbar))\n    .attr(\"stroke\", \"steelblue\").attr(\"stroke-dasharray\", \"4,4\").attr(\"stroke-width\", 1.5);\n\n  // Draw reference vertical line\n  svg.append(\"line\")\n    .attr(\"x1\", xScale(c)).attr(\"x2\", xScale(c))\n    .attr(\"y1\", yScale(-0.5)).attr(\"y2\", yScale(9))\n    .attr(\"stroke\", \"tomato\").attr(\"stroke-dasharray\", \"2,3\").attr(\"stroke-width\", 1.5);\n\n  svg.append(\"text\")\n    .attr(\"x\", xScale(c)).attr(\"y\", yScale(9.3))\n    .attr(\"text-anchor\", \"middle\").attr(\"fill\", \"tomato\").attr(\"font-size\", 12)\n    .text(`c = ${c.toFixed(1)}`);\n\n  // Positive/negative color for rectangles\n  const posColor = \"rgba(70, 130, 180, 0.3)\";\n  const negColor = \"rgba(220, 80, 80, 0.3)\";\n  const posStroke = \"steelblue\";\n  const negStroke = \"tomato\";\n\n  // Draw rectangles\n  for (const xi of data) {\n    const dev = xi - xbar;\n    const ht = xi - c;\n    const area = dev * ht;\n\n    if (Math.abs(dev) < 0.001) continue;\n\n    const isPositive = area >= 0;\n\n    const rx2 = xScale(Math.min(xbar, xi));\n    const ry2 = yScale(Math.max(c, xi));\n    const rw2 = Math.abs(xScale(xi) - xScale(xbar));\n    const rh2 = Math.abs(yScale(c) - yScale(xi));\n\n    svg.append(\"rect\")\n      .attr(\"x\", rx2).attr(\"y\", ry2)\n      .attr(\"width\", rw2).attr(\"height\", rh2)\n      .attr(\"fill\", isPositive ? posColor : negColor)\n      .attr(\"stroke\", isPositive ? posStroke : negStroke)\n      .attr(\"stroke-width\", 1.5);\n\n    const cx = (Math.min(xbar, xi) + Math.max(xbar, xi)) / 2;\n    const cy = (Math.min(c, xi) + Math.max(c, xi)) / 2;\n\n    if (rw2 > 15 && rh2 > 15) {\n      svg.append(\"text\")\n        .attr(\"x\", xScale(cx)).attr(\"y\", yScale(cy))\n        .attr(\"text-anchor\", \"middle\").attr(\"dominant-baseline\", \"middle\")\n        .attr(\"font-size\", 11).attr(\"fill\", \"#333\")\n        .text(`${dev.toFixed(0)}×${ht.toFixed(1)}`);\n    }\n  }\n\n  // Draw data points on the diagonal\n  for (const xi of data) {\n    svg.append(\"circle\")\n      .attr(\"cx\", xScale(xi)).attr(\"cy\", yScale(xi))\n      .attr(\"r\", 5).attr(\"fill\", \"black\");\n  }\n\n  // Compute total and display\n  let total = 0;\n  for (const xi of data) {\n    total += (xi - xbar) * (xi - c);\n  }\n  const variance = total / data.length;\n\n  svg.append(\"text\")\n    .attr(\"x\", xScale(7)).attr(\"y\", yScale(1.5))\n    .attr(\"text-anchor\", \"middle\").attr(\"font-size\", 14).attr(\"fill\", \"#333\")\n    .html(`Total area = ${total.toFixed(1)}`);\n\n  svg.append(\"text\")\n    .attr(\"x\", xScale(7)).attr(\"y\", yScale(0.5))\n    .attr(\"text-anchor\", \"middle\").attr(\"font-size\", 14).attr(\"font-weight\", \"bold\").attr(\"fill\", \"#333\")\n    .text(`Var = ${total.toFixed(1)} / ${data.length} = ${variance.toFixed(2)}`);\n\n  return svg.node();\n}\n```\n\n## Joint distributions and the CEF\n\nThe lecture works through an example where two parties (A and B) mobilize voters, with joint density $f(x, y) = 6(1 - x - y)$ for $x, y \\geq 0$ and $x + y \\leq 1$. Let's visualize this and compute the CEF.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a grid\ngrid <- expand.grid(x = seq(0, 1, length = 200), y = seq(0, 1, length = 200))\ngrid$density <- ifelse(grid$x + grid$y <= 1,\n                       6 * (1 - grid$x - grid$y), NA)\n\nggplot(grid, aes(x, y, fill = density)) +\n  geom_tile() +\n  scale_fill_viridis_c(na.value = \"white\", name = \"f(x,y)\") +\n  labs(title = \"Joint density: f(x, y) = 6(1 - x - y)\",\n       x = \"Party A mobilization (x)\",\n       y = \"Party B mobilization (y)\") +\n  coord_fixed() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](ch01-review_files/figure-html/joint-density-1.png){width=672}\n:::\n:::\n\n\nFrom the joint density we can derive the marginal $f_X(x) = 3(1-x)^2$, the conditional $f_{Y|X}(y|x) = \\frac{2(1-x-y)}{(1-x)^2}$, and the CEF $\\mathbb{E}[Y|X=x] = \\frac{1-x}{3}$. Let's verify by simulation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate from f(x,y) = 6(1-x-y) using rejection sampling\nset.seed(123)\nn_sim <- 50000\nsamples <- data.frame(x = numeric(0), y = numeric(0))\n\nwhile (nrow(samples) < n_sim) {\n  x_prop <- runif(n_sim)\n  y_prop <- runif(n_sim)\n  valid <- (x_prop + y_prop <= 1)\n  x_prop <- x_prop[valid]\n  y_prop <- y_prop[valid]\n  # Accept with probability proportional to density\n  accept_prob <- 6 * (1 - x_prop - y_prop) / 6  # max density is 6\n\n  keep <- runif(length(x_prop)) < accept_prob\n  samples <- rbind(samples, data.frame(x = x_prop[keep], y = y_prop[keep]))\n}\nsamples <- samples[1:n_sim, ]\n\n# Bin x and compute mean of y in each bin\nsamples$x_bin <- cut(samples$x, breaks = 30)\ncef_empirical <- aggregate(y ~ x_bin, data = samples, FUN = mean)\ncef_empirical$x_mid <- seq(0.02, 0.98, length = nrow(cef_empirical))\n\n# Plot simulation vs. theoretical CEF\nggplot(cef_empirical, aes(x_mid, y)) +\n  geom_point(alpha = 0.7) +\n  stat_function(fun = function(x) (1 - x) / 3, color = \"red\", linewidth = 1) +\n  labs(x = \"x\", y = \"E[Y | X = x]\",\n       title = \"CEF: simulation vs. theory\",\n       subtitle = \"Red line: (1 - x)/3\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](ch01-review_files/figure-html/cef-simulation-1.png){width=672}\n:::\n:::\n\n\nThe simulated conditional means track the theoretical CEF closely. This is a case where the CEF happens to be linear — but that's a special property of this density, not a general fact.\n\n## The law of iterated expectations {#sec-lie}\n\nThe LIE says $\\mathbb{E}[\\mathbb{E}[Y|X]] = \\mathbb{E}[Y]$. In words: the average of the conditional averages equals the overall average. Let's check.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Overall mean of Y from simulation\noverall_mean <- mean(samples$y)\n\n# Theoretical E[Y]: integrate y * f_Y(y)\n# f_Y(y) = 3(1-y)^2 by symmetry, so E[Y] = integral of y * 3(1-y)^2 from 0 to 1\nEY_theory <- integrate(function(y) y * 3 * (1 - y)^2, 0, 1)$value\n\n# E[E[Y|X]] = E[(1-X)/3] = integral of (1-x)/3 * 3(1-x)^2 dx = integral of (1-x)^3 dx\nEEY_X <- integrate(function(x) (1 - x)^3, 0, 1)$value\n\nc(simulation = overall_mean, `E[Y] theory` = EY_theory, `E[E[Y|X]]` = EEY_X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n simulation E[Y] theory   E[E[Y|X]] \n       0.25        0.25        0.25 \n```\n\n\n:::\n:::\n\n\nAll three agree: the LIE holds.\n\n::: {#thm-lie}\n## Law of Iterated Expectations\nFor any random variables $X$ and $Y$, $\\mathbb{E}[\\mathbb{E}[Y|X]] = \\mathbb{E}[Y]$. The average of the conditional averages equals the unconditional average.\n:::\n\n## Law of total variance\n\nThe law of total variance decomposes $\\text{Var}(Y) = \\mathbb{E}[\\text{Var}(Y|X)] + \\text{Var}(\\mathbb{E}[Y|X])$. This is the foundation of $R^2$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Var(Y) from simulation\nvar_y_sim <- var(samples$y)\n\n# Compute within each bin: variance of y and mean of y\nbin_stats <- aggregate(y ~ x_bin, data = samples, FUN = function(z) c(m = mean(z), v = var(z), n = length(z)))\nbin_stats <- do.call(data.frame, bin_stats)\n\n# E[Var(Y|X)] - average of conditional variances (weighted by bin size)\nweights <- bin_stats$y.n / sum(bin_stats$y.n)\nE_var_YX <- sum(weights * bin_stats$y.v)\n\n# Var(E[Y|X]) - variance of conditional means\nvar_EYX <- sum(weights * (bin_stats$y.m - sum(weights * bin_stats$y.m))^2)\n\nc(`Var(Y)` = var_y_sim,\n  `E[Var(Y|X)] + Var(E[Y|X])` = E_var_YX + var_EYX,\n  `explained fraction` = var_EYX / var_y_sim)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   Var(Y) E[Var(Y|X)] + Var(E[Y|X])        explained fraction \n                   0.0375                    0.0375                    0.1125 \n```\n\n\n:::\n:::\n\n\nThe decomposition works. The \"explained fraction\" is essentially $R^2$: how much of the total variance of $Y$ is captured by the CEF.\n\n::: {#thm-total-variance}\n## Law of Total Variance\n$\\text{Var}(Y) = \\mathbb{E}[\\text{Var}(Y|X)] + \\text{Var}(\\mathbb{E}[Y|X])$. Total variance decomposes into average conditional variance plus variance of conditional means.\n:::\n\n## From probability to matrices\n\nNow we shift gears. The CEF tells us _what_ we're trying to estimate. Linear algebra tells us _how_.\n\n### The design matrix\n\nIn regression, we organize our data as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{e}$. Let's build a design matrix and see what $\\mathbf{X}'\\mathbf{X}$ and $\\mathbf{X}'\\mathbf{y}$ look like.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2026)\nn <- 50\nx1 <- rnorm(n)\nx2 <- rnorm(n)\ny <- 2 + 3 * x1 - 1.5 * x2 + rnorm(n)\n\n# Build the design matrix (intercept + two regressors)\nX <- cbind(1, x1, x2)\ncolnames(X) <- c(\"intercept\", \"x1\", \"x2\")\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     intercept      x1     x2\n[1,]         1  0.5206 -1.170\n[2,]         1 -1.0797 -0.206\n[3,]         1  0.1392 -0.931\n[4,]         1 -0.0847  0.449\n[5,]         1 -0.6666 -0.645\n[6,]         1 -2.5161 -0.231\n```\n\n\n:::\n:::\n\n\n### The Gram matrix $\\mathbf{X}'\\mathbf{X}$\n\nThis matrix encodes all second-moment information about the regressors. Its $(i,j)$ entry is the inner product of columns $i$ and $j$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXtX <- t(X) %*% X\nXtX\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          intercept     x1    x2\nintercept    50.000 -0.813 -8.99\nx1           -0.813 46.685 11.93\nx2           -8.991 11.926 53.88\n```\n\n\n:::\n\n```{.r .cell-code}\n# It's symmetric\nall.equal(XtX, t(XtX))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# And positive definite (all eigenvalues positive)\neigen(XtX)$values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 66.5 48.1 36.0\n```\n\n\n:::\n:::\n\n\nAll eigenvalues are positive, so $\\mathbf{X}'\\mathbf{X}$ is positive definite and invertible.\n\n::: {.callout-note}\n## Full Rank Assumption\nOLS requires $\\mathbf{X}'\\mathbf{X}$ to be invertible, which holds when $\\mathbf{X}$ has full column rank — no column is a perfect linear combination of others. Equivalently, all eigenvalues of $\\mathbf{X}'\\mathbf{X}$ must be strictly positive.\n:::\n\n### Computing OLS by hand\n\nThe OLS estimator is $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}$. Let's compute it step by step.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXty <- t(X) %*% y\nbeta_hat <- solve(XtX) %*% Xty\n\n# Compare to lm()\nbeta_lm <- coef(lm(y ~ x1 + x2))\n\ncbind(manual = beta_hat, lm = beta_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   lm\nintercept  2.13  2.13\nx1         2.93  2.93\nx2        -1.50 -1.50\n```\n\n\n:::\n:::\n\n\nThey match. The `solve()` function computes the matrix inverse, and `%*%` does matrix multiplication.\n\n::: {.callout-tip}\n## Use `crossprod()` for Speed\n`crossprod(X)` computes $X'X$ more efficiently than `t(X) %*% X`, and `crossprod(X, y)` computes $X'y$. For large matrices, the speedup is substantial.\n:::\n\n## Projection and residuals {#sec-projection}\n\nOLS is a projection. The hat matrix $\\mathbf{P} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'$ projects $\\mathbf{y}$ onto the column space of $\\mathbf{X}$, and $\\mathbf{M} = \\mathbf{I} - \\mathbf{P}$ projects onto its orthogonal complement.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nP <- X %*% solve(XtX) %*% t(X)\nM <- diag(n) - P\n\ny_hat <- P %*% y      # fitted values\ne_hat <- M %*% y      # residuals\n\n# Residuals are orthogonal to every column of X\nt(X) %*% e_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               [,1]\nintercept -3.29e-14\nx1         4.09e-14\nx2         5.55e-15\n```\n\n\n:::\n:::\n\n\nAll zeros (up to floating point). This is the matrix version of the OLS first-order conditions: $\\mathbf{X}'\\mathbf{e} = \\mathbf{0}$.\n\n### Idempotency\n\nBoth $\\mathbf{P}$ and $\\mathbf{M}$ are idempotent: applying them twice is the same as applying them once. Projecting an already-projected vector does nothing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# P^2 = P\nmax(abs(P %*% P - P))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.55e-17\n```\n\n\n:::\n\n```{.r .cell-code}\n# M^2 = M\nmax(abs(M %*% M - M))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.44e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Eigenvalues of P are 0 or 1\nround(eigen(P)$values, 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 0\n```\n\n\n:::\n:::\n\n\nThe hat matrix has $k = 3$ eigenvalues equal to 1 (one per regressor) and $n - k = 47$ equal to 0. The trace — and therefore the rank — equals $k$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(trace_P = sum(diag(P)), k = ncol(X))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntrace_P       k \n      3       3 \n```\n\n\n:::\n\n```{.r .cell-code}\nc(trace_M = sum(diag(M)), `n - k` = n - ncol(X))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntrace_M   n - k \n     47      47 \n```\n\n\n:::\n:::\n\n\nThis is where degrees of freedom come from. The residuals live in an $(n-k)$-dimensional space, which is why we divide the sum of squared residuals by $n - k$ to get an unbiased estimate of $\\sigma^2$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma2_biased <- sum(e_hat^2) / n\nsigma2_unbiased <- sum(e_hat^2) / (n - ncol(X))\n\nc(biased = sigma2_biased, unbiased = sigma2_unbiased,\n  true = 1)  # we set sd = 1 in the DGP\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  biased unbiased     true \n    1.09     1.16     1.00 \n```\n\n\n:::\n:::\n\n\n## Positive definiteness and the OLS bowl\n\nWe saw that $\\mathbf{X}'\\mathbf{X}$ has all positive eigenvalues — it's positive definite. This has a visual payoff: the sum of squared residuals, as a function of $\\boldsymbol{\\beta}$, is a bowl with a unique minimum.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fix intercept at its OLS value, vary beta1 and beta2\nb0_hat <- beta_hat[1]\nb1_grid <- seq(1.5, 4.5, length = 50)\nb2_grid <- seq(-3, 0, length = 50)\n\nssr_surface <- outer(b1_grid, b2_grid, Vectorize(function(b1, b2) {\n  sum((y - X %*% c(b0_hat, b1, b2))^2)\n}))\n\ncontour(b1_grid, b2_grid, ssr_surface, nlevels = 25,\n        xlab = expression(beta[1]), ylab = expression(beta[2]),\n        main = \"Sum of Squared Residuals\")\npoints(beta_hat[2], beta_hat[3], pch = 19, col = \"red\", cex = 1.5)\n```\n\n::: {.cell-output-display}\n![](ch01-review_files/figure-html/ols-bowl-1.png){width=672}\n:::\n:::\n\n\nThe contours are ellipses (because $\\mathbf{X}'\\mathbf{X}$ is positive definite), and the red dot marks the OLS solution at the center.\n\n## The normal distribution\n\nMany results in the course rely on normality, especially for small-sample inference. Here's what different normal densities look like, and the key property that linear transformations preserve normality.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data.frame(x = c(-5, 5)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1),\n                aes(color = \"N(0, 1)\"), linewidth = 0.8) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = sqrt(0.5)),\n                aes(color = \"N(0, 0.5)\"), linewidth = 0.8) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = sqrt(2)),\n                aes(color = \"N(0, 2)\"), linewidth = 0.8) +\n  scale_color_manual(values = c(\"steelblue\", \"tomato\", \"forestgreen\"),\n                     name = \"\") +\n  labs(x = \"x\", y = \"f(x)\",\n       title = \"Normal densities with different variances\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](ch01-review_files/figure-html/normal-distributions-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# If X ~ N(2, 3), then aX + b ~ N(a*2 + b, a^2 * 3)\nset.seed(99)\nX_samp <- rnorm(10000, mean = 2, sd = sqrt(3))\nY_samp <- 4 * X_samp + 1  # a = 4, b = 1\n\nc(empirical_mean = mean(Y_samp), theoretical_mean = 4 * 2 + 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  empirical_mean theoretical_mean \n            9.02             9.00 \n```\n\n\n:::\n\n```{.r .cell-code}\nc(empirical_var = var(Y_samp), theoretical_var = 16 * 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  empirical_var theoretical_var \n           48.1            48.0 \n```\n\n\n:::\n:::\n\n\n## Summary\n\nThis chapter covered the computational foundations:\n\n- **Expectation** minimizes mean squared error — the simplest prediction problem.\n- **The CEF** generalizes this: the best prediction of $Y$ given $X$.\n- **The law of iterated expectations** connects conditional and unconditional means.\n- **The law of total variance** decomposes variance into explained and unexplained parts ($R^2$).\n- **The design matrix** $\\mathbf{X}$ and Gram matrix $\\mathbf{X}'\\mathbf{X}$ encode the regression problem.\n- **OLS is a projection** of $\\mathbf{y}$ onto the column space of $\\mathbf{X}$.\n- **Positive definiteness** guarantees a unique solution and a bowl-shaped objective.\n- **Degrees of freedom** ($n - k$) come from the dimension of the residual space.\n\nNext: [The CEF and Best Linear Predictor](ch02-cef-blp.qmd) — what happens when the CEF isn't linear, and why linear regression still works.\n",
    "supporting": [
      "ch01-review_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}