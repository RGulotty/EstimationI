{
  "hash": "3aef09116e9a74333a256d8cb4960bbc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"3. Multivariate OLS\"\nsubtitle: \"Deriving and computing the OLS estimator\"\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(carData)\noptions(digits = 3)\ntr <- function(M) sum(diag(M))  # R has no built-in trace\n```\n:::\n\n\nThe OLS estimator $\\hat\\beta = (X'X)^{-1}X'y$ is a matrix formula. Every piece of it — the transpose, the product, the inverse — has a direct R function. This chapter builds OLS from those building blocks, first geometrically in two dimensions, then in full matrix form with real data.\n\n**Questions this chapter answers:**\n\n1. What R functions implement matrix operations, and how does `crossprod()` relate to the normal equations?\n2. What is the geometry of OLS — why is regression a projection?\n3. How do we derive $\\hat\\beta = (X'X)^{-1}X'y$ from the minimization of SSE?\n4. How do standard errors arise from $s_e^2(X'X)^{-1}$, and what makes them precise or imprecise?\n\n## R's matrix toolkit\n\nBefore deriving anything, here are the operations we'll use throughout.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(2, 1, 1, 3), nrow = 2)\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    2    1\n[2,]    1    3\n```\n\n\n:::\n\n```{.r .cell-code}\n# 1. Transpose: t()\nt(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    2    1\n[2,]    1    3\n```\n\n\n:::\n\n```{.r .cell-code}\n# 2. Matrix multiply: %*%  (not * which is element-wise)\nB <- matrix(c(1, 0, -1, 2), nrow = 2)\nA %*% B\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    2    0\n[2,]    1    5\n```\n\n\n:::\n\n```{.r .cell-code}\n# 3. Inverse: solve()\nsolve(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]  0.6 -0.2\n[2,] -0.2  0.4\n```\n\n\n:::\n\n```{.r .cell-code}\nA %*% solve(A)  # identity\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          [,1] [,2]\n[1,]  1.00e+00    0\n[2,] -1.11e-16    1\n```\n\n\n:::\n\n```{.r .cell-code}\n# 4. Trace: sum(diag())\ntr(A)   # sum of diagonal elements\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5\n```\n\n\n:::\n\n```{.r .cell-code}\n# 5. Eigendecomposition: eigen()\neigen(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\neigen() decomposition\n$values\n[1] 3.62 1.38\n\n$vectors\n      [,1]   [,2]\n[1,] 0.526 -0.851\n[2,] 0.851  0.526\n```\n\n\n:::\n\n```{.r .cell-code}\n# 6. Determinant: det()\ndet(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5\n```\n\n\n:::\n\n```{.r .cell-code}\n# 7. Cross product shortcuts\nx_vec <- c(1, 2, 3)\ny_vec <- c(4, 5, 6)\nc(manual = sum(x_vec * y_vec),\n  crossprod = as.numeric(crossprod(x_vec, y_vec)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   manual crossprod \n       32        32 \n```\n\n\n:::\n:::\n\n\n`crossprod(X)` computes $X'X$ faster than `t(X) %*% X`, and `crossprod(X, y)` computes $X'y$. We'll use these constantly.\n\n## Geometry: projection in two dimensions\n\nYou're used to plotting data with variables on the axes — one axis for $X$, one for $Y$, and each point is an observation. The geometric view of regression flips this: each axis is an *observation*, and each variable is a *vector*. A variable with $n$ observations is a vector in $\\mathbb{R}^n$.\n\nWhy think this way? Because regression asks: among all scalar multiples of $\\mathbf{x}$ (all predictions of the form $b\\mathbf{x}$), which one is closest to $\\mathbf{y}$? That's a projection — dropping a perpendicular from $\\mathbf{y}$ onto the line spanned by $\\mathbf{x}$.\n\nWith just two observations, we can see this on a page. Suppose we survey two students: we record how long each spent on homework ($\\mathbf{h}$) and how long reading the textbook ($\\mathbf{t}$). Each variable is a 2-vector, and we can plot both in $\\mathbb{R}^2$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Student 1: 3 hrs homework, 2.5 hrs reading\n# Student 2: 5 hrs homework, 2 hrs reading\nh <- c(3, 5)    # outcome: homework time\ntt <- c(2.5, 2)  # predictor: reading time\n```\n:::\n\n\nWe want to predict $\\mathbf{h}$ using $\\mathbf{t}$: find $b$ so that $b\\mathbf{t}$ is as close to $\\mathbf{h}$ as possible. Geometrically, $b\\mathbf{t}$ must lie on the line through $\\mathbf{t}$ (the dotted line in the plot below), and the best choice is the one where the \"miss\" — the residual — is perpendicular to that line.\n\nThe **vector projection** of $\\mathbf{h}$ onto $\\mathbf{t}$ solves this:\n\n$$\\hat{\\mathbf{h}} = \\frac{\\mathbf{t} \\cdot \\mathbf{h}}{\\mathbf{t} \\cdot \\mathbf{t}} \\mathbf{t}$$\n\nThe scalar $b = \\frac{\\mathbf{t} \\cdot \\mathbf{h}}{\\mathbf{t} \\cdot \\mathbf{t}}$ minimizes $\\|\\mathbf{h} - b\\mathbf{t}\\|^2$ — the sum of squared errors. This is the least squares solution, derived purely from geometry.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The projection coefficient\nb <- as.numeric(crossprod(tt, h) / crossprod(tt))\nb\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.71\n```\n\n\n:::\n\n```{.r .cell-code}\n# The projected vector (fitted values)\nh_hat <- b * tt\nh_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.27 3.41\n```\n\n\n:::\n:::\n\n\nAnd this is exactly what `lm()` computes when we regress $\\mathbf{h}$ on $\\mathbf{t}$ without an intercept:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# OLS without intercept gives the same b\ncoef(lm(h ~ tt - 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  tt \n1.71 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Fitted values = the projection\ncbind(projection = h_hat, fitted = fitted(lm(h ~ tt - 1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  projection fitted\n1       4.27   4.27\n2       3.41   3.41\n```\n\n\n:::\n:::\n\n\nThe residual $\\mathbf{e} = \\mathbf{h} - \\hat{\\mathbf{h}}$ is orthogonal to $\\mathbf{t}$ — their dot product is zero:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ne_vec <- h - h_hat\nas.numeric(crossprod(tt, e_vec))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -4.44e-16\n```\n\n\n:::\n:::\n\n\nThis orthogonality is the geometric content of the normal equations $X'e = 0$. Here's the full picture:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_arrows <- data.frame(\n  x0 = c(0, 0, 0, h_hat[1]),\n  y0 = c(0, 0, 0, h_hat[2]),\n  x1 = c(h[1], tt[1], h_hat[1], h[1]),\n  y1 = c(h[2], tt[2], h_hat[2], h[2]),\n  label = c(\"h (outcome)\", \"t (regressor)\", \"h-hat (fitted)\", \"e (residual)\"),\n  color = c(\"black\", \"gray50\", \"forestgreen\", \"tomato\")\n)\n\nslope_t <- tt[2] / tt[1]\n\nggplot() +\n  geom_abline(intercept = 0, slope = slope_t, linetype = \"dotted\", alpha = 0.3) +\n  geom_segment(data = df_arrows,\n               aes(x = x0, y = y0, xend = x1, yend = y1, color = label),\n               arrow = arrow(length = unit(0.15, \"inches\")),\n               linewidth = 1.1) +\n  scale_color_manual(values = c(\"h (outcome)\" = \"black\",\n                                \"t (regressor)\" = \"gray50\",\n                                \"h-hat (fitted)\" = \"forestgreen\",\n                                \"e (residual)\" = \"tomato\"),\n                     name = \"\") +\n  coord_fixed(xlim = c(-1, 5), ylim = c(-1, 6)) +\n  labs(x = \"Observation 1\", y = \"Observation 2\",\n       title = \"OLS finds the closest point on the line of t to h\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![OLS finds the closest point on the line of t to h](ch03-ols_files/figure-html/geometry-plot-1.png){width=480}\n:::\n:::\n\n\nThe green vector ($\\hat{\\mathbf{h}}$) is the best prediction in the \"column space\" of $\\mathbf{t}$, and the red vector ($\\mathbf{e}$) is the part of $\\mathbf{h}$ that $\\mathbf{t}$ cannot explain. With $n = 100$ observations, these vectors live in $\\mathbb{R}^{100}$ and we can't draw them — but the geometry is identical. With multiple regressors, the \"line\" becomes a plane (or hyperplane), and the projection lands on the closest point in that plane.\n\n## Building the design matrix\n\nThe model $y = X\\beta + e$ stacks $n$ observations into a matrix. Each row of $X$ is one observation; each column is one variable. The first column is typically all ones (the intercept).\n\nWe'll use the Canadian Prestige dataset: the Pineo-Porter prestige score of occupations, predicted by average education (years) and average income (dollars) of workers in each occupation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(Prestige)\nn <- nrow(Prestige)\nK <- 3  # intercept + education + income\n\nX <- cbind(1, Prestige$education, Prestige$income)\ny <- Prestige$prestige\n\ndim(X)   # n x K\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 102   3\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]  [,3]\n[1,]    1 13.1 12351\n[2,]    1 12.3 25879\n[3,]    1 12.8  9271\n[4,]    1 11.4  8865\n[5,]    1 14.6  8403\n[6,]    1 15.6 11030\n```\n\n\n:::\n:::\n\n\nThe two fundamental products in OLS are $X'X$ (a $K \\times K$ matrix) and $X'y$ (a $K \\times 1$ vector):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXtX <- crossprod(X)       # K x K: t(X) %*% X\nXtX\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]    [,2]     [,3]\n[1,]    102    1095 6.93e+05\n[2,]   1095   12513 8.12e+06\n[3,] 693386 8121410 6.53e+09\n```\n\n\n:::\n\n```{.r .cell-code}\nXty <- crossprod(X, y)    # K x 1: t(X) %*% y\nXty\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n[1,]     4777\n[2,]    55326\n[3,] 37748108\n```\n\n\n:::\n:::\n\n\n$X'X$ encodes the relationships among the regressors. The diagonal holds $\\sum X_k^2$ for each variable; the off-diagonals hold $\\sum X_j X_k$. Dividing by $n$ gives the sample second-moment matrix.\n\n## Bivariate OLS: the formula connection\n\nBefore the matrix derivation, recall the bivariate OLS formula: $\\hat\\beta_1 = \\text{Cov}(X, Y)/\\text{Var}(X)$. This is the sample analogue of the BLP coefficient from Chapter 2. Let's verify it matches the matrix formula using just education as a predictor:\n\n\n::: {.cell}\n\n```{.r .cell-code}\neduc <- Prestige$education\n\n# Formula approach\nbeta1_formula <- cov(educ, y) / var(educ)\nbeta0_formula <- mean(y) - beta1_formula * mean(educ)\n\n# Matrix approach (2x2 system)\nX_biv <- cbind(1, educ)\nbeta_biv <- solve(crossprod(X_biv), crossprod(X_biv, y))\n\n# lm() approach\nbeta_lm <- coef(lm(prestige ~ education, data = Prestige))\n\ncbind(formula = c(beta0_formula, beta1_formula),\n      matrix = beta_biv,\n      lm = beta_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     formula            lm\n      -10.73 -10.73 -10.73\neduc    5.36   5.36   5.36\n```\n\n\n:::\n:::\n\n\nAll three give the same answer. The matrix formula $\\hat\\beta = (X'X)^{-1}X'y$ generalizes the bivariate $\\text{Cov}/\\text{Var}$ formula to any number of regressors.\n\n## Deriving OLS with matrix calculus {#sec-ols-derivation}\n\nThe sum of squared errors in matrix form is:\n\n$$\\text{SSE}(\\beta) = (y - X\\beta)'(y - X\\beta) = \\underbrace{y'y}_{\\text{constant}} - \\underbrace{2y'X\\beta}_{\\text{linear}} + \\underbrace{\\beta'X'X\\beta}_{\\text{quadratic}}$$ {#eq-ols}\n\nLet's build each piece in R and verify the expansion:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_test <- c(0, 1, 0.001)  # an arbitrary beta to test\n\n# Direct computation\nsse_direct <- as.numeric(crossprod(y - X %*% beta_test))\n\n# Expanded form\npiece1 <- as.numeric(crossprod(y))                         # y'y\npiece2 <- as.numeric(2 * crossprod(y, X %*% beta_test))    # 2y'Xbeta\npiece3 <- as.numeric(t(beta_test) %*% XtX %*% beta_test)  # beta'X'Xbeta\n\nc(direct = sse_direct, expanded = piece1 - piece2 + piece3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  direct expanded \n  102760   102760 \n```\n\n\n:::\n:::\n\n\nSetting $\\partial \\text{SSE}/\\partial \\beta = -2X'y + 2X'X\\hat\\beta = 0$ gives the **normal equations**:\n\n$$X'X\\hat\\beta = X'y$$ {#eq-normal-equations}\n\nSolving with `solve()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# solve(A, b) solves the system Ax = b — better than solve(A) %*% b\nbeta_hat <- solve(XtX, Xty)\nbeta_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n[1,] -6.84778\n[2,]  4.13744\n[3,]  0.00136\n```\n\n\n:::\n\n```{.r .cell-code}\n# lm() gives the same thing\ncoef(lm(prestige ~ education + income, data = Prestige))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)   education      income \n   -6.84778     4.13744     0.00136 \n```\n\n\n:::\n:::\n\n\nNote: `solve(A, b)` is preferred over `solve(A) %*% b` — it avoids computing the full inverse, which is slower and less numerically stable.\n\n::: {#thm-ols}\n## The OLS Estimator\nThe OLS estimator $\\hat\\beta = (X'X)^{-1}X'y$ is the unique minimizer of $\\text{SSE}(\\beta) = (y - X\\beta)'(y - X\\beta)$ when $X'X$ is positive definite.\n:::\n\n### The second-order condition\n\nThe second derivative of $\\text{SSE}$ is $2X'X$. This is a minimum when $X'X$ is positive definite — all eigenvalues are positive:\n\n\n::: {.cell}\n\n```{.r .cell-code}\neigen(XtX)$values\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.53e+09 2.44e+03 5.83e+00\n```\n\n\n:::\n:::\n\n\nAll positive, confirming positive definiteness. If any eigenvalue were zero, $X'X$ would be singular and `solve()` would fail.\n\n## What collinearity does to $X'X$\n\nWhen a column of $X$ is a linear combination of others, $X'X$ loses rank:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add a redundant column: income2 = 2 * income\nX_bad <- cbind(X, 2 * Prestige$income)\n\nXtX_bad <- crossprod(X_bad)\ndet(XtX_bad)          # essentially zero\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\neigen(XtX_bad)$values  # last eigenvalue collapses\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.27e+10 2.44e+03 5.83e+00 4.55e-13\n```\n\n\n:::\n:::\n\n\nIn practice, *near*-collinearity (very small but nonzero eigenvalues) inflates standard errors without crashing `solve()`. The **condition number** — ratio of largest to smallest eigenvalue — measures how close to singular:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nevals <- eigen(XtX)$values\nc(largest = evals[1], smallest = evals[K], condition = evals[1] / evals[K])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  largest  smallest condition \n 6.53e+09  5.83e+00  1.12e+09 \n```\n\n\n:::\n:::\n\n\nR's `lm()` handles exact collinearity by dropping the redundant column:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(lm(prestige ~ education + income + I(2 * income), data = Prestige))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)     education        income I(2 * income) \n     -6.84778       4.13744       0.00136            NA \n```\n\n\n:::\n:::\n\n\n::: {.callout-warning}\n## Near-Collinearity Inflates Standard Errors\nWhen columns of $X$ are nearly linearly dependent, $X'X$ has a near-zero eigenvalue, making $(X'X)^{-1}$ very large. This inflates the variance of $\\hat\\beta$ without causing `solve()` to fail — standard errors balloon silently. Check the condition number of $X'X$ to detect this.\n:::\n\n## The projection matrix {#sec-projection-matrix}\n\nThe projection matrix $P = X(X'X)^{-1}X'$ maps any $n$-vector onto the column space of $X$. In two dimensions (our earlier example), it projected $\\mathbf{h}$ onto the line of $\\mathbf{t}$. With $K = 3$ regressors, it projects $\\mathbf{y}$ onto a 3-dimensional subspace of $\\mathbb{R}^n$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nP <- X %*% solve(XtX) %*% t(X)\ndim(P)  # n x n\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 102 102\n```\n\n\n:::\n\n```{.r .cell-code}\nmod <- lm(prestige ~ education + income, data = Prestige)\n```\n:::\n\n\n::: {#def-projection-matrix}\n## Projection (Hat) Matrix\nThe projection matrix $P = X(X'X)^{-1}X'$ maps any $n$-vector onto the column space of $X$. It is symmetric ($P' = P$) and idempotent ($P^2 = P$), with eigenvalues in $\\{0, 1\\}$ and $\\text{tr}(P) = K$.\n:::\n\nEvery property of $P$ corresponds to a matrix operation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# P*y = fitted values\nall.equal(as.vector(P %*% y), as.numeric(fitted(mod)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# Symmetric: t(P) = P\nall.equal(t(P), P)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# Idempotent: P %*% P = P (projecting twice = projecting once)\nall.equal(P %*% P, P)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# P*X = X (X is already in its own column space)\nall.equal(P %*% X, X, check.attributes = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n### What idempotency means for eigenvalues\n\nIf $Pv = \\lambda v$, then $P^2 v = \\lambda^2 v$. But $P^2 = P$, so $\\lambda^2 = \\lambda$, which means $\\lambda \\in \\{0, 1\\}$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\neig_P <- eigen(P)$values\ntable(round(eig_P, 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n 0  1 \n99  3 \n```\n\n\n:::\n:::\n\n\n$K$ eigenvalues equal 1 (the column space of $X$) and $n - K$ equal 0 (the null space). The trace counts the 1s:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(trace_P = tr(P), K = K)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntrace_P       K \n      3       3 \n```\n\n\n:::\n:::\n\n\n### Projection onto the intercept\n\nThe simplest projection is onto a column of ones: $P_1 = \\mathbf{1}(\\mathbf{1}'\\mathbf{1})^{-1}\\mathbf{1}' = \\frac{1}{n}\\mathbf{1}\\mathbf{1}'$. This projects every observation onto the sample mean:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nones <- rep(1, n)\nP1 <- outer(ones, ones) / n  # outer product: 1*1' / n\n\n# P1 * y = sample mean for every observation\nall.equal(as.vector(P1 %*% y), rep(mean(y), n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nEvery additional regressor refines this baseline: the full $P$ starts from the mean and adds the directions explained by the other columns of $X$.\n\n## The annihilator matrix\n\nThe annihilator $M = I_n - P$ projects onto the orthogonal complement — the part of $y$ that $X$ cannot explain:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM <- diag(n) - P\n\n# M*y = residuals\nall.equal(as.vector(M %*% y), as.numeric(resid(mod)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# Idempotent and symmetric\nall.equal(M %*% M, M)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(t(M), M)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# M annihilates X: M*X = 0\nmax(abs(M %*% X))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.82e-11\n```\n\n\n:::\n:::\n\n\n::: {#def-annihilator}\n## Annihilator Matrix\nThe annihilator $M = I_n - P$ projects onto the orthogonal complement of the column space of $X$. It satisfies $MX = 0$ (annihilates $X$), is idempotent and symmetric, and has $\\text{tr}(M) = n - K$.\n:::\n\nEigenvalues are complementary to $P$: $n - K$ ones and $K$ zeros:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(trace_M = tr(M), n_minus_K = n - K)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  trace_M n_minus_K \n       99        99 \n```\n\n\n:::\n:::\n\n\nThe **demeaning matrix** $M_1 = I - P_1$ is a special case — it subtracts the mean:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM1 <- diag(n) - P1\nall.equal(as.vector(M1 %*% y), as.numeric(y - mean(y)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n## Application: regression to the mean\n\nHere's a classic application of bivariate OLS. Galton noticed that children of unusually tall parents tend to be shorter than their parents — and children of unusually short parents tend to be taller. This \"regression to the mean\" is not a causal mechanism; it's a consequence of the BLP slope being less than 1 when the correlation is less than 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate parent-child heights (jointly normal)\nset.seed(307)\nn_ht <- 1000\nrho <- 0.5   # correlation between parent and child height\n\nlibrary(MASS)\nheights <- mvrnorm(n_ht, mu = c(68, 68),\n                   Sigma = matrix(c(9, rho * 9, rho * 9, 9), 2, 2))\nparent_ht <- heights[, 1]\nchild_ht <- heights[, 2]\n\n# OLS by matrix formula\nX_ht <- cbind(1, parent_ht)\nbeta_ht <- solve(crossprod(X_ht), crossprod(X_ht, child_ht))\nbeta_ht\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]\n          37.065\nparent_ht  0.456\n```\n\n\n:::\n:::\n\n\nThe slope is $\\hat\\beta_1 \\approx$ 0.46, less than 1. So a parent who is 1 inch above average has a child who is only about 0.46 inches above average — regression toward the mean.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_ht <- data.frame(parent = parent_ht, child = child_ht)\n\nggplot(df_ht, aes(parent, child)) +\n  geom_point(alpha = 0.15, size = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"steelblue\", linewidth = 1.2) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  annotate(\"text\", x = 74, y = 74.5, label = \"slope = 1 (no regression)\", alpha = 0.5) +\n  labs(x = \"Parent height (in)\", y = \"Child height (in)\",\n       title = \"Regression to the mean\",\n       subtitle = paste0(\"Slope = \", round(beta_ht[2], 2),\n                         \" < 1: children of tall parents are less extreme\")) +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](ch03-ols_files/figure-html/regression-to-mean-plot-1.png){width=672}\n:::\n:::\n\n\nThe tallest parents (above the 95th percentile) have children who are closer to the mean:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntall <- parent_ht > quantile(parent_ht, 0.95)\nc(parent_mean = mean(parent_ht[tall]),\n  child_mean = mean(child_ht[tall]),\n  difference = mean(parent_ht[tall]) - mean(child_ht[tall]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparent_mean  child_mean  difference \n      74.19       71.15        3.04 \n```\n\n\n:::\n:::\n\n\n## Residuals vs. disturbances\n\nThe true model is $y = X\\beta + e$ where $e$ is unobservable. The residuals $\\hat{e} = My$ relate to the disturbances through:\n\n$$\\hat{e} = My = M(X\\beta + e) = \\underbrace{MX}_{= 0}\\beta + Me = Me$$\n\nLet's simulate to see this. We know $\\beta$ and $e$ because we generate the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(307)\nn_sim <- 100\nK_sim <- 2\nX_sim <- cbind(1, rnorm(n_sim))\nbeta_true <- c(2, 3)\ne_true <- rnorm(n_sim, sd = 2)\ny_sim <- X_sim %*% beta_true + e_true\n\n# Build M for this design\nP_sim <- X_sim %*% solve(crossprod(X_sim)) %*% t(X_sim)\nM_sim <- diag(n_sim) - P_sim\n\n# Residuals = M * disturbances\ne_hat <- as.vector(M_sim %*% y_sim)\nall.equal(e_hat, as.vector(M_sim %*% e_true))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# Residuals have smaller variance — M zeroes out K dimensions\nc(var_disturbances = var(e_true), var_residuals = var(e_hat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nvar_disturbances    var_residuals \n             4.5              4.5 \n```\n\n\n:::\n:::\n\n\n## Estimating $\\sigma^2$: the trace trick\n\nThe natural estimator $\\hat\\sigma^2 = \\hat{e}'\\hat{e}/n$ is biased downward because $\\hat{e}'\\hat{e} = e'Me \\leq e'e$ ($M$ is positive semi-definite). The unbiased estimator divides by $n - K$.\n\nThe proof is a chain of matrix operations. Every step translates to R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 1: e'Me is a scalar = its own trace\nscalar_form <- as.numeric(t(e_true) %*% M_sim %*% e_true)\ntrace_form <- tr(M_sim %*% tcrossprod(e_true))  # tr(M * ee')\n\nc(scalar = scalar_form, trace = trace_form)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nscalar  trace \n   445    445 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Step 2: E[ee'] = sigma^2 * I, so E[tr(Mee')] = sigma^2 * tr(M)\n# tr(M) = n - K, so E[e'hat * e'hat] = sigma^2 * (n - K)\nc(trace_M = tr(M_sim), n_minus_K = n_sim - K_sim)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  trace_M n_minus_K \n       98        98 \n```\n\n\n:::\n:::\n\n\n::: {.callout-note}\n## The $n - K$ Divisor\nThe unbiased variance estimator divides by $n - K$ (not $n$) because the residuals live in an $(n - K)$-dimensional subspace. The $K$ \"lost\" dimensions are consumed by estimating $\\hat\\beta$. This is the matrix version of Bessel's correction.\n:::\n\nThis is why the unbiased estimator is $s_e^2 = \\hat{e}'\\hat{e}/(n-K)$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Back to the Prestige data\ne_hat_prestige <- resid(mod)\n\nsigma2_biased <- as.numeric(crossprod(e_hat_prestige)) / n\nsigma2_unbiased <- as.numeric(crossprod(e_hat_prestige)) / (n - K)\n\nc(biased = sigma2_biased,\n  unbiased = sigma2_unbiased,\n  R_sigma2 = sigma(mod)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  biased unbiased R_sigma2 \n    59.2     61.0     61.0 \n```\n\n\n:::\n:::\n\n\nThe underestimation shows up in the eigenvalues of $M$: $n - K$ eigenvalues are 1, and $K$ are 0. The residuals live in an $(n-K)$-dimensional subspace:\n\n\n::: {.cell}\n\n```{.r .cell-code}\neig_M <- round(eigen(M)$values, 10)\nc(eigenvalues_equal_1 = sum(eig_M == 1),\n  eigenvalues_equal_0 = sum(eig_M == 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\neigenvalues_equal_1 eigenvalues_equal_0 \n                 99                   3 \n```\n\n\n:::\n:::\n\n\n## Variance of $\\hat\\beta$: building $s_e^2(X'X)^{-1}$\n\nUnder homoskedasticity, $\\text{Var}(\\hat\\beta|X) = \\sigma^2(X'X)^{-1}$. Each piece is a matrix operation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 1: (X'X)^{-1}\nXtX_inv <- solve(XtX)\nXtX_inv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          [,1]      [,2]      [,3]\n[1,]  1.70e-01 -1.64e-02  2.35e-06\n[2,] -1.64e-02  2.00e-03 -7.41e-07\n[3,]  2.35e-06 -7.41e-07  8.24e-10\n```\n\n\n:::\n\n```{.r .cell-code}\n# Step 2: multiply by s_e^2\nvcov_manual <- sigma(mod)^2 * XtX_inv\n\n# Step 3: compare to R\nall.equal(vcov_manual, vcov(mod), check.attributes = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nStandard errors are the square roots of the diagonal:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nse_manual <- sqrt(diag(vcov_manual))\nse_R <- coef(summary(mod))[, \"Std. Error\"]\n\ncbind(manual = se_manual, R = se_R)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              manual        R\n(Intercept) 3.218977 3.218977\neducation   0.348912 0.348912\nincome      0.000224 0.000224\n```\n\n\n:::\n:::\n\n\n### Why $(X'X)^{-1}$ determines precision\n\nThe eigenvalues of $(X'X)^{-1}$ are the reciprocals of those of $X'X$. Large eigenvalues of $X'X$ (strong signal) become small eigenvalues of $(X'X)^{-1}$ (precise estimates). Near-collinearity creates a tiny eigenvalue in $X'X$, which inflates variance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\neig_XtX <- eigen(XtX)$values\neig_inv <- eigen(XtX_inv)$values\n\ncbind(XtX = eig_XtX, XtX_inv = eig_inv, product = eig_XtX * eig_inv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          XtX  XtX_inv  product\n[1,] 6.53e+09 1.71e-01 1.12e+09\n[2,] 2.44e+03 4.10e-04 1.00e+00\n[3,] 5.83e+00 1.53e-10 8.93e-10\n```\n\n\n:::\n:::\n\n\nThe products are all 1: the eigenvalues invert exactly.\n\n## Application: the Prestige regression\n\nLet's interpret the full regression. Education and income both predict occupational prestige:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = prestige ~ education + income, data = Prestige)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.404  -5.331   0.015   4.980  17.689 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6.847779   3.218977   -2.13    0.036 *  \neducation    4.137444   0.348912   11.86  < 2e-16 ***\nincome       0.001361   0.000224    6.07  2.4e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.81 on 99 degrees of freedom\nMultiple R-squared:  0.798,\tAdjusted R-squared:  0.794 \nF-statistic:  196 on 2 and 99 DF,  p-value: <2e-16\n```\n\n\n:::\n:::\n\n\nThe coefficient on education (4.1) says: holding income constant, one additional year of average education is associated with about 4.1 points more prestige. The coefficient on income (0.001) is small in magnitude because income is in dollars — a \\$1,000 increase predicts about 1.4 points.\n\nLet's see which occupations the model fits well and poorly, using the projection and annihilator:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nPrestige$fitted <- as.vector(P %*% y)\nPrestige$resid <- as.vector(M %*% y)\n\n# Largest positive residuals: more prestige than education+income predict\nhead(Prestige[order(-Prestige$resid), c(\"education\", \"income\", \"prestige\", \"fitted\", \"resid\")], 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    education income prestige fitted resid\nfarmers                  6.84   3643     44.1   26.4  17.7\nelectronic.workers       8.76   3942     50.8   34.8  16.0\nphysio.therapsts        13.62   5092     72.1   56.4  15.7\nmedical.technicians     12.79   5180     67.5   53.1  14.4\nnurses                  12.46   4614     64.7   51.0  13.7\n```\n\n\n:::\n\n```{.r .cell-code}\n# Largest negative residuals: less prestige than predicted\nhead(Prestige[order(Prestige$resid), c(\"education\", \"income\", \"prestige\", \"fitted\", \"resid\")], 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                          education income prestige fitted resid\nnewsboys                       9.62    918     14.8   34.2 -19.4\ncollectors                    11.20   4741     29.4   45.9 -16.5\nfile.clerks                   12.09   3016     32.7   47.3 -14.6\nservice.station.attendant      9.93   2370     23.3   37.5 -14.2\nbartenders                     8.50   3930     20.2   33.7 -13.5\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(Prestige, aes(fitted, resid)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_text(data = Prestige[abs(Prestige$resid) > 15, ],\n            aes(label = rownames(Prestige)[abs(Prestige$resid) > 15]),\n            hjust = -0.1, size = 3) +\n  labs(x = \"Fitted values (Py)\", y = \"Residuals (My)\",\n       title = \"Prestige: fitted vs. residuals\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](ch03-ols_files/figure-html/prestige-residual-plot-1.png){width=672}\n:::\n:::\n\n\n## ANOVA as inner products\n\nThe decomposition $y = \\hat{y} + \\hat{e}$ is orthogonal. In matrix terms, $\\hat{y}'\\hat{e} = (Py)'(My) = y'PMy = 0$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.numeric(crossprod(fitted(mod), resid(mod)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.07e-13\n```\n\n\n:::\n:::\n\n\nAfter centering, the inner products give sums of squares:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# SST = ||M1 * y||^2  (total variation around the mean)\nSST <- as.numeric(crossprod(M1 %*% y))\n\n# SSR = ||(P - P1) * y||^2  (variation explained by regressors beyond the mean)\nSSR <- as.numeric(crossprod(fitted(mod) - mean(y)))\n\n# SSE = ||M * y||^2  (unexplained variation)\nSSE <- as.numeric(crossprod(resid(mod)))\n\nc(SST = SST, SSR_plus_SSE = SSR + SSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         SST SSR_plus_SSE \n       29895        29895 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Cross term is zero when X includes a constant\nas.numeric(crossprod(fitted(mod) - mean(y), resid(mod)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8.74e-13\n```\n\n\n:::\n:::\n\n\n## $R^2$\n\n$R^2 = \\text{SSR}/\\text{SST} = 1 - \\text{SSE}/\\text{SST}$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(R2 = SSR / SST,\n  R2_alt = 1 - SSE / SST,\n  R_reports = summary(mod)$r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       R2    R2_alt R_reports \n    0.798     0.798     0.798 \n```\n\n\n:::\n:::\n\n\nAdding regressors can only increase $R^2$, even if the variable is noise. The adjusted $R^2$ penalizes for $K$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nPrestige$noise <- rnorm(n)\nmod_noise <- lm(prestige ~ education + income + noise, data = Prestige)\n\nc(R2_original = summary(mod)$r.squared,\n  R2_with_noise = summary(mod_noise)$r.squared,\n  adj_R2_original = summary(mod)$adj.r.squared,\n  adj_R2_with_noise = summary(mod_noise)$adj.r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      R2_original     R2_with_noise   adj_R2_original adj_R2_with_noise \n            0.798             0.799             0.794             0.792 \n```\n\n\n:::\n:::\n\n\nRaw $R^2$ ticks up; adjusted $R^2$ drops — correctly penalizing the useless variable.\n\n$R^2$ measures descriptive fit, not causal validity. Typical values: cross-sectional micro data $\\approx 0.2$--$0.4$, aggregate time series $\\approx 0.7$--$0.9$.\n\n## Naming conventions: a warning\n\nDifferent textbooks use SSE and SSR with opposite meanings. In this course (following Hansen), SSR is \"regression\" (explained) and SSE is \"error\" (unexplained). Some texts reverse these. The math is always $\\text{SST} = \\text{explained} + \\text{unexplained}$.\n\n## Summary\n\nThe OLS estimator is a sequence of matrix operations:\n\n| Math | R | What it does |\n|------|---|-------------|\n| $X'X$ | `crossprod(X)` | Gram matrix of regressors |\n| $(X'X)^{-1}$ | `solve(crossprod(X))` | Inverse |\n| $\\hat\\beta = (X'X)^{-1}X'y$ | `solve(crossprod(X), crossprod(X, y))` | OLS coefficients |\n| $P = X(X'X)^{-1}X'$ | `X %*% solve(crossprod(X)) %*% t(X)` | Projection (hat matrix) |\n| $M = I - P$ | `diag(n) - P` | Annihilator |\n| $\\text{tr}(M)$ | `sum(diag(M))` | Degrees of freedom ($n - K$) |\n| eigenvalues of $P$ | `eigen(P)$values` | All 0 or 1 |\n| $s_e^2(X'X)^{-1}$ | `sigma(mod)^2 * solve(crossprod(X))` | Variance-covariance of $\\hat\\beta$ |\n| $\\text{SE}(\\hat\\beta_k)$ | `sqrt(diag(vcov(mod)))` | Standard errors |\n\nKey facts:\n\n- OLS is **projection**: $\\hat{y} = Py$ is the closest point to $y$ in the column space of $X$, and $\\hat{e} = My$ is the orthogonal residual.\n- $P$ and $M$ are **symmetric** and **idempotent**, with eigenvalues in $\\{0, 1\\}$.\n- $\\text{tr}(P) = K$ and $\\text{tr}(M) = n - K$ count dimensions.\n- The **trace trick** proves $s_e^2$ is unbiased: $\\mathbb{E}[e'Me] = \\sigma^2 \\text{tr}(M) = \\sigma^2(n-K)$.\n- Eigenvalues of $(X'X)^{-1}$ are reciprocals of those of $X'X$: near-collinearity inflates variance.\n- **Regression to the mean** is a consequence of $\\hat\\beta_1 < 1$ when $|\\rho| < 1$.\n\nNext: [Sensitivity and Leverage](ch04-sensitivity.qmd) — the Frisch-Waugh-Lovell theorem, partial $R^2$, and influential observations.\n",
    "supporting": [
      "ch03-ols_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}