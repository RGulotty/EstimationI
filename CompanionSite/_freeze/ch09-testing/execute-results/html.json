{
  "hash": "7292bbbd346caabb382b2de78152d0de",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"9. Hypothesis Testing\"\nsubtitle: \"The F-test, the test trinity, multiple testing, and power\"\n---\n\nChapter 8 gave us sandwich standard errors and bootstrap confidence intervals. This chapter puts them to work: we test hypotheses, build confidence regions, guard against false discoveries, and ask whether our sample is large enough to detect effects we care about.\n\nThe emphasis is on *doing* --- running tests in R, interpreting the output correctly, and avoiding common pitfalls. We organize around applied questions:\n\n1. **How do I test whether a coefficient is zero?** (t-tests, robust Wald)\n2. **How do I test multiple restrictions at once?** (F-test, joint Wald)\n3. **When do different tests disagree?** (Wald, Score, F)\n4. **How do I get confidence sets for nonlinear parameters?** (test inversion, Fieller)\n5. **What if I test many hypotheses?** (Bonferroni, Holm)\n6. **How large a sample do I need?** (power analysis)\n\nWe use the `Salaries` dataset from `carData` throughout: salaries of 397 U.S. professors, with rank, discipline, years since PhD, years of service, and sex.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(estimatr)\nlibrary(car)\nlibrary(carData)\noptions(digits = 4)\n\ndata(Salaries)\n```\n:::\n\n\n## Testing a single coefficient\n\nWe start with a wage equation and ask: controlling for rank, discipline, and experience, does sex predict salary?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- lm(salary ~ rank + discipline + yrs.since.phd + yrs.service + sex,\n          data = Salaries)\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = salary ~ rank + discipline + yrs.since.phd + yrs.service + \n    sex, data = Salaries)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-65248 -13211  -1775  10384  99592 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      65955       4589   14.37  < 2e-16 ***\nrankAssocProf    12908       4145    3.11    0.002 ** \nrankProf         45066       4238   10.63  < 2e-16 ***\ndisciplineB      14418       2343    6.15  1.9e-09 ***\nyrs.since.phd      535        241    2.22    0.027 *  \nyrs.service       -490        212   -2.31    0.021 *  \nsexMale           4784       3859    1.24    0.216    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22500 on 390 degrees of freedom\nMultiple R-squared:  0.455,\tAdjusted R-squared:  0.446 \nF-statistic: 54.2 on 6 and 390 DF,  p-value: <2e-16\n```\n\n\n:::\n:::\n\n\nThe `summary()` output gives t-statistics and p-values, but these assume homoskedasticity. The robust version uses sandwich standard errors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoeftest(mod, vcov. = vcovHC(mod, type = \"HC1\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      65955       2896   22.78  < 2e-16 ***\nrankAssocProf    12908       2206    5.85  1.0e-08 ***\nrankProf         45066       3285   13.72  < 2e-16 ***\ndisciplineB      14418       2316    6.22  1.2e-09 ***\nyrs.since.phd      535        312    1.71    0.088 .  \nyrs.service       -490        307   -1.60    0.111    \nsexMale           4784       2396    2.00    0.047 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nThese are the same tests, built from the same Wald statistic $W = (\\hat{\\beta}_j / \\widehat{\\text{se}}_j)^2$. The only difference is the denominator. When the two disagree, trust the robust version --- it doesn't assume $\\text{Var}(e_i \\mid X_i)$ is constant.\n\nYou can also use `linearHypothesis()` for the same test, which makes the Wald structure explicit:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinearHypothesis(mod, \"sexMale = 0\", vcov. = vcovHC(mod, type = \"HC1\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\nsexMale = 0\n\nModel 1: restricted model\nModel 2: salary ~ rank + discipline + yrs.since.phd + yrs.service + sex\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df    F Pr(>F)  \n1    391                 \n2    390  1 3.99  0.047 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nThis tests $H_0: \\beta_{\\text{Male}} = 0$ against $H_1: \\beta_{\\text{Male}} \\neq 0$ using the robust covariance matrix. The Chisq column is the Wald statistic; divide by 1 (one restriction) and you get the squared t-statistic.\n\n### Testing against a non-zero value\n\nSuppose theory predicts a gender gap of \\$5,000. The null is $H_0: \\beta_{\\text{Male}} = 5000$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinearHypothesis(mod, \"sexMale = 5000\", vcov. = vcovHC(mod, type = \"HC1\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\nsexMale = 5000\n\nModel 1: restricted model\nModel 2: salary ~ rank + discipline + yrs.since.phd + yrs.service + sex\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df    F Pr(>F)\n1    391               \n2    390  1 0.01   0.93\n```\n\n\n:::\n:::\n\n\nThe test is not significant if the confidence interval from Chapter 8 already contains \\$5,000. Test and CI carry exactly the same information --- they are duals.\n\n\n## Testing multiple restrictions: the F-test\n\nIndividual t-tests tell you about one coefficient at a time. But sometimes you need a *joint* test. Does rank matter at all? That means testing two coefficients simultaneously (since rank is a three-level factor with two dummies):\n\n$$H_0: \\beta_{\\text{AssocProf}} = \\beta_{\\text{Prof}} = 0$$\n\n### By hand: comparing two regressions\n\nThe F-test compares the fit of an **unrestricted** model to a **restricted** model that imposes $H_0$:\n\n$$F = \\frac{(\\text{SSE}_R - \\text{SSE}_U) / q}{\\text{SSE}_U / (n - k)}$$\n\nwhere $q$ is the number of restrictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Unrestricted model (full)\nmod_U <- mod\n\n## Restricted model: drop rank\nmod_R <- lm(salary ~ discipline + yrs.since.phd + yrs.service + sex,\n            data = Salaries)\n\n## Manual F-stat\nSSE_U <- sum(resid(mod_U)^2)\nSSE_R <- sum(resid(mod_R)^2)\nq <- 2                              # two restrictions\nn <- nobs(mod_U)\nk <- length(coef(mod_U))\n\nF_stat <- ((SSE_R - SSE_U) / q) / (SSE_U / (n - k))\np_val  <- 1 - pf(F_stat, q, n - k)\n\ncat(\"F =\", round(F_stat, 2), \"  df1 =\", q, \"  df2 =\", n - k,\n    \"  p =\", format.pval(p_val), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nF = 68.41   df1 = 2   df2 = 390   p = <2e-16 \n```\n\n\n:::\n:::\n\n\n### The easy way: `anova()` and `linearHypothesis()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Nested model comparison\nanova(mod_R, mod_U)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: salary ~ discipline + yrs.since.phd + yrs.service + sex\nModel 2: salary ~ rank + discipline + yrs.since.phd + yrs.service + sex\n  Res.Df      RSS Df Sum of Sq    F Pr(>F)    \n1    392 2.68e+11                             \n2    390 1.98e+11  2  6.95e+10 68.4 <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Same test via linearHypothesis (with robust SEs)\nlinearHypothesis(mod, c(\"rankAssocProf = 0\", \"rankProf = 0\"),\n                 vcov. = vcovHC(mod, type = \"HC1\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\nrankAssocProf = 0\nrankProf = 0\n\nModel 1: restricted model\nModel 2: salary ~ rank + discipline + yrs.since.phd + yrs.service + sex\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df   F Pr(>F)    \n1    392                  \n2    390  2 109 <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nNote that `anova()` uses the classical (homoskedastic) F-test, while `linearHypothesis()` with a robust `vcov.` gives the heteroskedasticity-robust Wald test. In this example they agree; with strongly heteroskedastic data they might not.\n\n### Equality constraints\n\nDoes the return to years since PhD equal the return to years of service? This is $H_0: \\beta_{\\text{yrs.phd}} = \\beta_{\\text{yrs.service}}$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinearHypothesis(mod, \"yrs.since.phd = yrs.service\",\n                 vcov. = vcovHC(mod, type = \"HC1\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\nyrs.since.phd - yrs.service = 0\n\nModel 1: restricted model\nModel 2: salary ~ rank + discipline + yrs.since.phd + yrs.service + sex\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df    F Pr(>F)  \n1    391                 \n2    390  1 2.92  0.088 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nThis works because `linearHypothesis()` can test *any* linear restriction $\\mathbf{R}\\boldsymbol{\\beta} = \\mathbf{r}$.  Under the hood it constructs the restriction matrix $\\mathbf{R}$ and computes:\n$$W = (\\mathbf{R}\\hat{\\boldsymbol{\\beta}} - \\mathbf{r})' [\\mathbf{R}\\hat{\\mathbf{V}}\\mathbf{R}']^{-1} (\\mathbf{R}\\hat{\\boldsymbol{\\beta}} - \\mathbf{r}) \\;\\xrightarrow{d}\\; \\chi^2_q$$ {#eq-wald}\n\n::: {#thm-wald}\n## Wald Statistic\nFor testing $H_0: R\\beta = r$, the Wald statistic is $W = (R\\hat\\beta - r)'[R\\hat{V}R']^{-1}(R\\hat\\beta - r) \\xrightarrow{d} \\chi^2_q$ under $H_0$, where $q$ is the number of restrictions. With robust $\\hat{V}$, the test is valid under heteroskedasticity.\n:::\n\n\n## The test trinity: Wald, Score, and F\n\nThree classical approaches test the same null $H_0: \\mathbf{R}\\boldsymbol{\\beta} = \\mathbf{r}$:\n\n| Test | Estimates from | Measures |\n|------|----------------|----------|\n| **Wald** | Unrestricted model | How far $\\hat{\\boldsymbol{\\beta}}$ is from $H_0$ |\n| **Score (LM)** | Restricted model | Whether the objective function wants to move away from $H_0$ |\n| **F / LR** | Both models | How much the fit worsens when we impose $H_0$ |\n\nFor linear restrictions in a normal linear model, they are **monotone transformations of each other** and always give the same accept/reject decision.  Let's verify:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Test: rank doesn't matter (q = 2 restrictions)\n## Wald (homoskedastic, for comparability)\nW <- linearHypothesis(mod_U, c(\"rankAssocProf = 0\", \"rankProf = 0\"))\nW_stat <- W$F[2] * q  # linearHypothesis reports F; multiply by q for chi-sq scale\n\n## F from anova\nFtest <- anova(mod_R, mod_U)\n\n## Score test (LM): use restricted residuals\n## S = n * (1 - SSE_U / SSE_R) under homoskedasticity\nS <- n * (1 - SSE_U / SSE_R)\n\ncat(\"Wald statistic (chi-sq scale):\", round(W_stat, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWald statistic (chi-sq scale): 136.8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"F statistic (x q):           \", round(Ftest$F[2] * q, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nF statistic (x q):            136.8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Score statistic:             \", round(S, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nScore statistic:              103.1 \n```\n\n\n:::\n:::\n\n\nThe three numbers are close but not identical --- they use different variance estimates (unrestricted $s^2$, restricted $\\tilde{\\sigma}^2$, or pooled). Asymptotically they converge. Under normality, we have the [exact distributions](ch06-small-sample.qmd#thm-exact-distributions) that make the F-test slightly more conservative in finite samples, which is why `anova()` uses $F_{q, n-k}$ critical values instead of $\\chi^2_q / q$.\n\n### When do they diverge?\n\nFor **non-normal** models, the trinity can disagree because the three test statistics are no longer monotone transformations of each other. Here is an example using a joint test in the probit model from Chapter 7:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(Mroz)\nprobit <- glm(lfp ~ k5 + k618 + age + wc + lwg + inc,\n              family = binomial(link = \"probit\"), data = Mroz)\n\n## Test H0: k5 = 0, k618 = 0 (joint, q = 2)\nprobit_r <- glm(lfp ~ age + wc + lwg + inc,\n                family = binomial(link = \"probit\"), data = Mroz)\n\n## Wald\nW_probit <- linearHypothesis(probit, c(\"k5 = 0\", \"k618 = 0\"))\n\n## LR\nLR <- -2 * as.numeric(logLik(probit_r) - logLik(probit))\n\n## Score (Rao)\nS_probit <- anova(probit_r, probit, test = \"Rao\")\n\ncat(\"Wald chi-sq: \", round(W_probit$Chisq[2], 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWald chi-sq:  58.45 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"LR chi-sq:   \", round(LR, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLR chi-sq:    65.77 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Score chi-sq:\", round(S_probit$Rao[2], 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nScore chi-sq: 65.03 \n```\n\n\n:::\n:::\n\n\nThe three statistics differ noticeably: the Wald statistic (58.5) is substantially smaller than the LR (65.8) and Score (65.0). All three reject decisively here, but in borderline cases they could lead to different conclusions. The LR test tends to be the most reliable in nonlinear models because it directly measures the change in the log-likelihood.\n\n\n## Confidence regions and test inversion\n\nA 95% confidence interval is the set of values $\\theta_0$ that would *not be rejected* at the 5% level:\n\n$$\\hat{C} = \\{\\theta_0 : |T(\\theta_0)| \\leq 1.96\\}$$\n\nThis is **test inversion**. For a single linear coefficient, test inversion gives the familiar $\\hat{\\beta} \\pm 1.96 \\cdot \\widehat{\\text{se}}$. But for *nonlinear* functions of parameters, test inversion is more reliable than the delta method.\n\n::: {.callout-note}\n## Test Inversion Gives Confidence Sets\nA 95% confidence set is exactly the set of parameter values not rejected at the 5% level. For linear parameters, this gives the familiar $\\hat\\beta \\pm 1.96 \\cdot \\text{SE}$. For nonlinear functions, Fieller's method (test inversion over a grid) is more reliable than the delta method.\n:::\n\n### Confidence ellipses for two coefficients\n\nFor two parameters tested jointly, the confidence region is an **ellipse** --- the set of $(\\beta_1, \\beta_2)$ values consistent with the Wald test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Joint confidence region for the two experience variables\nconfidenceEllipse(mod, which.coef = c(\"yrs.since.phd\", \"yrs.service\"),\n                  vcov. = vcovHC(mod, type = \"HC1\"),\n                  main = \"95% joint confidence region\",\n                  xlab = \"Coefficient on yrs.since.phd\",\n                  ylab = \"Coefficient on yrs.service\",\n                  col = \"steelblue\", lwd = 2)\nabline(h = 0, lty = 2, col = \"gray50\")\nabline(v = 0, lty = 2, col = \"gray50\")\n## Add the 45-degree line for equality\nabline(a = 0, b = 1, lty = 3, col = \"coral\", lwd = 1.5)\nlegend(\"topright\", legend = c(\"95% joint region\", \"equality line\"),\n       col = c(\"steelblue\", \"coral\"), lty = c(1, 3), lwd = c(2, 1.5))\n```\n\n::: {.cell-output-display}\n![](ch09-testing_files/figure-html/confidence-ellipse-1.png){width=672}\n:::\n:::\n\n\nThe dashed lines at zero show the individual null values. The coral line shows where the two coefficients are equal. If the ellipse crosses a line, the corresponding null cannot be rejected.\n\n### Test inversion for a nonlinear parameter: Fieller's method\n\nSuppose we want a confidence interval for the **ratio** $\\theta = \\beta_{\\text{Prof}} / \\beta_{\\text{AssocProf}}$ --- how many times larger is the full professor salary premium than the associate professor premium? The delta method (Chapter 8) gives a quick CI, but it can be unreliable when the denominator is imprecisely estimated.\n\n**Fieller's trick**: rewrite $\\theta = \\beta_1 / \\beta_2$ as the linear restriction $\\beta_1 - \\theta \\beta_2 = 0$, then *invert* the Wald test over a grid of $\\theta$ values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb <- coef(mod)\nV <- vcovHC(mod, type = \"HC1\")\n\n## Delta method CI for comparison\ndm <- deltaMethod(mod, \"rankProf / rankAssocProf\",\n                  vcov. = vcovHC(mod, type = \"HC1\"))\ndm_ci <- c(dm$Estimate - 1.96 * dm$SE, dm$Estimate + 1.96 * dm$SE)\n\n## Grid search: invert the Wald test\ntheta_grid <- seq(0, 8, by = 0.01)\nidx_prof <- which(names(b) == \"rankProf\")\nidx_assoc <- which(names(b) == \"rankAssocProf\")\n\nin_CI <- sapply(theta_grid, function(th) {\n  r_vec <- numeric(length(b))\n  r_vec[idx_prof] <- 1\n  r_vec[idx_assoc] <- -th\n  num <- (sum(r_vec * b))^2\n  den <- t(r_vec) %*% V %*% r_vec\n  num / den <= qchisq(0.95, 1)\n})\nfieller_ci <- range(theta_grid[in_CI])\n\ncat(\"Point estimate:  \", round(dm$Estimate, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPoint estimate:   3.491 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Delta method CI: \", round(dm_ci, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDelta method CI:  2.602 4.381 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Fieller CI:      \", round(fieller_ci, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFieller CI:       2.81 4.79 \n```\n\n\n:::\n:::\n\n\nHere the two CIs are similar because the denominator ($\\beta_{\\text{AssocProf}}$) is precisely estimated. The ratio tells us that the full professor premium is about 3.5 times the associate professor premium.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Plot the Wald statistic over theta\nwald_vals <- sapply(theta_grid, function(th) {\n  r_vec <- numeric(length(b))\n  r_vec[idx_prof] <- 1\n  r_vec[idx_assoc] <- -th\n  num <- (sum(r_vec * b))^2\n  den <- c(t(r_vec) %*% V %*% r_vec)\n  num / den\n})\n\ndf_fieller <- data.frame(theta = theta_grid, W = wald_vals)\nggplot(df_fieller, aes(theta, W)) +\n  geom_line(color = \"steelblue\", linewidth = 0.8) +\n  geom_hline(yintercept = qchisq(0.95, 1), linetype = \"dashed\", color = \"coral\") +\n  annotate(\"text\", x = 6.5, y = qchisq(0.95, 1) + 0.5, label = \"95% critical value\",\n           color = \"coral\") +\n  coord_cartesian(ylim = c(0, 15)) +\n  labs(title = \"Test inversion: Wald statistic as a function of θ₀\",\n       subtitle = \"The Fieller CI is the set of θ₀ values below the dashed line\",\n       x = expression(theta[0] == beta[Prof] / beta[AssocProf]),\n       y = \"Wald statistic\")\n```\n\n::: {.cell-output-display}\n![](ch09-testing_files/figure-html/fieller-plot-1.png){width=768}\n:::\n:::\n\n\n### When Fieller and the delta method disagree\n\nThe real value of Fieller's method appears when the **denominator is imprecise**. Consider $\\theta = \\beta_{\\text{yrs.phd}} / \\beta_{\\text{yrs.service}}$ --- the ratio of the two experience coefficients. Both are individually insignificant:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Experience coefficients are imprecise\ncat(\"yrs.since.phd: estimate =\", round(b[\"yrs.since.phd\"]),\n    \"  SE =\", round(sqrt(V[\"yrs.since.phd\", \"yrs.since.phd\"])), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nyrs.since.phd: estimate = 535   SE = 312 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"yrs.service:   estimate =\", round(b[\"yrs.service\"]),\n    \"  SE =\", round(sqrt(V[\"yrs.service\", \"yrs.service\"])), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nyrs.service:   estimate = -490   SE = 307 \n```\n\n\n:::\n\n```{.r .cell-code}\n## Delta method gives a finite CI\ndm2 <- deltaMethod(mod, \"yrs.since.phd / yrs.service\",\n                   vcov. = vcovHC(mod, type = \"HC1\"))\ncat(\"\\nDelta method CI:\", round(c(dm2$Estimate - 1.96 * dm2$SE,\n                                  dm2$Estimate + 1.96 * dm2$SE), 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDelta method CI: -1.75 -0.43 \n```\n\n\n:::\n\n```{.r .cell-code}\n## Fieller: check a wide grid\ntheta_wide <- seq(-50, 50, by = 0.1)\nidx_phd <- which(names(b) == \"yrs.since.phd\")\nidx_svc <- which(names(b) == \"yrs.service\")\nin_CI2 <- sapply(theta_wide, function(th) {\n  r_vec <- numeric(length(b))\n  r_vec[idx_phd] <- 1\n  r_vec[idx_svc] <- -th\n  num <- (sum(r_vec * b))^2\n  den <- c(t(r_vec) %*% V %*% r_vec)\n  num / den <= qchisq(0.95, 1)\n})\n\n## Check if CI covers entire grid (unbounded)\nif (all(in_CI2)) {\n  cat(\"Fieller CI: (-∞, +∞)  [unbounded!]\\n\")\n} else if (any(in_CI2)) {\n  cat(\"Fieller CI:\", range(theta_wide[in_CI2]), \"\\n\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFieller CI: (-∞, +∞)  [unbounded!]\n```\n\n\n:::\n:::\n\n\nThe delta method reports a finite CI of width ~1.3, but the Fieller CI is **unbounded** --- the entire real line. This is the correct answer: since $\\beta_{\\text{yrs.service}}$ is not significantly different from zero, the ratio could be anything. The delta method's finite interval is overconfident.\n\n\n## Multiple testing {#sec-multiple-testing}\n\nWhen you look at many coefficients, some will be \"significant\" by chance.\n\n### Simulation: false discoveries under the null\n\nTo make this concrete: generate 20 pure-noise regressors, test each at 5%, and count how many reject:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(303)\nn <- 200\nk_noise <- 20\nY <- rnorm(n)\nX_noise <- matrix(rnorm(n * k_noise), nrow = n)\ncolnames(X_noise) <- paste0(\"X\", 1:k_noise)\ndat_noise <- data.frame(Y, X_noise)\n\nmod_noise <- lm(Y ~ ., data = dat_noise)\npvals <- summary(mod_noise)$coefficients[-1, 4]  # drop intercept\n\ncat(\"Number of regressors with p < 0.05:\", sum(pvals < 0.05), \"out of\", k_noise, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNumber of regressors with p < 0.05: 1 out of 20 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Significant variables:\", paste(names(pvals[pvals < 0.05]), collapse = \", \"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSignificant variables: X12 \n```\n\n\n:::\n:::\n\n\nUnder the global null, we *expect* $20 \\times 0.05 = 1$ false rejection on average. Repeat this many times to see the problem clearly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nB <- 5000\nn <- 200\nk_noise <- 20\n\nfalse_rejections <- replicate(B, {\n  Y <- rnorm(n)\n  X <- matrix(rnorm(n * k_noise), nrow = n)\n  pv <- summary(lm(Y ~ X))$coefficients[-1, 4]\n  sum(pv < 0.05)\n})\n\ncat(\"P(at least one false rejection):\", mean(false_rejections >= 1), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(at least one false rejection): 0.6204 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Expected:\", round(1 - (1 - 0.05)^k_noise, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected: 0.642 \n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(data.frame(rejections = false_rejections), aes(rejections)) +\n  geom_bar(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = 1, linetype = \"dashed\", color = \"coral\") +\n  labs(title = \"False rejections when all 20 nulls are true\",\n       subtitle = paste0(\"P(≥1 false rejection) = \",\n                         round(mean(false_rejections >= 1), 3)),\n       x = \"Number of rejections at α = 0.05\",\n       y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](ch09-testing_files/figure-html/fwer-simulation-1.png){width=672}\n:::\n:::\n\n\nWith 20 tests at 5%, you have about a 64% chance of at least one false rejection. This is the **familywise error rate (FWER)** problem.\n\n::: {.callout-warning}\n## Multiple Testing Inflates the Familywise Error Rate\nWith $k$ independent tests at level $\\alpha$, the probability of at least one false rejection is $1 - (1 - \\alpha)^k$. With 20 tests at 5%, this exceeds 60%. Use Holm's correction (uniformly more powerful than Bonferroni) to control the FWER.\n:::\n\n### Corrections: Bonferroni and Holm\n\nThe **Bonferroni correction** rejects the $j$th hypothesis only if $p_j < \\alpha / k$. It controls the FWER for any dependence structure:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Back to our salary model\npvals_salary <- summary(mod)$coefficients[-1, 4]\n\ncorrections <- data.frame(\n  raw = round(pvals_salary, 4),\n  bonferroni = round(p.adjust(pvals_salary, method = \"bonferroni\"), 4),\n  holm = round(p.adjust(pvals_salary, method = \"holm\"), 4)\n)\ncorrections\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 raw bonferroni   holm\nrankAssocProf 0.0020     0.0119 0.0079\nrankProf      0.0000     0.0000 0.0000\ndisciplineB   0.0000     0.0000 0.0000\nyrs.since.phd 0.0270     0.1619 0.0643\nyrs.service   0.0214     0.1286 0.0643\nsexMale       0.2158     1.0000 0.2158\n```\n\n\n:::\n:::\n\n\n**Holm's method** is uniformly more powerful than Bonferroni while still controlling the FWER. It works by ordering the p-values from smallest to largest and comparing the $j$th smallest to $\\alpha / (k - j + 1)$. Use Holm whenever you would use Bonferroni.\n\n### When to worry about multiple testing\n\nMultiple testing corrections matter when you:\n\n- Examine many coefficients and report the \"most significant\"\n- Try many specifications and present the one that \"works\"\n- Test across subgroups (by age, gender, region, ...)\n- Use stepwise selection procedures\n\nThey matter *less* when you have a small number of pre-specified hypotheses motivated by theory.\n\n\n## Power: can you detect the effect?\n\nA test's **power** is the probability it rejects when $H_0$ is false:\n\n$$\\text{Power}(\\theta) = P[\\text{Reject } H_0 \\mid \\theta \\neq \\theta_0]$$\n\nPower depends on four things: the true effect size, the standard error, the significance level, and the sample size. In OLS, for a two-sided test of $H_0: \\beta = 0$:\n\n$$\\delta = \\frac{|\\beta|}{\\text{se}(\\hat{\\beta})} \\approx \\frac{|\\beta| \\cdot \\text{sd}(X) \\cdot \\sqrt{n}}{\\sigma_e}$$\n\nPower $\\approx \\Phi(\\delta - z_{\\alpha/2}) + \\Phi(-\\delta - z_{\\alpha/2})$, where $\\delta$ is the signal-to-noise ratio.\n\n::: {#thm-power}\n## Power of a Two-Sided Test\nFor testing $H_0: \\beta = 0$ at level $\\alpha$, the power against alternative $\\beta \\neq 0$ is approximately $\\Phi(\\delta - z_{\\alpha/2}) + \\Phi(-\\delta - z_{\\alpha/2})$, where $\\delta = |\\beta|/\\text{SE}(\\hat\\beta)$ is the signal-to-noise ratio.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Power for a two-sided t-test in OLS\npower_ols <- function(effect, sigma_e, sd_x, n, alpha = 0.05) {\n  se <- sigma_e / (sd_x * sqrt(n))\n  delta <- abs(effect) / se\n  z <- qnorm(1 - alpha / 2)\n  pnorm(delta - z) + pnorm(-delta - z)\n}\n```\n:::\n\n\n### Power curves\n\nHow does power vary with sample size and effect size?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_seq <- seq(20, 500, by = 5)\neffects <- c(0.05, 0.10, 0.20, 0.40)\n\ndf_power <- do.call(rbind, lapply(effects, function(eff) {\n  data.frame(\n    n = n_seq,\n    power = sapply(n_seq, function(nn) power_ols(eff, sigma_e = 1, sd_x = 1, nn)),\n    effect = paste0(\"β = \", eff)\n  )\n}))\n\nggplot(df_power, aes(n, power, color = effect)) +\n  geom_line(linewidth = 1) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"gray50\") +\n  annotate(\"text\", x = 480, y = 0.82, label = \"80% power\", color = \"gray50\") +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\", color = \"gray70\") +\n  annotate(\"text\", x = 480, y = 0.07, label = \"α = 0.05\", color = \"gray70\") +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Power curves for a two-sided OLS t-test\",\n       subtitle = \"σ_e = 1, sd(X) = 1\",\n       x = \"Sample size (n)\", y = \"Power\", color = \"True effect\") +\n  coord_cartesian(ylim = c(0, 1))\n```\n\n::: {.cell-output-display}\n![Power curves for a two-sided OLS t-test](ch09-testing_files/figure-html/power-curves-1.png){width=768}\n:::\n:::\n\n\nSmall effects require enormous samples. With $\\beta = 0.10$ and $\\sigma_e / \\text{sd}(X) = 1$, you need roughly $n = 800$ for 80% power.\n\n::: {.callout-tip}\n## The 80% Power Rule of Thumb\nA study should have at least 80% power to detect the smallest effect of scientific interest. This requires $n \\approx 16\\sigma_e^2 / (\\beta^2 \\cdot \\text{Var}(X))$ for a two-sided 5% test. Adding relevant controls reduces $\\sigma_e$ and boosts power for free.\n:::\n\n### Simulation: observing power\n\nLet's verify the formula with a Monte Carlo simulation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12)\nB <- 2000\ntrue_beta <- 0.3\nsigma_e <- 1\nn_vals <- c(20, 50, 100, 200)\n\nsim_power <- function(n, B) {\n  rejections <- replicate(B, {\n    x <- rnorm(n)\n    y <- true_beta * x + rnorm(n, sd = sigma_e)\n    pval <- summary(lm(y ~ x))$coefficients[\"x\", \"Pr(>|t|)\"]\n    pval < 0.05\n  })\n  mean(rejections)\n}\n\nresults <- data.frame(\n  n = n_vals,\n  simulated = sapply(n_vals, sim_power, B = B),\n  formula = sapply(n_vals, function(nn) power_ols(true_beta, sigma_e, 1, nn))\n)\nresults\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    n simulated formula\n1  20    0.2290  0.2687\n2  50    0.5575  0.5641\n3 100    0.8250  0.8508\n4 200    0.9880  0.9888\n```\n\n\n:::\n:::\n\n\nThe simulated power matches the formula closely.\n\n### Controls boost power\n\nAdding a relevant control variable reduces $\\sigma_e$ without reducing $\\text{sd}(X)$, giving a free power boost:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(99)\nB <- 2000\nn <- 100\nbeta_x <- 0.2\nbeta_z <- 1.0  # strong control\n\n## Without control\npower_no_ctrl <- mean(replicate(B, {\n  x <- rnorm(n); z <- rnorm(n)\n  y <- beta_x * x + beta_z * z + rnorm(n)\n  summary(lm(y ~ x))$coefficients[\"x\", \"Pr(>|t|)\"] < 0.05\n}))\n\n## With control\npower_with_ctrl <- mean(replicate(B, {\n  x <- rnorm(n); z <- rnorm(n)\n  y <- beta_x * x + beta_z * z + rnorm(n)\n  summary(lm(y ~ x + z))$coefficients[\"x\", \"Pr(>|t|)\"] < 0.05\n}))\n\ncat(\"Power without control:\", round(power_no_ctrl, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPower without control: 0.294 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Power with control:   \", round(power_with_ctrl, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPower with control:    0.5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Residual SD without z:\", round(sqrt(beta_z^2 + 1), 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResidual SD without z: 1.41 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Residual SD with z:   \", round(1, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResidual SD with z:    1 \n```\n\n\n:::\n:::\n\n\nThe control cuts the residual standard deviation roughly in half, dramatically increasing power. This is why pre-treatment covariates help in experiments even though they are unnecessary for unbiasedness.\n\n### Joint tests have less power\n\nTesting more restrictions simultaneously dilutes power. Fix the total non-centrality parameter and compare:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda_seq <- seq(0, 15, by = 0.1)\n\ndf_joint <- do.call(rbind, lapply(c(1, 2, 5, 10), function(q) {\n  data.frame(\n    lambda = lambda_seq,\n    power = 1 - pchisq(qchisq(0.95, q), q, ncp = lambda_seq),\n    q = paste0(\"q = \", q)\n  )\n}))\n\nggplot(df_joint, aes(lambda, power, color = q)) +\n  geom_line(linewidth = 1) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"gray50\") +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Power vs. non-centrality for joint tests\",\n       subtitle = \"More restrictions require a larger signal to achieve the same power\",\n       x = expression(paste(\"Non-centrality parameter \", lambda)),\n       y = \"Power\", color = \"Restrictions\")\n```\n\n::: {.cell-output-display}\n![](ch09-testing_files/figure-html/joint-test-power-1.png){width=672}\n:::\n:::\n\n\nA single-coefficient t-test is more powerful than a joint F-test that lumps in other restrictions. Test exactly what you care about.\n\n## Statistical vs. economic significance\n\nWith a large enough sample, *any* nonzero coefficient becomes statistically significant. This is test consistency: $P[\\text{Reject}] \\to 1$ whenever $\\beta \\neq 0$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(77)\ntrue_beta <- 0.01   # tiny effect\nn_vals <- c(50, 200, 1000, 5000, 20000)\nB <- 1000\n\nrejection_rate <- sapply(n_vals, function(nn) {\n  mean(replicate(B, {\n    x <- rnorm(nn)\n    y <- true_beta * x + rnorm(nn)\n    summary(lm(y ~ x))$coefficients[\"x\", \"Pr(>|t|)\"] < 0.05\n  }))\n})\n\ndf_consist <- data.frame(n = n_vals, rejection_rate = rejection_rate)\nggplot(df_consist, aes(n, rejection_rate)) +\n  geom_line(linewidth = 1, color = \"steelblue\") +\n  geom_point(size = 3, color = \"steelblue\") +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\", color = \"coral\") +\n  scale_x_log10() +\n  labs(title = \"Test consistency: even β = 0.01 gets rejected eventually\",\n       subtitle = \"With enough data, statistical significance is guaranteed for any nonzero effect\",\n       x = \"Sample size (log scale)\", y = \"Rejection rate at α = 0.05\")\n```\n\n::: {.cell-output-display}\n![](ch09-testing_files/figure-html/stat-vs-econ-1.png){width=768}\n:::\n:::\n\n\nA $p$-value tells you whether the data are inconsistent with $H_0$. It tells you nothing about whether the effect is large enough to matter. Always report the **magnitude** alongside the significance.\n\n\n## Applied workflow: the salary example\n\nLet's bring everything together with a thorough analysis of the professor salary data.\n\n### Step 1: Fit and inspect\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_full <- lm(salary ~ rank + discipline + yrs.since.phd + yrs.service + sex,\n               data = Salaries)\ncoeftest(mod_full, vcov. = vcovHC(mod_full, type = \"HC1\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      65955       2896   22.78  < 2e-16 ***\nrankAssocProf    12908       2206    5.85  1.0e-08 ***\nrankProf         45066       3285   13.72  < 2e-16 ***\ndisciplineB      14418       2316    6.22  1.2e-09 ***\nyrs.since.phd      535        312    1.71    0.088 .  \nyrs.service       -490        307   -1.60    0.111    \nsexMale           4784       2396    2.00    0.047 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### Step 2: Key hypotheses\n\n**Does rank matter?** (joint test, $q = 2$)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinearHypothesis(mod_full, c(\"rankAssocProf = 0\", \"rankProf = 0\"),\n                 vcov. = vcovHC(mod_full, type = \"HC1\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\nrankAssocProf = 0\nrankProf = 0\n\nModel 1: restricted model\nModel 2: salary ~ rank + discipline + yrs.since.phd + yrs.service + sex\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df   F Pr(>F)    \n1    392                  \n2    390  2 109 <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n**Does discipline matter?** (single restriction)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinearHypothesis(mod_full, \"disciplineB = 0\",\n                 vcov. = vcovHC(mod_full, type = \"HC1\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\ndisciplineB = 0\n\nModel 1: restricted model\nModel 2: salary ~ rank + discipline + yrs.since.phd + yrs.service + sex\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df    F  Pr(>F)    \n1    391                    \n2    390  1 38.7 1.2e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n**Are the two experience measures interchangeable?** ($\\beta_{\\text{phd}} = \\beta_{\\text{svc}}$)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinearHypothesis(mod_full, \"yrs.since.phd = yrs.service\",\n                 vcov. = vcovHC(mod_full, type = \"HC1\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\nyrs.since.phd - yrs.service = 0\n\nModel 1: restricted model\nModel 2: salary ~ rank + discipline + yrs.since.phd + yrs.service + sex\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df    F Pr(>F)  \n1    391                 \n2    390  1 2.92  0.088 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### Step 3: Confidence intervals\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Robust CIs\nci_robust <- coefci(mod_full, vcov. = vcovHC(mod_full, type = \"HC1\"))\nci_robust\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 2.5 %  97.5 %\n(Intercept)   60261.83 71648.6\nrankAssocProf  8571.09 17244.1\nrankProf      38607.31 51524.7\ndisciplineB    9863.61 18971.6\nyrs.since.phd   -79.26  1149.4\nyrs.service   -1092.74   113.7\nsexMale          72.96  9494.0\n```\n\n\n:::\n:::\n\n\n### Step 4: Multiple testing adjustment\n\nIf we're testing all six regressors simultaneously:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npvals_all <- coeftest(mod_full, vcov. = vcovHC(mod_full, type = \"HC1\"))[-1, 4]\ndata.frame(\n  raw_p = round(pvals_all, 4),\n  holm_p = round(p.adjust(pvals_all, method = \"holm\"), 4),\n  significant_raw = ifelse(pvals_all < 0.05, \"*\", \"\"),\n  significant_holm = ifelse(p.adjust(pvals_all, method = \"holm\") < 0.05, \"*\", \"\")\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               raw_p holm_p significant_raw significant_holm\nrankAssocProf 0.0000 0.0000               *                *\nrankProf      0.0000 0.0000               *                *\ndisciplineB   0.0000 0.0000               *                *\nyrs.since.phd 0.0876 0.1752                                 \nyrs.service   0.1114 0.1752                                 \nsexMale       0.0466 0.1397               *                 \n```\n\n\n:::\n:::\n\n\n### Step 5: Power assessment for the gender gap\n\nSuppose the \"true\" gender gap is \\$5,000 (about 5% of mean salary). Could we detect it?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma_e <- sigma(mod_full)\nsd_sex <- sd(as.numeric(Salaries$sex == \"Male\"))\nn <- nrow(Salaries)\n\npower_sex <- power_ols(effect = 5000, sigma_e = sigma_e,\n                       sd_x = sd_sex, n = n, alpha = 0.05)\ncat(\"Estimated power to detect a $5,000 gender gap:\", round(power_sex, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEstimated power to detect a $5,000 gender gap: 0.261 \n```\n\n\n:::\n\n```{.r .cell-code}\n## What sample size for 80% power?\nn_needed <- seq(100, 5000, by = 10)\npow_curve <- sapply(n_needed, function(nn)\n  power_ols(5000, sigma_e, sd_sex, nn, 0.05))\nn80 <- n_needed[which(pow_curve >= 0.80)[1]]\ncat(\"Sample size for 80% power:\", n80, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample size for 80% power: 1800 \n```\n\n\n:::\n:::\n\n\n\n## Practical advice (after Hansen)\n\nA checklist for reporting regression results:\n\n1. **Report standard errors, not just t-ratios.** Standard errors convey precision; t-statistics reduce everything to a binary yes/no.\n\n2. **Report p-values, not asterisks.** The difference between $p = 0.049$ and $p = 0.051$ is not meaningful. Stars obscure this.\n\n3. **Focus on substantive hypotheses.** Don't mechanically test every coefficient against zero. Test things that answer a scientific question.\n\n4. **\"Fail to reject\" $\\neq$ \"accept.\"** Low power can explain non-rejection. Always ask: could we have detected a meaningful effect?\n\n5. **Statistical significance $\\neq$ economic significance.** A tiny but significant coefficient may be policy-irrelevant. A large but insignificant coefficient may reflect low power.\n\n6. **Use robust inference by default.** There is rarely a good reason to report classical SEs when $n$ is not tiny. Use HC1/HC2 or the bootstrap.\n\n7. **Account for multiple testing.** If you report the \"most significant\" result from many tests, apply Holm or Bonferroni corrections.\n\n\n## Connection to GMM\n\nThe Wald test has a natural GMM interpretation. In GMM, testing $H_0: \\mathbf{R}\\boldsymbol{\\beta} = \\mathbf{r}$ is equivalent to testing whether the restricted moment conditions $E[\\mathbf{Z}'(Y - \\mathbf{X}\\boldsymbol{\\beta})] = \\mathbf{0}$ with $\\mathbf{R}\\boldsymbol{\\beta} = \\mathbf{r}$ are compatible with the data. The J-test of overidentifying restrictions, which we'll meet in Chapter 11, generalizes the F-test to the setting where there are more moment conditions than parameters.\n\n| OLS test | GMM counterpart |\n|----------|-----------------|\n| F-test (restricted vs. unrestricted) | Distance test (restricted vs. unrestricted GMM) |\n| Wald test | Wald test (same formula, different $\\hat{\\mathbf{V}}$) |\n| Score / LM test | Score test from restricted GMM |\n| Overall F-statistic | J-test of overidentifying restrictions |\n",
    "supporting": [
      "ch09-testing_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}