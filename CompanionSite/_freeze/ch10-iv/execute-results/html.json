{
  "hash": "d70ea8cc5041b4fa3f09fe5e84523cd2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"10. Instrumental Variables and 2SLS\"\nsubtitle: \"Endogeneity, the IV and 2SLS estimators, weak instruments, and LATE\"\n---\n\nWhen a regressor is correlated with the error --- whether from omitted variables, measurement error, or simultaneity --- OLS is [inconsistent](ch08-asymptotics.qmd#sec-consistency). No amount of data fixes this. **Instrumental variables** (IV) offer a way out: find a variable $Z$ that shifts the endogenous regressor but is otherwise unrelated to the outcome.\n\nThis chapter builds the IV toolkit through simulation and application:\n\n1. **Why does OLS fail?** (endogeneity, inconsistency)\n2. **How does IV work?** (the estimator, the Wald ratio, `iv_robust()`)\n3. **What about overidentification?** (2SLS, building it by hand)\n4. **Is my instrument strong enough?** (first-stage F, weak instrument bias)\n5. **Can I test for endogeneity?** (control function / Hausman test)\n6. **What does IV estimate under heterogeneity?** (LATE, compliers)\n7. **Applied example** (cigarette demand elasticity)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(estimatr)\nlibrary(car)\nlibrary(carData)\nlibrary(AER)\noptions(digits = 4)\n```\n:::\n\n\n\n## The endogeneity problem\n\nSuppose the true model is $Y = 1 + 0.5 X + e$, but $X$ and $e$ are correlated. We generate this by letting both $X$ and $e$ depend on an unobserved confounder:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nn <- 500\n\n## Data-generating process\nz <- rnorm(n)               # instrument (exogenous)\nv <- rnorm(n)               # first-stage error\ne <- 0.8 * v + rnorm(n, sd = 0.5)  # structural error (correlated with v!)\nx <- 1.5 * z + v            # endogenous regressor\ny <- 1 + 0.5 * x + e        # structural equation\ndat <- data.frame(y, x, z)\n```\n:::\n\n\nBecause $v$ appears in both $x$ and (through $e$), $\\text{Cor}(x, e) \\neq 0$. OLS is biased *and inconsistent*:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols <- lm(y ~ x, data = dat)\ncat(\"True β =\", 0.5, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue β = 0.5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"OLS  β =\", round(coef(ols)[\"x\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOLS  β = 0.7753 \n```\n\n\n:::\n:::\n\n\nThe bias does not vanish with more data --- OLS converges to the wrong value:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nn_vals <- c(50, 200, 1000, 5000, 20000)\nB <- 500\n\nsim_ols <- function(n) {\n  z <- rnorm(n)\n  v <- rnorm(n)\n  e <- 0.8 * v + rnorm(n, sd = 0.5)\n  x <- 1.5 * z + v\n  y <- 1 + 0.5 * x + e\n  coef(lm(y ~ x))[2]\n}\n\ndf_ols <- do.call(rbind, lapply(n_vals, function(nn) {\n  data.frame(estimate = replicate(B, sim_ols(nn)), n = nn)\n}))\n\nggplot(df_ols, aes(x = estimate)) +\n  geom_histogram(bins = 40, alpha = 0.6, fill = \"steelblue\") +\n  geom_vline(xintercept = 0.5, linetype = \"dashed\", color = \"coral\", linewidth = 1) +\n  facet_wrap(~ n, labeller = label_both, scales = \"free_y\") +\n  labs(title = \"OLS is inconsistent under endogeneity\",\n       subtitle = \"The distribution concentrates on the wrong value as n grows\",\n       x = expression(hat(beta)[OLS]))\n```\n\n::: {.cell-output-display}\n![OLS is inconsistent under endogeneity — the distribution concentrates on the wrong value](ch10-iv_files/figure-html/ols-inconsistency-1.png){width=768}\n:::\n:::\n\n\n::: {.callout-note}\n## Endogeneity Means Inconsistency\nWhen $\\text{Cov}(X, e) \\neq 0$, OLS converges to the wrong value — bias does not shrink with larger samples. This is fundamentally different from finite-sample bias (which vanishes as $n \\to \\infty$). IV trades precision for consistency.\n:::\n\n\n## The IV estimator {#sec-iv-estimator}\n\nThe IV estimator uses the instrument $Z$ (which is uncorrelated with $e$) to isolate the exogenous variation in $X$:\n\n$$\\hat{\\beta}_{IV} = (Z'X)^{-1} Z'Y$$ {#eq-iv}\n\nThis is the sample analogue of $\\beta = \\text{Cov}(Z, Y) / \\text{Cov}(Z, X)$.\n\n::: {#thm-iv}\n## IV Estimator\nThe IV estimator $\\hat\\beta_{IV} = (Z'X)^{-1}Z'Y$ is consistent for $\\beta$ when $\\mathbb{E}[Z_i e_i] = 0$ (instrument exogeneity) and $\\mathbb{E}[Z_i X_i'] \\neq 0$ (instrument relevance). It is the sample analogue of $\\beta = \\text{Cov}(Z, Y) / \\text{Cov}(Z, X)$.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## IV by hand (just-identified: one instrument, one endogenous regressor)\nZ <- cbind(1, dat$z)   # instrument matrix (with intercept)\nX <- cbind(1, dat$x)   # regressor matrix\n\nbeta_iv <- solve(t(Z) %*% X) %*% t(Z) %*% dat$y\ncat(\"IV estimate (by hand):\", round(beta_iv[2], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIV estimate (by hand): 0.5086 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"True β:               \", 0.5, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue β:                0.5 \n```\n\n\n:::\n:::\n\n\n### Using `iv_robust()`\n\nThe `estimatr` package provides `iv_robust()`, which handles 2SLS estimation with heteroskedasticity-robust standard errors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Syntax: Y ~ endogenous + exogenous | instruments + exogenous\niv_fit <- iv_robust(y ~ x | z, data = dat)\nsummary(iv_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\niv_robust(formula = y ~ x | z, data = dat)\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|) CI Lower CI Upper  DF\n(Intercept)    0.963     0.0426    22.6 2.95e-78    0.879    1.046 498\nx              0.509     0.0286    17.8 4.67e-55    0.452    0.565 498\n\nMultiple R-squared:  0.65 ,\tAdjusted R-squared:  0.65 \nF-statistic:  316 on 1 and 498 DF,  p-value: <2e-16\n```\n\n\n:::\n:::\n\n\nThe coefficient on `x` is close to the true value of 0.5. Compare to OLS:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(\n  Estimator = c(\"OLS\", \"IV\"),\n  Estimate = c(coef(ols)[\"x\"], coef(iv_fit)[\"x\"]),\n  SE = c(sqrt(vcovHC(ols)[\"x\", \"x\"]), iv_fit$std.error[\"x\"]),\n  row.names = NULL\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Estimator Estimate      SE\n1       OLS   0.7753 0.02043\n2        IV   0.5086 0.02863\n```\n\n\n:::\n:::\n\n\nIV is consistent but less precise --- the standard error is larger. This is the cost of correcting for endogeneity.\n\n### The Wald estimator\n\nWhen the instrument is **binary**, the IV estimator simplifies to the **Wald ratio**: the reduced-form effect on $Y$ divided by the first-stage effect on $X$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Create a binary instrument version\nset.seed(42)\nn <- 1000\nz_bin <- rbinom(n, 1, 0.5)          # binary instrument\nv <- rnorm(n)\ne <- 0.8 * v + rnorm(n, sd = 0.5)\nx <- 0.6 * z_bin + v                 # first stage\ny <- 1 + 0.5 * x + e                 # structural equation\n\n## Wald estimator by hand\ny1 <- mean(y[z_bin == 1]); y0 <- mean(y[z_bin == 0])\nx1 <- mean(x[z_bin == 1]); x0 <- mean(x[z_bin == 0])\nwald <- (y1 - y0) / (x1 - x0)\n\ncat(\"Reduced form (Y):  \", round(y1 - y0, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReduced form (Y):   0.3984 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"First stage (X):   \", round(x1 - x0, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFirst stage (X):    0.6712 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Wald estimate:     \", round(wald, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWald estimate:      0.5936 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"True β:             0.5\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue β:             0.5\n```\n\n\n:::\n:::\n\n\nThe Wald ratio is the intent-to-treat (ITT) effect on $Y$ rescaled by the first-stage compliance rate. This is the building block of LATE, which we develop below.\n\n\n## Two-stage least squares {#sec-2sls}\n\nWhen there are **more instruments than endogenous regressors** ($l > k$, overidentification), we use **2SLS**:\n\n- **Stage 1**: Regress $X$ on all instruments $Z$ to get fitted values $\\hat{X} = P_Z X$\n- **Stage 2**: Regress $Y$ on $\\hat{X}$ (but compute SEs using the original $X$)\n\n::: {#thm-2sls}\n## Two-Stage Least Squares\nThe 2SLS estimator $\\hat\\beta_{2SLS} = (X'P_Z X)^{-1}X'P_Z Y$, where $P_Z = Z(Z'Z)^{-1}Z'$, handles overidentification ($\\ell > k$) by projecting $X$ onto the instrument space before applying OLS. It equals the GMM estimator with weight matrix $(Z'Z)^{-1}$.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## DGP with two instruments for one endogenous regressor\nset.seed(42)\nn <- 500\nz1 <- rnorm(n)\nz2 <- rnorm(n)\nv <- rnorm(n)\ne <- 0.8 * v + rnorm(n, sd = 0.5)\nx <- 0.8 * z1 + 0.6 * z2 + v   # two instruments\ny <- 1 + 0.5 * x + e\n\ndat2 <- data.frame(y, x, z1, z2)\n```\n:::\n\n\n### 2SLS by hand\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Stage 1: regress X on instruments\nstage1 <- lm(x ~ z1 + z2, data = dat2)\ndat2$x_hat <- fitted(stage1)\n\n## Stage 2: regress Y on fitted X\nstage2_naive <- lm(y ~ x_hat, data = dat2)\n\n## The coefficient is correct, but the SEs are wrong!\n## Correct SEs use the original X, not x_hat\ncat(\"2SLS coefficient (from two stages):\", round(coef(stage2_naive)[\"x_hat\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2SLS coefficient (from two stages): 0.5194 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Naive SE (wrong):                  \", round(summary(stage2_naive)$coef[\"x_hat\", 2], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNaive SE (wrong):                   0.0586 \n```\n\n\n:::\n:::\n\n\nThe naive second-stage SEs are **incorrect** because they treat $\\hat{X}$ as if it were $X$. The `iv_robust()` function handles this automatically:\n\n\n::: {.cell}\n\n```{.r .cell-code}\niv_fit2 <- iv_robust(y ~ x | z1 + z2, data = dat2)\ncat(\"2SLS coefficient (iv_robust):      \", round(coef(iv_fit2)[\"x\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2SLS coefficient (iv_robust):       0.5194 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Correct robust SE:                 \", round(iv_fit2$std.error[\"x\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorrect robust SE:                  0.0384 \n```\n\n\n:::\n:::\n\n\n### Verifying the matrix formula\n\nThe 2SLS estimator is $\\hat{\\beta}_{2SLS} = (X'P_Z X)^{-1} X'P_Z Y$ where $P_Z = Z(Z'Z)^{-1}Z'$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nZ <- cbind(1, dat2$z1, dat2$z2)\nX <- cbind(1, dat2$x)\nY <- dat2$y\n\nPZ <- Z %*% solve(t(Z) %*% Z) %*% t(Z)\nbeta_2sls <- solve(t(X) %*% PZ %*% X) %*% t(X) %*% PZ %*% Y\n\ncat(\"Matrix formula:\", round(beta_2sls[2], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatrix formula: 0.5194 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"iv_robust():  \", round(coef(iv_fit2)[\"x\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\niv_robust():   0.5194 \n```\n\n\n:::\n:::\n\n\n\n## Instrument strength and weak instruments\n\nIV pays for consistency with precision. The asymptotic variance of IV relative to OLS is:\n\n$$\\text{Avar}(\\hat{\\beta}_{IV}) = \\frac{\\text{Avar}(\\hat{\\beta}_{OLS})}{\\rho^2_{ZX}}$$\n\nWhen the instrument is weak ($\\rho_{ZX} \\approx 0$), IV becomes wildly imprecise and biased. The **first-stage F-statistic** measures instrument strength.\n\n### Simulation: bias grows as instruments weaken\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12)\nB <- 1000\nn <- 200\n\n## Vary the first-stage coefficient (instrument strength)\npi_vals <- c(0.02, 0.05, 0.1, 0.2, 0.5, 1.0)\n\nresults <- do.call(rbind, lapply(pi_vals, function(pi) {\n  estimates <- replicate(B, {\n    z <- rnorm(n)\n    v <- rnorm(n)\n    e <- 0.8 * v + rnorm(n, sd = 0.5)\n    x <- pi * z + v\n    y <- 1 + 0.5 * x + e\n    fit <- iv_robust(y ~ x | z, data = data.frame(y, x, z))\n    c(iv = coef(fit)[\"x\"],\n      F_stat = summary(lm(x ~ z))$fstatistic[1])\n  })\n  data.frame(\n    pi = pi,\n    iv_est = estimates[\"iv.x\", ],\n    F_stat = estimates[\"F_stat.value\", ],\n    label = paste0(\"π = \", pi, \"\\nmed F = \", round(median(estimates[\"F_stat.value\", ]), 1))\n  )\n}))\n\nggplot(results, aes(iv_est)) +\n  geom_histogram(bins = 50, alpha = 0.6, fill = \"steelblue\") +\n  geom_vline(xintercept = 0.5, color = \"coral\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = coef(lm(y ~ x, data = dat))[\"x\"],\n             color = \"gray50\", linetype = \"dotted\", linewidth = 0.8) +\n  facet_wrap(~ label, scales = \"free_y\") +\n  coord_cartesian(xlim = c(-2, 3)) +\n  labs(title = \"Weak instruments bias IV toward OLS\",\n       subtitle = \"Dashed red = true β. Dotted gray = OLS probability limit.\",\n       x = expression(hat(beta)[IV]))\n```\n\n::: {.cell-output-display}\n![](ch10-iv_files/figure-html/weak-instrument-sim-1.png){width=768}\n:::\n:::\n\n\nKey patterns:\n\n- **Strong instruments** (large $\\pi$, high F): IV is centered on the true value\n- **Weak instruments** (small $\\pi$, low F): IV is biased toward OLS and has enormous variance\n- The **F > 10 rule of thumb** (Staiger & Stock 1997) marks the boundary of acceptable instrument strength\n\n::: {.callout-warning}\n## Weak Instruments: F < 10 Rule of Thumb\nWhen the first-stage F-statistic is below 10 (Staiger & Stock, 1997), IV is biased toward OLS, wildly imprecise, and confidence intervals have poor coverage. Always report the first-stage F and treat results with F < 10 as unreliable.\n:::\n\n### The bias formula\n\nThe approximate bias of IV relative to OLS is:\n\n$$\\frac{\\text{Bias}(\\hat{\\beta}_{IV})}{\\text{Bias}(\\hat{\\beta}_{OLS})} \\approx \\frac{1}{F}$$ {#eq-2sls}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Compute median bias ratio by F-stat bins\nresults$F_bin <- cut(results$F_stat, breaks = c(0, 5, 10, 20, 50, 100, Inf),\n                     labels = c(\"<5\", \"5-10\", \"10-20\", \"20-50\", \"50-100\", \">100\"))\n\nbias_ols <- 0.8 / (0.8^2 + 1)  # approximate OLS bias (rho_xe * sigma_e / sigma_x)\nbias_by_F <- aggregate(iv_est ~ F_bin, data = results, function(x) mean(x) - 0.5)\nbias_by_F$bias_ratio <- bias_by_F$iv_est / bias_ols\n\nggplot(bias_by_F, aes(F_bin, bias_ratio)) +\n  geom_col(fill = \"steelblue\", alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"IV bias as a fraction of OLS bias\",\n       subtitle = \"Weak instruments (low F) leave most of the OLS bias uncorrected\",\n       x = \"First-stage F-statistic\", y = \"Bias(IV) / Bias(OLS)\")\n```\n\n::: {.cell-output-display}\n![](ch10-iv_files/figure-html/bias-vs-F-1.png){width=672}\n:::\n:::\n\n\n\n## Testing: endogeneity and instrument validity\n\n### The Hausman test via the control function\n\nThe **control function** approach makes endogeneity testing transparent. Add the first-stage residuals $\\hat{v}$ to the structural equation. If their coefficient $\\hat{\\alpha}$ is significant, $X$ is endogenous:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Use the overidentified simulation data\nstage1 <- lm(x ~ z1 + z2, data = dat2)\ndat2$v_hat <- residuals(stage1)\n\n## Control function regression\ncf_reg <- lm(y ~ x + v_hat, data = dat2)\ncoeftest(cf_reg, vcov. = vcovHC(cf_reg, type = \"HC1\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.9838     0.0224    43.9   <2e-16 ***\nx             0.5194     0.0226    22.9   <2e-16 ***\nv_hat         0.7616     0.0318    23.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nThe coefficient on `x` is the IV estimate of $\\beta$. The t-test on `v_hat` is the **Hausman endogeneity test** --- if significant, $X$ is endogenous and IV is needed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"Control function β on x:\", round(coef(cf_reg)[\"x\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nControl function β on x: 0.5194 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"2SLS β:                 \", round(coef(iv_fit2)[\"x\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2SLS β:                  0.5194 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Hausman p-value (α = 0):\", round(coeftest(cf_reg)[\"v_hat\", 4], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHausman p-value (α = 0): 0 \n```\n\n\n:::\n:::\n\n\nThe two estimates are numerically identical (under homoskedasticity). The Hausman test rejects, confirming that OLS is inconsistent here.\n\n### The Sargan test for overidentifying restrictions\n\nWith more instruments than endogenous regressors ($l > k$), we can test whether the **extra** instruments are valid. The **Sargan/J-test** checks whether the overidentifying restrictions hold:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Sargan test by hand\n## iv_robust stores residuals as y - X %*% beta_2sls\ne_hat <- dat2$y - iv_fit2$fitted.values\nsargan_reg <- lm(e_hat ~ z1 + z2, data = dat2)\nJ_stat <- n * summary(sargan_reg)$r.squared\nq <- 1  # l - k = 3 instruments - 2 parameters = 1 overidentifying restriction\np_sargan <- 1 - pchisq(J_stat, q)\n\ncat(\"Sargan J-statistic:\", round(J_stat, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSargan J-statistic: 0.162 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"p-value:           \", round(p_sargan, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\np-value:            0.688 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"(Fail to reject => instruments are consistent with validity)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Fail to reject => instruments are consistent with validity)\n```\n\n\n:::\n:::\n\n\nThe Sargan test cannot detect invalid instruments if *all* instruments are invalid in the same way. It only has power when some instruments are valid and others are not.\n\n\n## LATE: what does IV estimate under heterogeneity?\n\nWhen treatment effects vary across individuals, IV does not estimate the **average treatment effect** (ATE). With a binary instrument, it estimates the **Local Average Treatment Effect** (LATE) --- the effect for **compliers**, those whose treatment status is changed by the instrument.\n\n::: {#def-late}\n## Local Average Treatment Effect (LATE)\nWith a binary instrument, IV estimates the LATE: the average treatment effect for **compliers** — individuals whose treatment status is changed by the instrument. LATE is instrument-dependent: different instruments identify different complier populations.\n:::\n\n::: {.callout-warning}\n## LATE Is Local\nIV does not estimate the average treatment effect (ATE) for the full population. It estimates the effect for compliers only — a subgroup that cannot be identified individually. With heterogeneous effects, different instruments give different LATEs.\n:::\n\n### Simulation with heterogeneous effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(303)\nn <- 5000\n\n## Individual treatment effects: drawn from N(0.5, 0.3^2)\ntau_i <- rnorm(n, mean = 0.5, sd = 0.3)\n\n## Potential outcomes\ny0 <- rnorm(n, mean = 2)\ny1 <- y0 + tau_i\n\n## Binary instrument\nz <- rbinom(n, 1, 0.5)\n\n## Compliance types:\n## Define individual-level threshold (unobserved resistance)\n## Low threshold = always-taker, high threshold = never-taker\nu_resist <- runif(n)\nalways_taker <- (u_resist < 0.2)\nnever_taker  <- (u_resist > 0.7)\ncomplier     <- !always_taker & !never_taker\n\n## Treatment assignment: compliers follow instrument, others don't\nd <- ifelse(always_taker, 1,\n     ifelse(never_taker, 0,\n     z))  # compliers: D = Z\n\n## Make compliers have systematically different treatment effects\n## (essential heterogeneity: those who comply have larger effects)\ntau_i[complier] <- tau_i[complier] + 0.2\n\n## Observed outcome\ny <- ifelse(d == 1, y1, y0)\ndat_late <- data.frame(y, d, z, complier, tau_i)\n\ncat(\"True ATE:                    \", round(mean(tau_i), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue ATE:                     0.609 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"True ATT:                    \", round(mean(tau_i[d == 1]), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue ATT:                     0.626 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"True LATE (complier effect): \", round(mean(tau_i[complier]), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue LATE (complier effect):  0.708 \n```\n\n\n:::\n:::\n\n\nNow compare OLS, the naive difference in means, and IV:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## OLS (biased: always-takers inflate the treated group)\nols_est <- coef(lm(y ~ d, data = dat_late))[\"d\"]\n\n## Wald / IV estimate\nwald_est <- (mean(y[z == 1]) - mean(y[z == 0])) / (mean(d[z == 1]) - mean(d[z == 0]))\n\n## iv_robust\niv_late <- iv_robust(y ~ d | z, data = dat_late)\n\ndata.frame(\n  Estimand = c(\"ATE (true)\", \"LATE (true)\", \"OLS\", \"IV / Wald\"),\n  Value = round(c(mean(tau_i), mean(tau_i[complier]), ols_est, coef(iv_late)[\"d\"]), 3),\n  row.names = NULL\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Estimand Value\n1  ATE (true) 0.609\n2 LATE (true) 0.708\n3         OLS 0.515\n4   IV / Wald 0.576\n```\n\n\n:::\n:::\n\n\nIV recovers the **LATE** --- the effect for compliers --- not the ATE. This is instrument-dependent: a different instrument would identify a different set of compliers with potentially different effects.\n\n### Visualizing compliance types\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_late$type <- ifelse(always_taker, \"Always-taker\",\n                 ifelse(never_taker, \"Never-taker\", \"Complier\"))\ndat_late$type <- factor(dat_late$type, levels = c(\"Never-taker\", \"Complier\", \"Always-taker\"))\n\nggplot(dat_late, aes(tau_i, fill = type)) +\n  geom_histogram(bins = 40, alpha = 0.6, position = \"identity\") +\n  geom_vline(aes(xintercept = mean(tau_i[complier]), color = \"LATE\"),\n             linetype = \"dashed\", linewidth = 1) +\n  geom_vline(aes(xintercept = mean(tau_i), color = \"ATE\"),\n             linetype = \"dashed\", linewidth = 1) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_manual(values = c(\"ATE\" = \"coral\", \"LATE\" = \"steelblue\"),\n                     name = \"Estimand\") +\n  labs(title = \"Treatment effects by compliance type\",\n       subtitle = \"IV estimates the LATE (complier average), not the ATE (overall average)\",\n       x = expression(tau[i] == Y[i](1) - Y[i](0)), fill = \"Type\")\n```\n\n::: {.cell-output-display}\n![](ch10-iv_files/figure-html/compliance-types-1.png){width=768}\n:::\n:::\n\n\n\n## Applied example: cigarette demand elasticity\n\nWe estimate the price elasticity of cigarette demand using state-level panel data from Stock and Watson. Prices are endogenous (supply and demand are jointly determined), so we instrument with cigarette-specific sales taxes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"CigarettesSW\", package = \"AER\")\n\n## Use 1995 cross-section\ncig95 <- subset(CigarettesSW, year == \"1995\")\ncig85 <- subset(CigarettesSW, year == \"1985\")\n\n## Create real variables (deflate by CPI)\ncig95$rprice <- cig95$price / cig95$cpi\ncig95$rtaxs  <- cig95$taxs / cig95$cpi   # general + cig-specific tax\ncig95$rtax   <- cig95$tax / cig95$cpi    # cigarette-specific tax only\ncig95$rincome <- cig95$income / (cig95$population * cig95$cpi)\n\n## Log transform for elasticity interpretation\ncig95$lnpacks <- log(cig95$packs)\ncig95$lnprice <- log(cig95$rprice)\ncig95$lnrincome <- log(cig95$rincome)\n```\n:::\n\n\n### OLS: biased demand elasticity\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_cig <- lm(lnpacks ~ lnprice + lnrincome, data = cig95)\ncoeftest(ols_cig, vcov. = vcovHC(ols_cig, type = \"HC1\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   10.342      0.966   10.70  6.0e-14 ***\nlnprice       -1.407      0.261   -5.39  2.5e-06 ***\nlnrincome      0.344      0.260    1.32     0.19    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nThe OLS price elasticity is likely biased toward zero (or even positive) because high-demand states drive up prices --- simultaneity bias.\n\n### First stage: do taxes predict prices?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst_stage <- lm(lnprice ~ rtax + lnrincome, data = cig95)\ncoeftest(first_stage, vcov. = vcovHC(first_stage, type = \"HC1\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.170512   0.121836   34.23  < 2e-16 ***\nrtax        0.011226   0.000896   12.54  2.8e-16 ***\nlnrincome   0.080330   0.054332    1.48     0.15    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nFirst-stage F-statistic:\", round(summary(first_stage)$fstatistic[1], 1), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFirst-stage F-statistic: 203.5 \n```\n\n\n:::\n:::\n\n\nTaxes strongly predict prices (F well above 10). The instrument is relevant.\n\n### 2SLS: IV demand elasticity\n\n\n::: {.cell}\n\n```{.r .cell-code}\niv_cig <- iv_robust(lnpacks ~ lnprice + lnrincome | rtax + lnrincome,\n                    data = cig95)\nsummary(iv_cig)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\niv_robust(formula = lnpacks ~ lnprice + lnrincome | rtax + lnrincome, \n    data = cig95)\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|) CI Lower CI Upper DF\n(Intercept)   10.024      1.025    9.78 1.05e-12    7.958   12.089 45\nlnprice       -1.315      0.259   -5.08 7.16e-06   -1.836   -0.793 45\nlnrincome      0.299      0.248    1.20 2.35e-01   -0.201    0.798 45\n\nMultiple R-squared:  0.431 ,\tAdjusted R-squared:  0.406 \nF-statistic: 14.9 on 2 and 45 DF,  p-value: 1.05e-05\n```\n\n\n:::\n:::\n\n\n### With two instruments (overidentified)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Use both cigarette-specific tax and general sales tax\niv_cig2 <- iv_robust(lnpacks ~ lnprice + lnrincome | rtax + rtaxs + lnrincome,\n                     data = cig95)\nsummary(iv_cig2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\niv_robust(formula = lnpacks ~ lnprice + lnrincome | rtax + rtaxs + \n    lnrincome, data = cig95)\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|) CI Lower CI Upper DF\n(Intercept)     9.89      0.978   10.12 3.57e-13    7.926   11.864 45\nlnprice        -1.28      0.255   -5.02 8.74e-06   -1.790   -0.764 45\nlnrincome       0.28      0.255    1.10 2.77e-01   -0.233    0.793 45\n\nMultiple R-squared:  0.429 ,\tAdjusted R-squared:  0.404 \nF-statistic: 15.5 on 2 and 45 DF,  p-value: 7.55e-06\n```\n\n\n:::\n:::\n\n\n### Comparing OLS and IV\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomparison <- data.frame(\n  Estimator = c(\"OLS\", \"IV (one instrument)\", \"IV (two instruments)\"),\n  Elasticity = c(coef(ols_cig)[\"lnprice\"],\n                 coef(iv_cig)[\"lnprice\"],\n                 coef(iv_cig2)[\"lnprice\"]),\n  SE = c(sqrt(vcovHC(ols_cig)[\"lnprice\", \"lnprice\"]),\n         iv_cig$std.error[\"lnprice\"],\n         iv_cig2$std.error[\"lnprice\"]),\n  row.names = NULL\n)\ncomparison\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Estimator Elasticity     SE\n1                  OLS     -1.407 0.2750\n2  IV (one instrument)     -1.315 0.2590\n3 IV (two instruments)     -1.277 0.2547\n```\n\n\n:::\n:::\n\n\nThe IV estimate of the price elasticity is more negative than OLS, consistent with the simultaneity story: OLS understates the (negative) demand response because high demand pushes prices up.\n\n### Diagnostics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Hausman test via control function\ncig95$v_hat <- residuals(lm(lnprice ~ rtax + lnrincome, data = cig95))\ncf_cig <- lm(lnpacks ~ lnprice + lnrincome + v_hat, data = cig95)\ncat(\"Hausman test (t on v_hat):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHausman test (t on v_hat):\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(cf_cig, vcov. = vcovHC(cf_cig, type = \"HC1\"))[\"v_hat\", , drop = FALSE]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Estimate Std. Error t value Pr(>|t|)\nv_hat  -0.6682     0.6949 -0.9616   0.3415\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Sargan test (overidentified model with 2 instruments)\ne_iv <- cig95$lnpacks - iv_cig2$fitted.values\nsargan_cig <- lm(e_iv ~ rtax + rtaxs + lnrincome, data = cig95)\nJ <- nrow(cig95) * summary(sargan_cig)$r.squared\ncat(\"\\nSargan J-stat:\", round(J, 3), \"  p-value:\", round(1 - pchisq(J, 1), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSargan J-stat: 0.333   p-value: 0.564 \n```\n\n\n:::\n:::\n\n\n\n## Connection to GMM\n\nIV and 2SLS are special cases of the **Generalized Method of Moments** (GMM). We will see in Chapter 11 that 2SLS is a [special case of GMM](ch11-gmm.qmd#thm-gmm-2sls). The moment condition $E[Z_i(Y_i - X_i'\\beta)] = 0$ defines both:\n\n| Method | Weighting matrix $\\hat{W}$ | When optimal? |\n|--------|---------------------------|---------------|\n| IV (just-identified) | $(Z'Z/n)^{-1}$ | Always (unique solution) |\n| 2SLS | $(Z'Z/n)^{-1}$ | Under homoskedasticity |\n| Efficient GMM | $\\hat{\\Omega}^{-1}$ | Under heteroskedasticity |\n\nUnder homoskedasticity, 2SLS is efficient among all IV estimators. Under heteroskedasticity, efficient GMM does better by weighting moment conditions inversely to their variance.\n\nThe **Sargan/J-test** is the overidentification test of GMM: it checks whether the extra moment conditions (beyond what is needed for identification) are satisfied. We develop the full GMM framework in Chapter 11.\n",
    "supporting": [
      "ch10-iv_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}