{
  "hash": "90425a889c729b9643c54198022766c1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"11. GMM\"\nsubtitle: \"Generalized method of moments\"\n---\n\nThis chapter demonstrates the Generalized Method of Moments (GMM) framework computationally. We show that OLS, IV, and 2SLS are all special cases, then build up to efficient GMM estimation, the J-test, and a missing data application. The key reference is Hansen, Chapter 13.\n\n**Questions this chapter answers:**\n\n1. How does GMM unify OLS, IV, and 2SLS under a single framework of moment conditions?\n2. What is the optimal weighting matrix, and how does efficient GMM achieve the semiparametric variance bound?\n3. How does the J-test detect misspecification in overidentified models?\n4. How can nonlinear GMM handle problems like missing data?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(gmm)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(estimatr)\nlibrary(MASS)\n```\n:::\n\n\n## Everything is a moment condition\n\nEvery estimator we have studied solves a set of moment conditions $\\mathbb{E}[g_i(\\beta)] = 0$. The sample analog sets $\\bar{g}_n(\\hat\\beta) = \\frac{1}{n}\\sum g_i(\\hat\\beta) = 0$.\n\n| Estimator | Moment condition | Solution |\n|-----------|-----------------|----------|\n| OLS | $\\mathbb{E}[X_i(Y_i - X_i'\\beta)] = 0$ | the [OLS estimator](ch03-ols.qmd#eq-ols) |\n| IV | $\\mathbb{E}[Z_i(Y_i - X_i'\\beta)] = 0$ | $(Z'X)^{-1}Z'Y$ |\n| 2SLS | Same as IV, with $W = (Z'Z)^{-1}$ | $(X'P_Z X)^{-1}X'P_Z Y$ |\n\n::: {#def-gmm}\n## GMM Estimator (Hansen 13.1)\nThe GMM estimator minimizes $J(\\beta) = n\\,\\bar{g}_n(\\beta)' W\\, \\bar{g}_n(\\beta)$, where $\\bar{g}_n(\\beta) = \\frac{1}{n}\\sum g_i(\\beta)$ and $W$ is a positive definite weight matrix. Different choices of $W$ yield different estimators; the optimal $W = \\Omega^{-1}$ minimizes asymptotic variance.\n:::\n\n::: {.callout-note}\n## GMM Unifies Everything\nEvery estimator in this course — OLS, GLS, IV, 2SLS, probit MLE — is a GMM estimator with specific moment conditions and weight matrices. The GMM framework provides a single set of tools for estimation, inference, and specification testing.\n:::\n\nLet's verify this computationally. We simulate an IV model with one endogenous regressor and one instrument:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nn <- 500\n\n# DGP: Y = 1 + 0.5*X + e, where X is endogenous\nu <- rnorm(n)\nZ <- rnorm(n)\nX <- 0.8 * Z + u          # X correlated with u\ne <- 0.6 * u + rnorm(n)   # e correlated with u through shared u\nY <- 1 + 0.5 * X + e\n```\n:::\n\n\nOLS is biased because $\\text{Cov}(X, e) \\neq 0$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# OLS: E[X(Y - Xb)] = 0\nbeta_ols <- solve(t(cbind(1, X)) %*% cbind(1, X)) %*% t(cbind(1, X)) %*% Y\ncat(\"OLS slope:\", round(beta_ols[2], 4), \" (biased, true = 0.5)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOLS slope: 0.8725  (biased, true = 0.5)\n```\n\n\n:::\n:::\n\n\nIV uses the instrument $Z$ to form moment conditions $\\mathbb{E}[Z(Y - X\\beta)] = 0$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# IV: E[Z(Y - Xb)] = 0, just-identified (ell = k)\nXmat <- cbind(1, X)\nZmat <- cbind(1, Z)\nbeta_iv <- solve(t(Zmat) %*% Xmat) %*% t(Zmat) %*% Y\ncat(\"IV slope:\", round(beta_iv[2], 4), \" (consistent)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIV slope: 0.4949  (consistent)\n```\n\n\n:::\n:::\n\n\nWhen the number of moment conditions $\\ell$ equals the number of parameters $k$, we can solve exactly. But what happens when $\\ell > k$?\n\n## The overidentification problem\n\nWith more instruments than parameters, no $\\beta$ can simultaneously zero out all moment conditions. We need a principled way to get as close as possible.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now add a second instrument\nZ2 <- 0.5 * X + rnorm(n, sd = 2)  # second instrument (weaker)\nZmat2 <- cbind(1, Z, Z2)           # 3 instruments, 2 parameters\n\n# Can't solve Z'(Y - Xb) = 0 exactly (3 equations, 2 unknowns)\n# Check: the IV formula with the full Z matrix is not square\ncat(\"Z'X dimensions:\", nrow(t(Zmat2) %*% Xmat), \"x\", ncol(t(Zmat2) %*% Xmat), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nZ'X dimensions: 3 x 2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"(not square -- can't invert directly)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(not square -- can't invert directly)\n```\n\n\n:::\n:::\n\n\n### The GMM criterion function\n\nFor a positive definite weight matrix $W$, the GMM criterion is:\n\n$$J(\\beta) = n\\,\\bar{g}_n(\\beta)' W\\, \\bar{g}_n(\\beta)$$ {#eq-gmm-criterion}\n\nThe GMM estimator minimizes this weighted quadratic form.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# GMM criterion function for the linear IV model\ngmm_criterion <- function(beta, Y, X, Z, W) {\n  g_bar <- colMeans(Z * as.numeric(Y - X %*% beta))  # sample moments\n  n <- nrow(X)\n  n * t(g_bar) %*% W %*% g_bar\n}\n\n# Closed-form GMM estimator: (X'Z W Z'X)^{-1} (X'Z W Z'Y)\ngmm_linear <- function(Y, X, Z, W) {\n  XZ <- t(X) %*% Z\n  solve(XZ %*% W %*% t(XZ)) %*% XZ %*% W %*% t(Z) %*% Y\n}\n```\n:::\n\n\nDifferent weight matrices yield different estimators:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identity matrix\nW_ident <- diag(3)\nbeta_gmm_ident <- gmm_linear(Y, Xmat, Zmat2, W_ident)\n\n# (Z'Z)^{-1} -- this gives 2SLS\nW_2sls <- solve(t(Zmat2) %*% Zmat2)\nbeta_gmm_2sls <- gmm_linear(Y, Xmat, Zmat2, W_2sls)\n\n# Verify 2SLS matches\nPZ <- Zmat2 %*% solve(t(Zmat2) %*% Zmat2) %*% t(Zmat2)\nbeta_2sls <- solve(t(Xmat) %*% PZ %*% Xmat) %*% t(Xmat) %*% PZ %*% Y\n\ncat(\"GMM (W = I):     slope =\", round(beta_gmm_ident[2], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGMM (W = I):     slope = 0.6327 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"GMM (W = 2SLS):  slope =\", round(beta_gmm_2sls[2], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGMM (W = 2SLS):  slope = 0.5307 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"2SLS directly:   slope =\", round(beta_2sls[2], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2SLS directly:   slope = 0.5307 \n```\n\n\n:::\n:::\n\n\n::: {#thm-gmm-2sls}\n## 2SLS as GMM (Hansen 13.2)\nWhen $W = (Z'Z/n)^{-1}$, the GMM estimator for the linear IV model reduces to 2SLS: $\\hat\\beta_{2SLS} = (X'P_Z X)^{-1}X'P_Z Y$. Under homoskedasticity, 2SLS is efficient among all GMM estimators.\n:::\n\nThe choice of $W$ matters when the model is overidentified---but the optimal choice is $W = \\Omega^{-1}$, where $\\Omega = \\mathbb{E}[Z_i Z_i' e_i^2]$ is the variance of the moment conditions.\n\n## Efficient GMM by hand {#sec-efficient-gmm}\n\nThe optimal weight matrix $W = \\Omega^{-1}$ minimizes the asymptotic variance of the GMM estimator. The two-step procedure is:\n\n::: {#thm-efficient-gmm}\n## Efficient GMM (Hansen 13.4-13.5)\nThe efficient GMM estimator uses $W = \\hat\\Omega^{-1}$, where $\\hat\\Omega = \\frac{1}{n}\\sum g_i(\\tilde\\beta)g_i(\\tilde\\beta)'$ and $\\tilde\\beta$ is a preliminary consistent estimator. This achieves the semiparametric efficiency bound among all estimators using the same moment conditions.\n:::\n\n1. Estimate $\\beta$ by 2SLS (using $W = (Z'Z)^{-1}$). Compute residuals $\\tilde{e}_i$.\n2. Estimate $\\hat\\Omega = \\frac{1}{n}\\sum Z_i Z_i' \\tilde{e}_i^2$, set $\\hat{W} = \\hat\\Omega^{-1}$, and re-estimate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 1: 2SLS\nbeta_step1 <- beta_2sls\ne_step1 <- Y - Xmat %*% beta_step1\n\n# Step 2: Estimate optimal weight matrix\nOmega_hat <- (t(Zmat2) %*% diag(as.numeric(e_step1^2)) %*% Zmat2) / n\nW_opt <- solve(Omega_hat)\n\n# Re-estimate with optimal W\nbeta_step2 <- gmm_linear(Y, Xmat, Zmat2, W_opt)\n\ncat(\"Step 1 (2SLS) slope:     \", round(beta_step1[2], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStep 1 (2SLS) slope:      0.5307 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Step 2 (efficient) slope:\", round(beta_step2[2], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStep 2 (efficient) slope: 0.529 \n```\n\n\n:::\n:::\n\n\nWe can iterate---updating $\\hat\\Omega$ and re-estimating---until convergence:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Iterated GMM\nbeta_iter <- beta_2sls\nfor (s in 1:20) {\n  e_s <- Y - Xmat %*% beta_iter\n  Omega_s <- (t(Zmat2) %*% diag(as.numeric(e_s^2)) %*% Zmat2) / n\n  W_s <- solve(Omega_s)\n  beta_new <- gmm_linear(Y, Xmat, Zmat2, W_s)\n  if (max(abs(beta_new - beta_iter)) < 1e-8) {\n    cat(\"Converged in\", s, \"iterations\\n\")\n    break\n  }\n  beta_iter <- beta_new\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConverged in 5 iterations\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Iterated GMM slope:\", round(beta_iter[2], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIterated GMM slope: 0.529 \n```\n\n\n:::\n:::\n\n\n## Using the gmm package\n\nThe `gmm` package handles moment function specification, weighting, and inference. For a linear IV model, we can pass the formula directly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Linear IV via gmm()\n# Formula: Y ~ X | Z  (endogenous ~ instruments)\ndat <- data.frame(Y = Y, X = X, Z1 = Z, Z2 = Z2)\n\n# Two-step GMM\nfit_2step <- gmm(Y ~ X, ~ Z1 + Z2, data = dat, type = \"twoStep\")\nsummary(fit_2step)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\ngmm(g = Y ~ X, x = ~Z1 + Z2, type = \"twoStep\", data = dat)\n\n\nMethod:  twoStep \n\nKernel:  Quadratic Spectral(with bw =  0.57799 )\n\nCoefficients:\n             Estimate    Std. Error  t value     Pr(>|t|)  \n(Intercept)  9.4738e-01  4.6839e-02  2.0226e+01  5.7701e-91\nX            5.3688e-01  5.7361e-02  9.3595e+00  8.0104e-21\n\nJ-Test: degrees of freedom is 1 \n                J-test    P-value \nTest E(g)=0:    4.620103  0.031599\n\nInitial values of the coefficients\n(Intercept)           X \n  0.9419577   0.5306796 \n```\n\n\n:::\n:::\n\n\nCompare to the iterative version:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_iter <- gmm(Y ~ X, ~ Z1 + Z2, data = dat, type = \"iterative\")\ncat(\"Two-step slope:\", round(coef(fit_2step)[\"X\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTwo-step slope: 0.5369 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Iterated slope:\", round(coef(fit_iter)[\"X\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIterated slope: 0.5369 \n```\n\n\n:::\n:::\n\n\nAnd verify against `iv_robust` (which computes 2SLS):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_iv <- iv_robust(Y ~ X | Z1 + Z2, data = dat)\ncat(\"2SLS (iv_robust):\", round(coef(fit_iv)[\"X\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2SLS (iv_robust): 0.5307 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Two-step GMM:    \", round(coef(fit_2step)[\"X\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTwo-step GMM:     0.5369 \n```\n\n\n:::\n:::\n\n\nUnder homoskedasticity, 2SLS and efficient GMM give the same estimates (Hansen Thm 13.6). The differences here arise because we simulated heteroskedastic-style errors.\n\n## When does efficient GMM help? A simulation\n\n2SLS is efficient only under homoskedasticity. Under heteroskedasticity, efficient GMM can reduce variance. Let's demonstrate with a Monte Carlo:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(99)\nB <- 500\nn_sim <- 300\nbeta_true <- 0.5\nresults <- data.frame(method = character(), estimate = numeric())\n\nfor (b in 1:B) {\n  u <- rnorm(n_sim)\n  z1 <- rnorm(n_sim)\n  z2 <- rnorm(n_sim)\n  x <- 0.6 * z1 + 0.3 * z2 + u\n\n  # Heteroskedastic errors: variance depends on z1\n  sigma_i <- 0.5 + abs(z1)\n  e <- sigma_i * rnorm(n_sim) + 0.5 * u\n  y <- 1 + beta_true * x + e\n\n  d <- data.frame(y = y, x = x, z1 = z1, z2 = z2)\n\n  # 2SLS\n  iv_fit <- tryCatch(\n    iv_robust(y ~ x | z1 + z2, data = d, se_type = \"classical\"),\n    error = function(e) NULL\n  )\n\n  # Two-step GMM\n  gmm_fit <- tryCatch(\n    gmm(y ~ x, ~ z1 + z2, data = d, type = \"twoStep\"),\n    error = function(e) NULL\n  )\n\n  if (!is.null(iv_fit) && !is.null(gmm_fit)) {\n    results <- rbind(results,\n      data.frame(method = \"2SLS\", estimate = coef(iv_fit)[\"x\"]),\n      data.frame(method = \"Efficient GMM\", estimate = coef(gmm_fit)[\"x\"])\n    )\n  }\n}\n\n# Compare variances\nvar_table <- aggregate(estimate ~ method, data = results,\n                       FUN = function(x) c(mean = mean(x), var = var(x)))\nvar_table <- cbind(var_table[1], as.data.frame(var_table[[2]]))\nnames(var_table) <- c(\"method\", \"mean\", \"variance\")\nvar_table$efficiency_ratio <- var_table$variance / min(var_table$variance)\nprint(var_table, digits = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         method   mean variance efficiency_ratio\n1          2SLS 0.5025  0.03440            1.067\n2 Efficient GMM 0.5000  0.03224            1.000\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(results, aes(x = estimate, fill = method)) +\n  geom_density(alpha = 0.4) +\n  geom_vline(xintercept = beta_true, linetype = \"dashed\") +\n  labs(title = \"2SLS vs efficient GMM under heteroskedasticity\",\n       subtitle = paste0(\"n = \", n_sim, \", \", B, \" replications\"),\n       x = expression(hat(beta)), y = \"Density\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"steelblue\", \"coral\"))\n```\n\n::: {.cell-output-display}\n![2SLS vs. efficient GMM under heteroskedasticity](ch11-gmm_files/figure-html/efficiency-plot-1.png){width=672}\n:::\n:::\n\n\nUnder heteroskedasticity, efficient GMM has lower variance because it weights moment conditions by the inverse of their variance---upweighting informative, low-noise moments and downweighting noisy ones. Just as the [Gauss-Markov theorem](ch05-gls.qmd#thm-gauss-markov) bounds OLS among linear unbiased estimators, the efficient GMM achieves the semiparametric bound among all estimators using the same moment conditions.\n\n## The J-test for overidentification {#sec-j-test}\n\nWhen $\\ell > k$ (more moment conditions than parameters), the model imposes testable restrictions. If the moment conditions are correctly specified, the minimized criterion value should be small.\n\n$$J = n\\,\\bar{g}_n(\\hat\\beta)'\\hat\\Omega^{-1}\\bar{g}_n(\\hat\\beta) \\xrightarrow{d} \\chi^2_{\\ell - k}$$ {#eq-j-test}\n\n::: {#thm-j-test}\n## J-Test for Overidentification (Hansen 13.14)\nUnder correct specification, the minimized GMM criterion $J = n\\,\\bar{g}_n(\\hat\\beta)'\\hat\\Omega^{-1}\\bar{g}_n(\\hat\\beta) \\xrightarrow{d} \\chi^2_{\\ell - k}$, where $\\ell$ is the number of moment conditions and $k$ is the number of parameters. Rejection indicates that the moment conditions are mutually inconsistent.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# J-test from our earlier fit (valid instruments)\nspecTest(fit_2step)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n ##  J-Test: degrees of freedom is 1  ## \n\n                J-test    P-value \nTest E(g)=0:    4.620103  0.031599\n```\n\n\n:::\n:::\n\n\nHere $\\ell - k = 3 - 2 = 1$ degree of freedom. A large p-value means we fail to reject: the overidentifying restrictions are consistent with the data.\n\n### When the J-test rejects\n\nIf an instrument is invalid (correlated with the error), the J-test should detect this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn <- 500\nu <- rnorm(n)\nz1 <- rnorm(n)\nz2_bad <- rnorm(n)\nx <- 0.6 * z1 + 0.3 * z2_bad + u\ne <- 0.5 * u + 0.4 * z2_bad + rnorm(n)  # z2_bad enters the error!\ny <- 1 + 0.5 * x + e\n\nd_bad <- data.frame(y = y, x = x, z1 = z1, z2 = z2_bad)\nfit_bad <- gmm(y ~ x, ~ z1 + z2, data = d_bad, type = \"twoStep\")\nspecTest(fit_bad)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n ##  J-Test: degrees of freedom is 1  ## \n\n                J-test      P-value   \nTest E(g)=0:    1.4055e+01  1.7758e-04\n```\n\n\n:::\n:::\n\n\nThe J-test has a limitation: if all instruments are invalid in the same direction, the test may not detect the problem. It tests whether the instruments *disagree* with each other, not whether any individual instrument is valid.\n\n::: {.callout-warning}\n## J-Test Limitations\nThe J-test only detects when instruments *disagree* with each other. If all instruments are invalid in the same direction, the test has no power. It tests internal consistency of the overidentifying restrictions, not the validity of any individual instrument.\n:::\n\n### J-test rejection rates by simulation\n\nLet's verify the J-test has correct size (under valid instruments) and power (under an invalid instrument):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nB <- 500\nn_sim <- 300\nrejections <- data.frame(scenario = character(), rejected = logical())\n\nfor (scenario in c(\"valid\", \"invalid\")) {\n  for (b in 1:B) {\n    u <- rnorm(n_sim)\n    z1 <- rnorm(n_sim)\n    z2 <- rnorm(n_sim)\n    x <- 0.6 * z1 + 0.3 * z2 + u\n\n    if (scenario == \"valid\") {\n      e <- 0.5 * u + rnorm(n_sim)\n    } else {\n      e <- 0.5 * u + 0.3 * z2 + rnorm(n_sim)  # z2 in error\n    }\n    y <- 1 + 0.5 * x + e\n\n    d <- data.frame(y = y, x = x, z1 = z1, z2 = z2)\n    fit <- tryCatch(gmm(y ~ x, ~ z1 + z2, data = d, type = \"twoStep\"),\n                    error = function(e) NULL)\n    if (!is.null(fit)) {\n      jtest <- tryCatch(specTest(fit), error = function(e) NULL)\n      if (!is.null(jtest)) {\n        pval <- jtest$test[1, 2]  # p-value\n        rejections <- rbind(rejections,\n          data.frame(scenario = scenario, rejected = pval < 0.05))\n      }\n    }\n  }\n}\n\nrej_rates <- aggregate(rejected ~ scenario, data = rejections, mean)\nnames(rej_rates)[2] <- \"rejection_rate\"\nrej_rates\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  scenario rejection_rate\n1  invalid          0.998\n2    valid          0.068\n```\n\n\n:::\n:::\n\n\nUnder valid instruments, the rejection rate should be near 0.05 (the nominal size). Under the invalid instrument, the rejection rate should be substantially higher.\n\n## Nonlinear GMM: custom moment functions\n\nThe `gmm` package also accepts user-defined moment functions for nonlinear models. The moment function takes parameters $\\theta$ and data $x$, and returns an $n \\times \\ell$ matrix of moment contributions.\n\nHere's a simple example: estimating the mean and variance of a distribution simultaneously using moment conditions $\\mathbb{E}[Y - \\mu] = 0$ and $\\mathbb{E}[(Y - \\mu)^2 - \\sigma^2] = 0$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Moment function: E[Y - mu] = 0 and E[(Y - mu)^2 - sigma2] = 0\ng_meanvar <- function(theta, x) {\n  mu <- theta[1]\n  sigma2 <- theta[2]\n  m1 <- x - mu\n  m2 <- (x - mu)^2 - sigma2\n  cbind(m1, m2)  # n x 2 matrix\n}\n\n# Generate data\nset.seed(1)\ny_data <- rnorm(200, mean = 3, sd = 2)\n\n# GMM estimation (just-identified: 2 moments, 2 parameters)\nfit_mv <- gmm(g_meanvar, x = y_data, t0 = c(0, 1))\ncat(\"GMM estimates:  mu =\", round(coef(fit_mv)[1], 3),\n    \" sigma2 =\", round(coef(fit_mv)[2], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGMM estimates:  mu = 3.071  sigma2 = 3.436 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Sample moments: mean =\", round(mean(y_data), 3),\n    \" var =\", round(var(y_data) * (200-1)/200, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample moments: mean = 3.071  var = 3.436 \n```\n\n\n:::\n:::\n\n\nThe GMM estimates match the sample moments exactly because this is a just-identified model.\n\n## Application: missing data and GMM\n\nThis section implements the Abrevaya & Donald (2017) approach to missing data using GMM. The idea is to exploit moment conditions from *both* complete and incomplete observations.\n\n### The setup\n\nSuppose we want to estimate:\n$$y_i = \\beta_0 + \\alpha \\cdot x_i + \\beta_1 \\cdot z_i + \\varepsilon_i$$\n\nwhere $x_i$ is sometimes missing but $z_i$ is always observed. If $x_i$ has a linear projection on $z_i$:\n$$x_i = \\gamma_0 + \\gamma_1 \\cdot z_i + \\xi_i$$\n\nthen for observations where $x_i$ is missing, we can substitute:\n$$y_i = (\\beta_0 + \\alpha\\gamma_0) + (\\beta_1 + \\alpha\\gamma_1) z_i + (\\varepsilon_i + \\alpha\\xi_i)$$\n\nThis gives us three blocks of moment conditions---and more conditions than parameters (overidentification).\n\n### Simulating the WLS-like data\n\nWe simulate data similar to the Wisconsin Longitudinal Study, where education ($y$) depends on a BMI-related rating ($x$, sometimes missing) and IQ ($z$, always observed):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(314)\nn_wls <- 5000\n\n# Parameters\nbeta_0_true <- 12\nalpha_true <- -0.04\nbeta_iq_true <- 0.06\ngamma_0_true <- 3.5\ngamma_iq_true <- 0.005\n\n# Generate data\niq <- rnorm(n_wls, mean = 100, sd = 15)\nxi <- rnorm(n_wls, sd = 1)\nbmi <- gamma_0_true + gamma_iq_true * iq + xi\neps <- rnorm(n_wls, sd = 1.5)\neduc <- beta_0_true + alpha_true * bmi + beta_iq_true * iq + eps\n\n# Missingness: depends on iq (MAR) but not on eps or xi\nprob_missing <- plogis(-2 + 0.01 * iq)  # ~20% missing overall\nmissing <- rbinom(n_wls, 1, prob_missing)\ncat(\"Fraction missing:\", round(mean(missing), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFraction missing: 0.273 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Set bmi to 0 for missing observations (placeholder)\nbmi_obs <- ifelse(missing == 0, bmi, 0)\n\nwls <- data.frame(educ = educ, bmi = bmi_obs, iq = iq, bmimissing = missing)\n```\n:::\n\n\n### Complete case and dummy variable approaches\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Complete case: drop missing observations\ncc_fit <- lm(educ ~ bmi + iq, data = subset(wls, bmimissing == 0))\n\n# Dummy variable: fill in 0, add missing indicator\ndv_fit <- lm_robust(educ ~ bmi + iq + bmimissing, data = wls)\n\ncat(\"Complete case:   alpha =\", round(coef(cc_fit)[\"bmi\"], 4),\n    \" SE =\", round(summary(cc_fit)$coefficients[\"bmi\", \"Std. Error\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nComplete case:   alpha = -0.0573  SE = 0.0248 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Dummy variable:  alpha =\", round(coef(dv_fit)[\"bmi\"], 4),\n    \" SE =\", round(dv_fit$std.error[\"bmi\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDummy variable:  alpha = -0.0573  SE = 0.0249 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"True alpha:     \", alpha_true, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue alpha:      -0.04 \n```\n\n\n:::\n:::\n\n\n### GMM with three blocks of moment conditions\n\nNow we implement the Abrevaya & Donald GMM estimator. The five parameters are $\\theta = (\\beta_0, \\alpha, \\beta_{iq}, \\gamma_0, \\gamma_{iq})$ and we have seven moment conditions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Moment function: 7 conditions for 5 parameters\ng_missing <- function(theta, x) {\n  beta_0  <- theta[1]\n  alpha   <- theta[2]\n  beta_iq <- theta[3]\n  gamma_0 <- theta[4]\n  gamma_iq <- theta[5]\n\n  educ <- x[, 1]\n  bmi  <- x[, 2]\n  iq   <- x[, 3]\n  m    <- x[, 4]  # missing indicator\n\n  # Block 1: structural equation, observed cases only\n  resid1 <- (1 - m) * (educ - beta_0 - alpha * bmi - beta_iq * iq)\n\n  # Block 2: projection equation, observed cases only\n  resid2 <- (1 - m) * (bmi - gamma_0 - gamma_iq * iq)\n\n  # Block 3: substituted equation, missing cases only\n  resid3 <- m * (educ - (gamma_0 * alpha + beta_0) - (gamma_iq * alpha + beta_iq) * iq)\n\n  cbind(\n    resid1,              # m1: E[(1-m)(y - b0 - a*x - b1*z)] = 0\n    resid1 * bmi,        # m2: E[(1-m)(y - ...)*x] = 0\n    resid1 * iq,         # m3: E[(1-m)(y - ...)*z] = 0\n    resid2,              # m4: E[(1-m)(x - g0 - g1*z)] = 0\n    resid2 * iq,         # m5: E[(1-m)(x - ...)*z] = 0\n    resid3,              # m6: E[m(y - delta'z)] = 0\n    resid3 * iq          # m7: E[m(y - delta'z)*z] = 0\n  )\n}\n```\n:::\n\n\nThe instruments for each block multiply the residuals: Block 1 uses $(1, x, z)$, Block 2 uses $(1, z)$, Block 3 uses $(1, z)$. Total: $3 + 2 + 2 = 7$ moment conditions for 5 parameters, giving us **2 overidentifying restrictions**.\n\n### Starting values and estimation\n\nGood starting values help the optimizer. We use complete-case regressions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Starting values from complete cases\ncc_reg <- lm(educ ~ bmi + iq, data = subset(wls, bmimissing == 0))\nproj_reg <- lm(bmi ~ iq, data = subset(wls, bmimissing == 0))\n\nstart_vals <- c(\n  beta_0   = unname(coef(cc_reg)[\"(Intercept)\"]),\n  alpha    = unname(coef(cc_reg)[\"bmi\"]),\n  beta_iq  = unname(coef(cc_reg)[\"iq\"]),\n  gamma_0  = unname(coef(proj_reg)[\"(Intercept)\"]),\n  gamma_iq = unname(coef(proj_reg)[\"iq\"])\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data matrix\nx_mat <- as.matrix(wls[, c(\"educ\", \"bmi\", \"iq\", \"bmimissing\")])\n\n# Two-step GMM\ngmm_fit <- gmm(\n  g_missing,\n  x = x_mat,\n  t0 = start_vals,\n  type = \"twoStep\",\n  wmatrix = \"ident\",\n  vcov = \"HAC\"\n)\n\nsummary(gmm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\ngmm(g = g_missing, x = x_mat, t0 = start_vals, type = \"twoStep\", \n    wmatrix = \"ident\", vcov = \"HAC\")\n\n\nMethod:  One step GMM with W = identity \n\nKernel:  Quadratic Spectral\n\nCoefficients:\n          Estimate      Std. Error    t value       Pr(>|t|)    \nbeta_0      1.1602e+01    2.1935e+00    5.2894e+00    1.2271e-07\nalpha      -1.7500e-02    1.2947e-01   -1.3516e-01    8.9248e-01\nbeta_iq     6.3019e-02    1.6528e-02    3.8129e+00    1.3734e-04\ngamma_0     3.4071e+00    1.1759e-01    2.8975e+01   1.3572e-184\ngamma_iq    5.8239e-03    1.1712e-03    4.9726e+00    6.6058e-07\n\nJ-Test: degrees of freedom is 2 \n                J-test      P-value   \nTest E(g)=0:    1.9454e+01  5.9644e-05\n\n#############\nInformation related to the numerical optimization\nConvergence code =  1 \nFunction eval. =  502 \nGradian eval. =  NA \n```\n\n\n:::\n:::\n\n\n### Comparing all methods\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomparison <- data.frame(\n  Method = c(\"True\", \"Complete Case\", \"Dummy Variable\", \"GMM\"),\n  alpha = c(\n    alpha_true,\n    coef(cc_fit)[\"bmi\"],\n    coef(dv_fit)[\"bmi\"],\n    coef(gmm_fit)[\"alpha\"]\n  ),\n  SE_alpha = c(\n    NA,\n    summary(cc_fit)$coefficients[\"bmi\", \"Std. Error\"],\n    dv_fit$std.error[\"bmi\"],\n    sqrt(vcov(gmm_fit)[\"alpha\", \"alpha\"])\n  ),\n  beta_iq = c(\n    beta_iq_true,\n    coef(cc_fit)[\"iq\"],\n    coef(dv_fit)[\"iq\"],\n    coef(gmm_fit)[\"beta_iq\"]\n  )\n)\ncomparison$alpha <- round(comparison$alpha, 4)\ncomparison$SE_alpha <- round(comparison$SE_alpha, 4)\ncomparison$beta_iq <- round(comparison$beta_iq, 4)\ncomparison\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Method   alpha SE_alpha beta_iq\n1           True -0.0400       NA  0.0600\n2  Complete Case -0.0573   0.0248  0.0578\n3 Dummy Variable -0.0573   0.0249  0.0577\n4            GMM -0.0175   0.1295  0.0630\n```\n\n\n:::\n:::\n\n\nGMM uses all observations (both complete and incomplete cases), which can improve efficiency. The complete case estimator is consistent but throws away data. The dummy variable approach can be inconsistent depending on the missingness mechanism.\n\n### J-test for the missing data model\n\nThe J-test checks whether the overidentifying restrictions are satisfied---that is, whether the linear projection assumption holds equally for observed and missing subgroups:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspecTest(gmm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n ##  J-Test: degrees of freedom is 2  ## \n\n                J-test      P-value   \nTest E(g)=0:    1.9454e+01  5.9644e-05\n```\n\n\n:::\n:::\n\n\nWith 7 moments and 5 parameters, the J-test has $7 - 5 = 2$ degrees of freedom. A large p-value means we fail to reject the model specification.\n\n### Monte Carlo: comparing estimator properties\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(77)\nB_mc <- 300\nn_mc <- 1000\nmc_results <- data.frame(method = character(), alpha_hat = numeric())\n\nfor (b in 1:B_mc) {\n  iq_b <- rnorm(n_mc, 100, 15)\n  xi_b <- rnorm(n_mc)\n  bmi_b <- gamma_0_true + gamma_iq_true * iq_b + xi_b\n  eps_b <- rnorm(n_mc, sd = 1.5)\n  educ_b <- beta_0_true + alpha_true * bmi_b + beta_iq_true * iq_b + eps_b\n\n  prob_m <- plogis(-2 + 0.01 * iq_b)\n  m_b <- rbinom(n_mc, 1, prob_m)\n  bmi_obs_b <- ifelse(m_b == 0, bmi_b, 0)\n\n  # Complete case\n  cc_b <- lm(educ_b[m_b == 0] ~ bmi_b[m_b == 0] + iq_b[m_b == 0])\n\n  # Dummy variable\n  dv_b <- lm(educ_b ~ bmi_obs_b + iq_b + m_b)\n\n  # GMM\n  x_b <- cbind(educ_b, bmi_obs_b, iq_b, m_b)\n  proj_b <- lm(bmi_b[m_b == 0] ~ iq_b[m_b == 0])\n  start_b <- c(beta_0 = unname(coef(cc_b)[1]),\n               alpha = unname(coef(cc_b)[2]),\n               beta_iq = unname(coef(cc_b)[3]),\n               gamma_0 = unname(coef(proj_b)[1]),\n               gamma_iq = unname(coef(proj_b)[2]))\n\n  gmm_b <- tryCatch(\n    gmm(g_missing, x = x_b, t0 = start_b, type = \"twoStep\",\n        wmatrix = \"ident\", vcov = \"HAC\"),\n    error = function(e) NULL\n  )\n\n  mc_results <- rbind(mc_results,\n    data.frame(method = \"Complete Case\", alpha_hat = unname(coef(cc_b)[2])),\n    data.frame(method = \"Dummy Variable\", alpha_hat = unname(coef(dv_b)[2]))\n  )\n  if (!is.null(gmm_b)) {\n    mc_results <- rbind(mc_results,\n      data.frame(method = \"GMM\", alpha_hat = unname(coef(gmm_b)[\"alpha\"])))\n  }\n}\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmc_summary <- aggregate(alpha_hat ~ method, data = mc_results,\n                        FUN = function(x) c(\n                          bias = mean(x) - alpha_true,\n                          variance = var(x),\n                          mse = mean((x - alpha_true)^2)\n                        ))\nmc_summary <- cbind(mc_summary[1], round(as.data.frame(mc_summary[[2]]), 6))\nnames(mc_summary) <- c(\"Method\", \"Bias\", \"Variance\", \"MSE\")\nmc_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Method     Bias Variance      MSE\n1  Complete Case 0.004402 0.003086 0.003095\n2 Dummy Variable 0.004618 0.003053 0.003064\n3            GMM 0.107638 0.340920 0.351369\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mc_results, aes(x = alpha_hat, fill = method)) +\n  geom_density(alpha = 0.4) +\n  geom_vline(xintercept = alpha_true, linetype = \"dashed\") +\n  labs(title = \"Estimator comparison with missing data\",\n       subtitle = paste0(\"True alpha = \", alpha_true, \", n = \", n_mc,\n                        \", \", B_mc, \" replications\"),\n       x = expression(hat(alpha)), y = \"Density\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"steelblue\", \"coral\", \"forestgreen\"))\n```\n\n::: {.cell-output-display}\n![](ch11-gmm_files/figure-html/missing-mc-plot-1.png){width=672}\n:::\n:::\n\n\n## GMM as a unifying framework\n\nThe progression through the course is a series of generalizations, each nesting the previous:\n\n| Estimator | Moment condition | Weight matrix | When efficient |\n|-----------|-----------------|---------------|----------------|\n| OLS | $\\mathbb{E}[X_i e_i] = 0$ | $(X'X)^{-1}$ (implicit) | Homoskedastic, exogenous |\n| GLS | $\\mathbb{E}[\\bar{X}_i \\Sigma^{-1} e_i] = 0$ | Known $\\Sigma$ | Known error structure |\n| IV | $\\mathbb{E}[Z_i e_i] = 0$ | $\\ell = k$ (unique) | Just-identified |\n| 2SLS | $\\mathbb{E}[Z_i e_i] = 0$ | $(Z'Z)^{-1}$ | Homoskedastic |\n| **GMM** | $\\mathbb{E}[g_i(\\beta)] = 0$ | $\\hat\\Omega^{-1}$ | **Always** (semiparametric bound) |\n\nEvery test we have seen---$t$-tests, $F$-tests, Hausman tests, Sargan tests---is a special case of a GMM test, often valid under weaker assumptions.\n\n### Recovering OLS as GMM\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# OLS via gmm package (instruments = regressors)\nols_gmm <- gmm(Y ~ X, ~ X, data = dat)\nols_lm <- lm(Y ~ X, data = dat)\ncat(\"GMM (as OLS):\", round(coef(ols_gmm)[\"X\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nGMM (as OLS): 0.8725 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"lm():        \", round(coef(ols_lm)[\"X\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlm():         0.8725 \n```\n\n\n:::\n:::\n\n\nWhen the instruments are the regressors themselves ($Z = X$), the model is just-identified and GMM reduces to OLS regardless of the weight matrix.\n\n## Connection to MTE\n\nThe Marginal Treatment Effect (MTE) framework (Heckman & Vytlacil, 2005) uses GMM to estimate treatment effects that vary across the population:\n\n$$\\Delta^{MTE}(u_D) = \\mathbb{E}[Y_1 - Y_0 \\mid U_D = u_D]$$\n\nwhere $U_D$ indexes resistance to treatment. The LATE estimated by IV is a weighted average of the MTE curve over the complier population. GMM estimation of MTE involves:\n\n- **Nonlinear moment conditions**: $\\mathbb{E}[Y \\mid X, Z]$ depends on $\\int_0^{P(Z)} \\Delta^{MTE}(u)\\,du$, which is nonlinear in the MTE parameters.\n- **Overidentification from multiple instruments**: more instruments than MTE parameters.\n- **J-test**: tests whether the MTE specification (e.g., polynomial degree) fits the data.\n\nThe `ivmte` package (Shea & Torgovitsky) implements this under the hood. Conceptually, the call looks like:\n\n```r\n# Not run -- requires ivmte package and appropriate data\nlibrary(ivmte)\nresult <- ivmte(\n  data = df,\n  outcome = \"y\",\n  treatment = \"d\",\n  instrument = \"z\",\n  target = \"ate\",             # or \"att\", \"late\", \"prte\"\n  m0 = ~ u + I(u^2),         # MTE polynomial for control\n  m1 = ~ u + I(u^2),         # MTE polynomial for treated\n  propensity = d ~ z\n)\n```\n\nThe key insight from GMM: by specifying moment conditions rather than a full parametric model, we can estimate flexible treatment effect heterogeneity while maintaining testable restrictions via the J-test.\n\n## Summary\n\n- **GMM is a unifying framework**: OLS, GLS, IV, and 2SLS are all special cases of GMM with specific weight matrices.\n- **Efficient GMM** sets $W = \\hat\\Omega^{-1}$ (the inverse of the moment covariance), minimizing asymptotic variance. Two-step and iterated procedures achieve this.\n- **2SLS is efficient only under homoskedasticity**. Under heteroskedasticity, efficient GMM provides tighter estimates.\n- **The J-test** is a natural diagnostic for overidentified models. It tests whether the moment conditions are mutually consistent.\n- **Nonlinear GMM** (custom moment functions) handles problems like missing data (Abrevaya & Donald) and MTE estimation.\n- **The semiparametric efficiency bound** (Chamberlain, 1987): efficient GMM extracts the maximum information from moment conditions without distributional assumptions.\n",
    "supporting": [
      "ch11-gmm_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}