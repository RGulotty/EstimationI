{
  "hash": "ddd85d6db0cb80e5f37d02b15fa15dac",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"4. Sensitivity and Leverage\"\nsubtitle: \"Frisch-Waugh-Lovell, partial R², and influential observations\"\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(carData)\noptions(digits = 3)\ntr <- function(M) sum(diag(M))\ndata(Prestige)\n```\n:::\n\n\nIn applied work we rarely care about every regressor equally. We have a treatment or variable of interest ($X_1$) and controls we include to avoid omitted variable bias ($X_2$). Partitioning $X = [X_1 \\; X_2]$ lets us answer three questions: What is the formula for $\\hat\\beta_1$ holding $X_2$ constant? What happens if we omit $X_2$? And how sensitive is $\\hat\\beta_1$ to confounders we cannot observe?\n\n**Questions this chapter answers:**\n\n1. How does the Frisch-Waugh-Lovell theorem decompose a multivariate regression into residual-on-residual regressions?\n2. Which observations exert the most influence on OLS estimates, and how does leverage measure this?\n3. How do sensitivity analysis tools (Cinelli-Hazlett) quantify robustness to unobserved confounders?\n\n## The Frisch-Waugh-Lovell theorem {#sec-fwl}\n\nThe FWL theorem says: the coefficient $\\hat\\beta_2$ from the full regression $y = X_1\\beta_1 + X_2\\beta_2 + e$ is identical to the coefficient from regressing the residualized outcome on the residualized treatment — after partialling out $X_1$ from both.\n\nIn matrix terms, let $M_1 = I - X_1(X_1'X_1)^{-1}X_1'$ be the annihilator for $X_1$. Then:\n\n$$\\hat\\beta_2 = (X_2'M_1 X_2)^{-1} X_2' M_1 y$$ {#eq-fwl}\n\nThis is just OLS on the residuals $M_1 y$ and $M_1 X_2$ — the parts of $y$ and $X_2$ that $X_1$ cannot explain.\n\n::: {#thm-fwl}\n## Frisch-Waugh-Lovell Theorem\nThe coefficient $\\hat\\beta_2$ from the full regression $y = X_1\\beta_1 + X_2\\beta_2 + e$ equals the coefficient from regressing $M_1 y$ on $M_1 X_2$, where $M_1 = I - X_1(X_1'X_1)^{-1}X_1'$. That is: partial out the controls from both sides, then run OLS.\n:::\n\n::: {.callout-note}\n## FWL as a Bridge Between Chapters\nThe FWL theorem connects OLS geometry (Chapter 3) to applied causal inference. Partial regression plots — residualized $Y$ vs. residualized $X$ — visualize the multivariate coefficient in two dimensions. This same logic underlies fixed effects estimation (Chapter 12): demeaning within groups is FWL with group dummies as controls.\n:::\n\nLet's verify with the Prestige data. We'll show that the coefficient on `education` from a regression controlling for `income` and `women` is the same as the coefficient from the residual-on-residual regression:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Full regression\nmod_full <- lm(prestige ~ education + income + women, data = Prestige)\n\n# Step 1: Residualize both y and education against (income, women)\ne_y <- resid(lm(prestige ~ income + women, data = Prestige))\ne_educ <- resid(lm(education ~ income + women, data = Prestige))\n\n# Step 2: Regress residuals on residuals\nmod_fwl <- lm(e_y ~ e_educ)\n\nc(full_regression = coef(mod_full)[\"education\"],\n  FWL = coef(mod_fwl)[\"e_educ\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfull_regression.education                FWL.e_educ \n                     4.19                      4.19 \n```\n\n\n:::\n:::\n\n\nIdentical. FWL tells us that the coefficient on education reflects only the variation in education *not* explained by income and women.\n\n### FWL with matrices\n\nLet's do it with the projection and annihilator matrices directly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- Prestige$prestige\nn <- nrow(Prestige)\n\n# X1 = controls (intercept, income, women)\nX1 <- cbind(1, Prestige$income, Prestige$women)\n# X2 = variable of interest (education)\nX2 <- Prestige$education\n\n# Build the annihilator for X1\nP1 <- X1 %*% solve(crossprod(X1)) %*% t(X1)\nM1 <- diag(n) - P1\n\n# FWL formula: beta_2 = (X2'M1 X2)^{-1} X2'M1 y\nbeta_fwl <- as.numeric(solve(t(X2) %*% M1 %*% X2) %*% t(X2) %*% M1 %*% y)\n\nc(matrix_FWL = beta_fwl, lm = coef(mod_full)[\"education\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  matrix_FWL lm.education \n        4.19         4.19 \n```\n\n\n:::\n:::\n\n\n## Plotting partial effects\n\nOne practical benefit of FWL: it lets us visualize relationships from a multivariate regression in two dimensions. After partialling out the controls, we can scatter the residualized $y$ against the residualized $x$ and draw the partial regression line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_partial <- data.frame(educ_resid = e_educ, prestige_resid = e_y,\n                         job = rownames(Prestige))\n\nggplot(df_partial, aes(educ_resid, prestige_resid)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"steelblue\", linewidth = 1) +\n  geom_text(data = df_partial[abs(df_partial$prestige_resid) > 15, ],\n            aes(label = job), hjust = -0.1, size = 2.5, alpha = 0.7) +\n  labs(x = \"Education residual (net of income, women)\",\n       y = \"Prestige residual (net of income, women)\",\n       title = \"Partial regression plot: education → prestige\",\n       subtitle = paste0(\"Slope = \", round(coef(mod_fwl)[2], 2),\n                         \" (same as the multivariate coefficient)\")) +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Partial regression plot: education → prestige (FWL residuals)](ch04-sensitivity_files/figure-html/partial-plot-1.png){width=672}\n:::\n:::\n\n\nThe slope of this line *is* the multivariate regression coefficient. Each point shows an occupation's education and prestige after removing what income and gender composition predict. Ministers have high prestige residuals — more prestige than their income and gender composition would suggest.\n\n## Leverage: which observations pull the line?\n\nThe diagonal elements of the hat matrix $P = X(X'X)^{-1}X'$ measure **leverage** — how unusual each observation's $X$ values are relative to the rest of the data. The $i$-th leverage value is:\n\n$$h_{ii} = X_i'(X'X)^{-1}X_i$$\n\n::: {#def-leverage}\n## Leverage\nThe leverage of observation $i$ is $h_{ii} = X_i'(X'X)^{-1}X_i$, the $i$-th diagonal element of the hat matrix $P$. It measures how unusual the observation's covariates are: $K/n \\leq h_{ii} \\leq 1$, and $\\sum h_{ii} = K$.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- cbind(1, Prestige$education, Prestige$income, Prestige$women)\nP <- X %*% solve(crossprod(X)) %*% t(X)\nK <- ncol(X)\n\n# Leverage = diagonal of P\nh <- diag(P)\n\n# hatvalues() gives the same thing\nall.equal(h, as.numeric(hatvalues(mod_full)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# Label with occupation names for later use\nnames(h) <- rownames(Prestige)\n\n# Properties: leverage is between 0 and 1, sums to K\nc(min = min(h), max = max(h), sum = sum(h), K = K)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   min    max    sum      K \n0.0104 0.3422 4.0000 4.0000 \n```\n\n\n:::\n:::\n\n\nA regression is **balanced** when leverage values are roughly equal at $K/n$. Observations far from the center of the $X$ space have high leverage — they pull the regression line toward them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_lev <- data.frame(leverage = h, job = rownames(Prestige))\n\nggplot(df_lev, aes(x = reorder(job, leverage), y = leverage)) +\n  geom_point(size = 0.8) +\n  geom_hline(yintercept = K / n, linetype = \"dashed\", color = \"tomato\") +\n  annotate(\"text\", x = 10, y = K / n + 0.005, label = \"K/n (balanced)\",\n           color = \"tomato\", size = 3) +\n  labs(x = \"\", y = \"Leverage (h_ii)\",\n       title = \"Leverage values for Prestige regression\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n```\n\n::: {.cell-output-display}\n![](ch04-sensitivity_files/figure-html/leverage-distribution-1.png){width=576}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Which occupations have the highest leverage?\nhead(sort(h, decreasing = TRUE), 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     general.managers            physicians             ministers \n               0.3422                0.2435                0.1021 \n              lawyers sewing.mach.operators \n               0.0988                0.0978 \n```\n\n\n:::\n:::\n\n\n## Leave-one-out regression\n\nIf we refit the model dropping observation $i$, how much does $\\hat\\beta$ change? The leave-one-out coefficient is:\n\n$$\\hat\\beta_{(-i)} = \\hat\\beta - (X'X)^{-1}X_i \\tilde{e}_i$$\n\nwhere $\\tilde{e}_i = \\hat{e}_i / (1 - h_{ii})$ is the **leave-one-out residual** — the ordinary residual inflated by the leverage. High leverage shrinks the ordinary residual (the observation pulls the line toward itself), so dividing by $(1 - h_{ii})$ corrects for this.\n\n::: {#thm-leave-one-out}\n## Leave-One-Out Formula\nThe leave-one-out coefficient change is $\\hat\\beta_{(-i)} = \\hat\\beta - (X'X)^{-1}X_i \\tilde{e}_i$, where $\\tilde{e}_i = \\hat{e}_i / (1 - h_{ii})$. High leverage shrinks ordinary residuals; dividing by $(1 - h_{ii})$ corrects for this self-influence.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ordinary residuals\ne_hat <- resid(mod_full)\n\n# Leave-one-out residuals\ne_tilde <- e_hat / (1 - h)\n\n# Studentized residuals: leave-one-out residual / its standard error\n# rstudent() uses sigma_{(-i)}, the error variance without obs i\nrst <- rstudent(mod_full)\n\n# Compare the first few\nhead(cbind(ordinary = e_hat, leave_one_out = e_tilde, studentized = rst))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    ordinary leave_one_out studentized\ngov.administrators      4.58          4.71       0.590\ngeneral.managers       -9.39        -14.28      -1.485\naccountants             4.69          4.78       0.601\npurchasing.officers     4.22          4.28       0.540\nchemists                8.15          8.55       1.065\nphysicists              4.47          4.73       0.584\n```\n\n\n:::\n:::\n\n\nAn observation is **influential** if it has both high leverage and a large residual. The change in fitted values when observation $i$ is dropped is:\n\n$$\\hat{Y}_i - \\tilde{Y}_i = h_{ii} \\tilde{e}_i$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_infl <- data.frame(leverage = h, rstudent = rst, job = rownames(Prestige))\ndf_infl$flag <- abs(rst) > 2 | h > 3 * K / n\n\nggplot(df_infl, aes(leverage, rstudent)) +\n  geom_point(aes(color = flag), size = 1.5) +\n  geom_text(data = df_infl[df_infl$flag, ],\n            aes(label = job), hjust = -0.1, size = 2.5) +\n  geom_hline(yintercept = c(-2, 2), linetype = \"dashed\", alpha = 0.4) +\n  geom_vline(xintercept = 3 * K / n, linetype = \"dashed\", alpha = 0.4) +\n  scale_color_manual(values = c(\"FALSE\" = \"gray50\", \"TRUE\" = \"tomato\"),\n                     guide = \"none\") +\n  labs(x = \"Leverage (h_ii)\", y = \"Studentized residual\",\n       title = \"Influential observations: high leverage AND large residual\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](ch04-sensitivity_files/figure-html/influential-obs-1.png){width=576}\n:::\n:::\n\n\nObservations in the upper-right or lower-right are candidates for investigation: they have unusual $X$ values *and* don't fit the model well. This could indicate a data error, a different population, or a genuinely interesting case.\n\n## Regression weights: which observations matter most?\n\nThe FWL result reveals that OLS assigns implicit weights to observations. For a single variable of interest $z$ in a regression with controls $X$, the coefficient is:\n\n$$b = \\frac{\\sum z_i^* y_i}{\\sum z_i^{*2}}$$\n\nwhere $z_i^* = (Mz)_i$ is the residual from regressing $z$ on the controls. Observations where $z_i^{*2}$ is large — where the variable of interest has a lot of variation *not explained by controls* — receive the most weight.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Women's share in occupation: variable of interest\n# Controls: intercept, education, income\nz_star <- as.vector(M1 %*% Prestige$women)  # M1 already built above\n\n# Regression weights\nomega <- z_star^2\nnames(omega) <- rownames(Prestige)\n\n# Most and least weighted occupations\ncat(\"Highest weight (most variation in women% net of controls):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHighest weight (most variation in women% net of controls):\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(sort(omega, decreasing = TRUE), 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              physicians         general.managers osteopaths.chiropractors \n                4.53e-26                 2.33e-26                 1.67e-26 \n                 lawyers             farm.workers \n                1.57e-26                 9.39e-27 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nLowest weight (almost no unique variation):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLowest weight (almost no unique variation):\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(sort(omega), 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n construction.foremen          receptionsts              athletes \n             3.77e-32              1.97e-31              8.91e-31 \ncommercial.travellers     sales.supervisors \n             1.08e-30              1.77e-30 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Verify: weighted formula gives same coefficient as lm\nb_weighted <- sum(z_star * y) / sum(z_star^2)\nc(weighted_formula = b_weighted, lm = coef(mod_full)[\"women\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nweighted_formula         lm.women \n       -4.26e+14        -8.91e-03 \n```\n\n\n:::\n:::\n\n\nOccupations like general managers and ministers — where the share of women is very different from what education and income would predict — have thousands of times more influence on the coefficient than occupations where women's share is well-predicted by the controls.\n\n## Partial $R^2$\n\nThe partial $R^2$ measures how much of the *remaining* variance in $Y$ (after accounting for $X_1$) is explained by $X_2$:\n\n$$R^2_{Y \\sim X_2 | X_1} = 1 - \\frac{\\text{RSS}(X_1, X_2)}{\\text{RSS}(X_1)} = \\frac{\\text{RSS}(X_1) - \\text{RSS}(X_1, X_2)}{\\text{RSS}(X_1)}$$ {#eq-partial-r2}\n\nFor a single variable, the partial $R^2$ equals the squared partial correlation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# RSS from controls only\nmod_controls <- lm(prestige ~ income + women, data = Prestige)\nRSS_controls <- sum(resid(mod_controls)^2)\n\n# RSS from full model\nRSS_full <- sum(resid(mod_full)^2)\n\n# Partial R^2 of education given (income, women)\npartial_r2_educ <- 1 - RSS_full / RSS_controls\n\n# Equivalently: squared correlation of FWL residuals\ncor_fwl <- cor(e_y, e_educ)^2\n\nc(partial_R2 = partial_r2_educ, squared_partial_cor = cor_fwl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         partial_R2 squared_partial_cor \n              0.542               0.542 \n```\n\n\n:::\n:::\n\n\nWe can also compute partial $R^2$ for each variable using the matrix formula. The FWL residuals give us everything we need:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Partial R^2 for each variable\npartial_r2 <- function(mod, var_name) {\n  formula_reduced <- update(formula(mod), paste(\"~ . -\", var_name))\n  mod_reduced <- lm(formula_reduced, data = Prestige)\n  1 - sum(resid(mod)^2) / sum(resid(mod_reduced)^2)\n}\n\ndata.frame(\n  variable = c(\"education\", \"income\", \"women\"),\n  partial_R2 = sapply(c(\"education\", \"income\", \"women\"),\n                       function(v) partial_r2(mod_full, v))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           variable partial_R2\neducation education   0.542079\nincome       income   0.185784\nwomen         women   0.000874\n```\n\n\n:::\n:::\n\n\nEducation has a high partial $R^2$ — it explains a large share of prestige variation that income and women's share cannot. Women's share has a very low partial $R^2$: once we know education and income, knowing the gender composition adds almost nothing.\n\n## Sensitivity analysis: Cinelli-Hazlett (2020)\n\nEven after including controls, there may be *unobserved* confounders. The OVB formula from Chapter 2 says the bias from omitting a variable $Z$ is:\n\n$$\\text{bias} = \\underbrace{\\frac{\\text{Cov}(D, Z)}{\\text{Var}(D)}}_{\\text{imbalance}} \\times \\underbrace{\\frac{\\text{Cov}(Z^{\\perp D}, Y^{\\perp D})}{\\text{Var}(Z^{\\perp D})}}_{\\text{impact}}$$\n\nCinelli and Hazlett (2020) reparameterize this in terms of partial $R^2$ values, which are easier to reason about:\n\n$$|\\text{bias}| \\propto \\sqrt{\\frac{R^2_{D \\sim Z} \\cdot R^2_{Y \\sim Z|D}}{1 - R^2_{D \\sim Z}}}$$\n\nThe key insight: a confounder must predict *both* treatment and outcome to generate meaningful bias. If either partial $R^2$ is small, the bias is small.\n\nLet's simulate and run a sensitivity analysis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nN <- 1000\nbeta_D <- 2; beta_Z <- 3\n\nD <- rbinom(N, 1, 0.5)\nZ <- rnorm(N)\nY <- beta_D * D + beta_Z * Z + rnorm(N)\n\n# Naive model (omitting Z) -- biased\ncoef(lm(Y ~ D))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)           D \n     0.0569      1.9519 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Full model (including Z) -- unbiased\nmodel_full_sim <- lm(Y ~ D + Z)\ncoef(model_full_sim)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)           D           Z \n     0.0254      1.9417      3.0600 \n```\n\n\n:::\n:::\n\n\nThe naive model overestimates the effect of $D$. The full model recovers $\\beta_D \\approx 2$. But what if there were *another* confounder we couldn't observe? We use the observed confounder $Z$ as a benchmark:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How strong is Z as a confounder?\n# Partial R^2 of Z on D\npartial_r2_DZ <- cor(D, Z)^2  # for a single variable, partial R^2 ≈ cor^2\n\n# Partial R^2 of Z on Y|D\nRSS_D_only <- sum(resid(lm(Y ~ D))^2)\nRSS_full_sim <- sum(resid(model_full_sim)^2)\npartial_r2_YZ_D <- 1 - RSS_full_sim / RSS_D_only\n\nc(R2_D_Z = partial_r2_DZ, R2_Y_Z_given_D = partial_r2_YZ_D)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        R2_D_Z R2_Y_Z_given_D \n      2.78e-06       9.07e-01 \n```\n\n\n:::\n:::\n\n\nAn unobserved confounder would need partial $R^2$ values at least this large with both $D$ and $Y$ to generate comparable bias. If the strongest observed predictor explains only a few percent of residual variation, an omitted variable would need to be far stronger to overturn the result.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The sensemakr package automates this analysis\nif (requireNamespace(\"sensemakr\", quietly = TRUE)) {\n  library(sensemakr)\n  sens <- sensemakr(model_full_sim, treatment = \"D\",\n                    benchmark_covariates = \"Z\")\n  summary(sens)\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSensitivity Analysis to Unobserved Confounding\n\nModel Formula: Y ~ D + Z\n\nNull hypothesis: q = 1 and reduce = TRUE \n-- This means we are considering biases that reduce the absolute value of the current estimate.\n-- The null hypothesis deemed problematic is H0:tau = 0 \n\nUnadjusted Estimates of 'D': \n  Coef. estimate: 1.94 \n  Standard Error: 0.062 \n  t-value (H0:tau = 0): 31.2 \n\nSensitivity Statistics:\n  Partial R2 of treatment with outcome: 0.494 \n  Robustness Value, q = 1: 0.614 \n  Robustness Value, q = 1, alpha = 0.05: 0.592 \n\nVerbal interpretation of sensitivity statistics:\n\n-- Partial R2 of the treatment with the outcome: an extreme confounder (orthogonal to the covariates) that explains 100% of the residual variance of the outcome, would need to explain at least 49.4% of the residual variance of the treatment to fully account for the observed estimated effect.\n\n-- Robustness Value, q = 1: unobserved confounders (orthogonal to the covariates) that explain more than 61.4% of the residual variance of both the treatment and the outcome are strong enough to bring the point estimate to 0 (a bias of 100% of the original estimate). Conversely, unobserved confounders that do not explain more than 61.4% of the residual variance of both the treatment and the outcome are not strong enough to bring the point estimate to 0.\n\n-- Robustness Value, q = 1, alpha = 0.05: unobserved confounders (orthogonal to the covariates) that explain more than 59.2% of the residual variance of both the treatment and the outcome are strong enough to bring the estimate to a range where it is no longer 'statistically different' from 0 (a bias of 100% of the original estimate), at the significance level of alpha = 0.05. Conversely, unobserved confounders that do not explain more than 59.2% of the residual variance of both the treatment and the outcome are not strong enough to bring the estimate to a range where it is no longer 'statistically different' from 0, at the significance level of alpha = 0.05.\n\nBounds on omitted variable bias:\n\n--The table below shows the maximum strength of unobserved confounders with association with the treatment and the outcome bounded by a multiple of the observed explanatory power of the chosen benchmark covariate(s).\n\n Bound Label R2dz.x R2yz.dx Treatment Adjusted Estimate Adjusted Se Adjusted T\n        1x Z      0       1         D              1.94           0        Inf\n Adjusted Lower CI Adjusted Upper CI\n              1.94              1.94\n```\n\n\n:::\n:::\n\n\nThe **robustness value** tells us the minimum strength an unobserved confounder must have (in terms of partial $R^2$ with both $D$ and $Y$) to explain away the entire estimated effect.\n\n::: {.callout-warning}\n## Sensitivity Does Not Prove Robustness\nA large robustness value means the result survives *hypothetical* confounders of a given strength — but it cannot rule out their existence. Sensitivity analysis quantifies what *would* be needed to overturn a finding; it does not establish that no such confounder exists.\n:::\n\n## Summary\n\n- **FWL theorem**: The coefficient on $X_2$ in $y = X_1\\beta_1 + X_2\\beta_2 + e$ equals the coefficient from regressing $M_1 y$ on $M_1 X_2$ — residualize both sides against the controls.\n- **Partial regression plots** let us visualize multivariate relationships in 2D using FWL residuals.\n- **Leverage** $h_{ii} = X_i'(X'X)^{-1}X_i$ measures how unusual observation $i$'s covariates are. High leverage + large residual = influential observation.\n- **Leave-one-out residuals** $\\tilde{e}_i = \\hat{e}_i/(1-h_{ii})$ correct for the self-influence of observation $i$.\n- **Regression weights**: OLS implicitly weights observations by $z_i^{*2}$, the squared residual from regressing the variable of interest on controls. Observations with more unique variation in the treatment get more weight.\n- **Partial $R^2$** measures the share of residual variance explained by a variable after accounting for other regressors.\n- **Sensitivity analysis** (Cinelli-Hazlett): bias from an omitted confounder depends on its partial $R^2$ with both treatment and outcome. Use observed covariates as benchmarks.\n\nNext: [Efficiency and GLS](ch05-gls.qmd) — the Gauss-Markov theorem and generalized least squares.\n",
    "supporting": [
      "ch04-sensitivity_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}