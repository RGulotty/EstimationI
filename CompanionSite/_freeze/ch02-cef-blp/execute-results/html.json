{
  "hash": "65128102e94e87b9e3988c11d3eacf72",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"2. The CEF and Best Linear Predictor\"\nsubtitle: \"Why regression approximates the CEF\"\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\noptions(digits = 3)\n```\n:::\n\n\nThe conditional expectation function $m(X) = \\mathbb{E}[Y|X]$ is the best predictor of $Y$ given $X$ — but we rarely know it. The best linear predictor (BLP) is the next best thing: a linear approximation that only requires knowing means, variances, and covariances. This chapter explores the gap between the two and what we gain (and lose) by going linear.\n\n**Questions this chapter answers:**\n\n1. What is the best predictor of $Y$ given $X$, and why is it the conditional expectation function?\n2. How does the best linear predictor (BLP) approximate a nonlinear CEF?\n3. When does the BLP equal the CEF, and why does joint normality guarantee linearity?\n4. How does omitted variable bias arise, and why don't additional controls always help?\n\n## CEF error: mean independence is not independence\n\nDefine the CEF error as $e = Y - m(X)$. By construction, $\\mathbb{E}[e|X] = 0$ — this is called mean independence. But mean independence is weaker than full independence: the *variance* of $e$ can still depend on $X$.\n\nHere's the lecture's example: $X \\sim \\text{Uniform}(-1, 1)$ and $Y = X^2 + X\\varepsilon$, where $\\varepsilon \\sim N(0,1)$ is independent of $X$.\n\n::: {#def-cef}\n## Conditional Expectation Function\nThe CEF is $m(X) = \\mathbb{E}[Y|X]$, the function that gives the mean of $Y$ for each value of $X$. The CEF error $e = Y - m(X)$ satisfies mean independence: $\\mathbb{E}[e|X] = 0$.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(307)\nn <- 5000\nx <- runif(n, -1, 1)\neps <- rnorm(n)\ny <- x^2 + x * eps\n\n# CEF is m(X) = X^2\nm_x <- x^2\ne <- y - m_x\n\n# Mean independence: E[e|X] ≈ 0 in every bin\nbins <- cut(x, breaks = 20)\nbin_means <- tapply(e, bins, mean)\nround(bin_means, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         (-1,-0.9]        (-0.9,-0.8]        (-0.8,-0.7]        (-0.7,-0.6] \n            -0.062              0.036              0.036              0.070 \n       (-0.6,-0.5]        (-0.5,-0.4]        (-0.4,-0.3]        (-0.3,-0.2] \n            -0.006              0.021              0.012              0.018 \n       (-0.2,-0.1]   (-0.1,-0.000463] (-0.000463,0.0995]     (0.0995,0.199] \n            -0.009              0.000              0.002              0.006 \n     (0.199,0.299]      (0.299,0.399]      (0.399,0.499]      (0.499,0.599] \n            -0.007             -0.031             -0.056             -0.037 \n     (0.599,0.699]      (0.699,0.799]      (0.799,0.899]          (0.899,1] \n             0.033             -0.003              0.016              0.026 \n```\n\n\n:::\n:::\n\n\nThe conditional means of $e$ are all near zero. But the conditional *variance* grows with $|X|$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- data.frame(x = x, e = e)\n\nggplot(df, aes(x, e)) +\n  geom_point(alpha = 0.15, size = 0.8) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"steelblue\", linewidth = 1) +\n  labs(x = \"X\", y = \"CEF error (e)\",\n       title = \"Mean independence holds, but errors are heteroskedastic\",\n       subtitle = \"Var(e|X) = X², so spread increases with |X|\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](ch02-cef-blp_files/figure-html/heteroskedastic-errors-1.png){width=672}\n:::\n:::\n\n\nThe smooth line hovers at zero (mean independence), but the spread fans out. This pattern — $\\text{Var}(e|X) = X^2$ — is heteroskedasticity. The CEF decomposes $Y$ into signal and noise, but doesn't guarantee the noise is uniform.\n\n## The CEF is the best predictor {#sec-cef-best}\n\nAmong *all* functions of $X$, the CEF minimizes mean squared prediction error. Let's demonstrate by comparing the CEF to some alternatives for the same DGP.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# True CEF: m(X) = X^2\n# Competitor 1: the unconditional mean\n# Competitor 2: a linear predictor\n# Competitor 3: a cubic predictor\n\nmse_cef <- mean((y - x^2)^2)\nmse_mean <- mean((y - mean(y))^2)\nmse_linear <- mean(residuals(lm(y ~ x))^2)\nmse_cubic <- mean(residuals(lm(y ~ x + I(x^2) + I(x^3)))^2)\n\ndata.frame(\n  predictor = c(\"Unconditional mean\", \"Linear (BLP)\", \"Cubic\", \"CEF (X²)\"),\n  MSE = round(c(mse_mean, mse_linear, mse_cubic, mse_cef), 4)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           predictor   MSE\n1 Unconditional mean 0.401\n2       Linear (BLP) 0.401\n3              Cubic 0.311\n4           CEF (X²) 0.312\n```\n\n\n:::\n:::\n\n\nThe CEF has the lowest MSE. The cubic comes close because $X^3$ has mean zero for symmetric $X$, so the cubic approximation is nearly $X^2$. The linear predictor does worse — it can't capture the curvature.\n\n## CEF vs. BLP: when the line misses the curve\n\nThe BLP minimizes $\\mathbb{E}[(Y - a - bX)^2]$ and yields $b = \\text{Cov}(X,Y)/\\text{Var}(X)$. When the CEF is nonlinear, the BLP draws the best straight line through a curved relationship.\n\n::: {#def-blp}\n## Best Linear Predictor\nThe BLP of $Y$ given $X$ is $\\alpha + \\beta'X$ where $\\beta = \\text{Var}(X)^{-1}\\text{Cov}(X, Y)$. The BLP error satisfies $\\mathbb{E}[Xe] = 0$ (uncorrelated with $X$) but not necessarily $\\mathbb{E}[e|X] = 0$.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Same DGP: Y = X^2 + X*eps\ndf <- data.frame(x = x, y = y)\n\nggplot(df, aes(x, y)) +\n  geom_point(alpha = 0.1, size = 0.5) +\n  stat_function(fun = function(x) x^2, aes(color = \"CEF: X²\"),\n                linewidth = 1.2) +\n  geom_smooth(method = \"lm\", se = FALSE, aes(color = \"BLP\"),\n              linewidth = 1.2) +\n  scale_color_manual(values = c(\"CEF: X²\" = \"tomato\", \"BLP\" = \"steelblue\"),\n                     name = \"\") +\n  labs(x = \"X\", y = \"Y\",\n       title = \"The BLP is the best line through a curved CEF\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![The BLP is the best line through a curved CEF](ch02-cef-blp_files/figure-html/cef-vs-blp-1.png){width=672}\n:::\n:::\n\n\nThe BLP residual $e = Y - X'\\beta$ satisfies $\\mathbb{E}[Xe] = 0$ (uncorrelated with $X$), but *not* $\\mathbb{E}[e|X] = 0$. The residual still contains nonlinear structure: the BLP intercept absorbs $\\mathbb{E}[X^2] = 1/3$, so the conditional mean of the residual traces out $X^2 - 1/3$ rather than zero:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- lm(y ~ x)\ndf$blp_resid <- residuals(mod)\n\nggplot(df, aes(x, blp_resid)) +\n  geom_point(alpha = 0.1, size = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"tomato\", linewidth = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"X\", y = \"BLP residual\",\n       title = \"BLP residuals still contain nonlinear structure\",\n       subtitle = \"E[e|X] ≠ 0, but E[Xe] = 0\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](ch02-cef-blp_files/figure-html/blp-residual-pattern-1.png){width=672}\n:::\n:::\n\n\n## When does BLP = CEF?\n\nThe BLP equals the CEF when the CEF is actually linear. An important sufficient condition: if $(X, Y)$ are jointly normal, the CEF is linear. Let's verify.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nset.seed(42)\n\n# Jointly normal (X, Y)\nSigma <- matrix(c(1, 0.7, 0.7, 1), 2, 2)\njoint <- mvrnorm(5000, mu = c(0, 0), Sigma = Sigma)\nx_norm <- joint[, 1]\ny_norm <- joint[, 2]\n\ndf_norm <- data.frame(x = x_norm, y = y_norm)\n\nggplot(df_norm, aes(x, y)) +\n  geom_point(alpha = 0.1, size = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, aes(color = \"BLP\"), linewidth = 1.2) +\n  geom_smooth(method = \"loess\", se = FALSE, aes(color = \"CEF (loess)\"), linewidth = 1.2) +\n  scale_color_manual(values = c(\"BLP\" = \"steelblue\", \"CEF (loess)\" = \"tomato\"),\n                     name = \"\") +\n  labs(title = \"Jointly normal: the BLP and CEF coincide\",\n       x = \"X\", y = \"Y\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](ch02-cef-blp_files/figure-html/blp-equals-cef-1.png){width=672}\n:::\n:::\n\n\nThe two lines overlap because the CEF is genuinely linear when the data are jointly normal.\n\n::: {.callout-note}\n## Joint Normality Implies Linear CEF\nIf $(X, Y)$ are jointly normal, the conditional expectation $\\mathbb{E}[Y|X]$ is exactly linear in $X$, and the BLP equals the CEF. This is a sufficient but not necessary condition — other distributions can also produce linear CEFs.\n:::\n\n## The BLP in formulas and code\n\nThe population BLP coefficient is $\\beta = \\text{Var}(X)^{-1}\\text{Cov}(X, Y)$. In a sample, OLS estimates this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bivariate: beta = Cov(X,Y) / Var(X)\nbeta_formula <- cov(x_norm, y_norm) / var(x_norm)\nalpha_formula <- mean(y_norm) - beta_formula * mean(x_norm)\n\n# OLS gives the same thing\nbeta_ols <- coef(lm(y_norm ~ x_norm))\n\ncbind(formula = c(alpha_formula, beta_formula),\n      OLS = beta_ols)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             formula      OLS\n(Intercept) -0.00942 -0.00942\nx_norm       0.69753  0.69753\n```\n\n\n:::\n:::\n\n\n## Dummy variables: the CEF for discrete X\n\nWhen $X$ is a categorical variable, the CEF is just group means. Regression with dummies recovers these exactly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2026)\nn <- 300\nparty <- sample(c(\"Dem\", \"Rep\", \"Ind\"), n, replace = TRUE,\n                prob = c(0.4, 0.35, 0.25))\napproval <- 50 + 10 * (party == \"Dem\") - 5 * (party == \"Rep\") + rnorm(n, sd = 8)\n\ndf_party <- data.frame(party = factor(party), approval = approval)\n\n# Group means = the CEF\ntapply(df_party$approval, df_party$party, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Dem  Ind  Rep \n59.6 50.7 45.9 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Regression with dummies (R does this automatically)\ncoef(lm(approval ~ party, data = df_party))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)    partyInd    partyRep \n       59.6        -8.9       -13.7 \n```\n\n\n:::\n:::\n\n\nThe intercept is the mean for the baseline group (alphabetically first: Dem), and the coefficients are differences from that baseline.\n\n## Interactions: when the slope depends on a group\n\nAn interaction term $D_i \\times x_i$ allows the effect of $x$ to differ by group. This is a richer model than parallel slopes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2026)\nn_int <- 400\ngroup <- sample(0:1, n_int, replace = TRUE)\nx_int <- rnorm(n_int)\n# Group 0: slope = 2, Group 1: slope = -1\ny_int <- 3 + 1.5 * group + 2 * x_int - 3 * group * x_int + rnorm(n_int)\n\ndf_int <- data.frame(x = x_int, y = y_int, group = factor(group))\n\n# Parallel slopes (no interaction)\nmod_parallel <- lm(y ~ group + x, data = df_int)\n\n# Interaction model\nmod_interact <- lm(y ~ group * x, data = df_int)\n\nggplot(df_int, aes(x, y, color = group)) +\n  geom_point(alpha = 0.3, size = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1) +\n  scale_color_manual(values = c(\"steelblue\", \"tomato\"), name = \"Group\") +\n  labs(title = \"Interaction: different slopes for each group\",\n       x = \"X\", y = \"Y\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](ch02-cef-blp_files/figure-html/interactions-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(mod_interact)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)      group1           x    group1:x \n       2.91        1.58        1.98       -2.85 \n```\n\n\n:::\n:::\n\n\nThe `group1:x` coefficient ($\\approx -3$) is the *difference* in slopes between group 1 and group 0. The marginal effect of $x$ for group 1 is `x` + `group1:x`.\n\n## Omitted variable bias {#sec-ovb}\n\nIf the true model is $Y = X_1'\\beta_1 + X_2'\\beta_2 + e$ but we only regress on $X_1$, we get $\\gamma_1 = \\beta_1 + \\Gamma_{12}\\beta_2$, where $\\Gamma_{12}$ is the regression of $X_2$ on $X_1$. The bias has a clear direction when we know the signs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2026)\nn_ovb <- 1000\n\n# X2 (institutions) is correlated with X1 (democracy)\nx1 <- rnorm(n_ovb)                        # democracy\nx2 <- 0.6 * x1 + rnorm(n_ovb, sd = 0.8)  # institutions (correlated)\ny_ovb <- 1 + 0.5 * x1 + 2 * x2 + rnorm(n_ovb)  # growth\n\n# True model\ncoef(lm(y_ovb ~ x1 + x2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)          x1          x2 \n      0.967       0.511       1.962 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Short regression (omit x2)\ncoef(lm(y_ovb ~ x1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)          x1 \n      0.942       1.661 \n```\n\n\n:::\n:::\n\n\nThe true effect of democracy ($\\beta_1$) is 0.5, but the short regression gives $\\gamma_1 \\approx 1.7$. The bias is $\\Gamma_{12} \\times \\beta_2 \\approx 0.6 \\times 2 = 1.2$, inflating the estimate because institutions are positively correlated with democracy and positively affect growth.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Verify the OVB formula\ngamma_12 <- coef(lm(x2 ~ x1))[2]  # regression of X2 on X1\nbeta_2 <- coef(lm(y_ovb ~ x1 + x2))[3]\n\nc(short_coef = coef(lm(y_ovb ~ x1))[2],\n  long_coef_plus_bias = coef(lm(y_ovb ~ x1 + x2))[2] + gamma_12 * beta_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         short_coef.x1 long_coef_plus_bias.x1 \n                  1.66                   1.66 \n```\n\n\n:::\n:::\n\n\nThe OVB formula $\\gamma_1 = \\beta_1 + \\Gamma_{12}\\beta_2$ matches exactly.\n\n::: {#thm-ovb}\n## Omitted Variable Bias\nIf the true model is $Y = X_1'\\beta_1 + X_2'\\beta_2 + e$ and we regress $Y$ on $X_1$ alone, the short regression coefficient is $\\gamma_1 = \\beta_1 + \\Gamma_{12}\\beta_2$, where $\\Gamma_{12}$ is the coefficient from regressing $X_2$ on $X_1$. The bias $\\Gamma_{12}\\beta_2$ depends on both the correlation between $X_1$ and $X_2$ and the effect of $X_2$ on $Y$.\n:::\n\n::: {.callout-warning}\n## OVB Is About Population, Not Sample\nThe omitted variable bias formula $\\gamma_1 = \\beta_1 + \\Gamma_{12}\\beta_2$ is an exact population relationship between projection coefficients. It does not require large samples, normality, or any distributional assumption — it follows purely from the algebra of linear projection.\n:::\n\n### Adding variables doesn't always help\n\nA subtlety from the lecture: if the true model has three variables and you can't observe $X_3$, adding $X_2$ can make the bias on $\\beta_1$ worse, depending on the correlation structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2026)\nn_ow <- 1000\nz1 <- rnorm(n_ow)\nz2 <- 0.3 * z1 + rnorm(n_ow)\nz3 <- -0.8 * z1 + 0.5 * z2 + rnorm(n_ow)\ny_ow <- 1 + 1 * z1 + 0.5 * z2 + 3 * z3 + rnorm(n_ow)\n\n# True beta_1 = 1\nc(true = coef(lm(y_ow ~ z1 + z2 + z3))[2],\n  short = coef(lm(y_ow ~ z1))[2],\n  medium = coef(lm(y_ow ~ z1 + z2))[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  true.z1  short.z1 medium.z1 \n    1.015    -0.886    -1.408 \n```\n\n\n:::\n:::\n\n\nWhy does adding $X_2$ make things worse? Apply the OVB formula to the medium regression: omitting $X_3$ biases $\\hat\\beta_1$ by $\\Gamma_{13\\cdot 2}\\beta_3$, where $\\Gamma_{13\\cdot 2}$ is the coefficient on $X_1$ in the regression of $X_3$ on $(X_1, X_2)$. Conditioning on $X_2$ can change the partial correlation between $X_1$ and $X_3$, amplifying the bias.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# OVB decomposition for the medium model\ngamma_13_2 <- coef(lm(z3 ~ z1 + z2))[2]  # partial effect of z1 on z3, controlling for z2\nbeta_3 <- coef(lm(y_ow ~ z1 + z2 + z3))[4]\n\nc(medium_bias = coef(lm(y_ow ~ z1 + z2))[2] - 1,\n  ovb_formula = gamma_13_2 * beta_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmedium_bias.z1 ovb_formula.z1 \n         -2.41          -2.42 \n```\n\n\n:::\n:::\n\n\nDepending on the correlation structure, the \"medium\" model (with $X_2$ but not $X_3$) may be further from the truth than the \"short\" model. More controls are not automatically better.\n\n## Causal interpretation: regression as a weighted average\n\nThe lecture ends with a deep result: when you regress $Y$ on a treatment dummy $D$ and controls $X$, the regression coefficient is a weighted average of the group-specific treatment effects $\\delta_X = \\mathbb{E}[Y|X, D=1] - \\mathbb{E}[Y|X, D=0]$, with weights proportional to the conditional variance of $D$ given $X$.\n\n$$\\delta_R = \\frac{\\mathbb{E}[\\sigma^2_{D|X} \\cdot \\delta_X]}{\\mathbb{E}[\\sigma^2_{D|X}]}$$ {#eq-blp}\n\nGroups where treatment is most variable (closest to a 50/50 split) get the most weight.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2026)\nn_cw <- 5000\n\n# X determines both treatment probability and treatment effect\nx_cw <- runif(n_cw, 0, 1)\nprob_treat <- x_cw^2  # treatment rare for low X, common for high X\nD <- rbinom(n_cw, 1, prob_treat)\n\n# Heterogeneous treatment effect: large for low X, negative for high X\ndelta_x <- 10 - 16 * x_cw   # ranges from +10 at X=0 to -6 at X=1\ny_cw <- 2 + 3 * x_cw + delta_x * D + rnorm(n_cw)\n\n# Regression coefficient\ndelta_R <- coef(lm(y_cw ~ D + x_cw))[2]\n\n# Weighted average with weights = Var(D|X) = p(1-p)\nwt <- prob_treat * (1 - prob_treat)\ndelta_weighted <- sum(wt * delta_x) / sum(wt)\n\nc(regression = delta_R, weighted_avg = delta_weighted,\n  unweighted_avg = mean(delta_x))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  regression.D   weighted_avg unweighted_avg \n       -0.4937         0.0374         2.0417 \n```\n\n\n:::\n:::\n\n\nThe unweighted average treatment effect is about 2, but the regression coefficient is near 0. That's because treatment assignment $p(X) = X^2$ is most variable around $X \\approx 0.7$ (where $p(1-p)$ peaks), and at that point the treatment effect $\\delta(X) = 10 - 16(0.7) \\approx -1$ is near zero or negative. The regression upweights these observations and downweights the large positive effects at low $X$, where treatment is rare and $\\text{Var}(D|X)$ is small.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_cw <- data.frame(x = x_cw, delta = delta_x, weight = wt)\n\nggplot(df_cw, aes(x)) +\n  geom_line(aes(y = delta, color = \"Treatment effect δ(X)\"), linewidth = 1.2) +\n  geom_line(aes(y = weight * 40, color = \"Weight: Var(D|X) (rescaled)\"),\n            linewidth = 1.2) +\n  scale_y_continuous(\n    name = \"Treatment effect δ(X)\",\n    sec.axis = sec_axis(~ . / 40, name = \"Var(D|X)\")\n  ) +\n  scale_color_manual(values = c(\"Treatment effect δ(X)\" = \"tomato\",\n                                \"Weight: Var(D|X) (rescaled)\" = \"steelblue\"),\n                     name = \"\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  labs(x = \"X\", title = \"Regression weights favor where treatment is most variable\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](ch02-cef-blp_files/figure-html/causal-weights-plot-1.png){width=672}\n:::\n:::\n\n\nThis is why a randomized experiment with $p = 0.5$ for everyone gives equal weight to all subgroups: when $\\text{Var}(D|X)$ is constant, the variance-weighted average reduces to the simple average.\n\n## Summary\n\n- The **CEF** $m(X) = \\mathbb{E}[Y|X]$ is the best predictor, but the CEF error can be heteroskedastic (mean independence $\\neq$ independence).\n- The **BLP** $X'\\beta$ is the best *linear* approximation. It equals the CEF when the CEF is linear (e.g., jointly normal data).\n- BLP residuals satisfy $\\mathbb{E}[Xe] = 0$ (weaker than $\\mathbb{E}[e|X] = 0$), so nonlinear patterns can remain.\n- **Dummy variables** make the CEF discrete: the regression recovers group means.\n- **Interactions** allow slopes to vary by group or by the value of a moderator.\n- **Omitted variable bias** has a precise formula: $\\gamma_1 = \\beta_1 + \\Gamma_{12}\\beta_2$. Adding controls doesn't always reduce bias.\n- **Causal interpretation**: regression weights treatment effects by the conditional variance of treatment assignment.\n\nNext: [Multivariate OLS](ch03-ols.qmd) — moving from population quantities to sample estimation.\n",
    "supporting": [
      "ch02-cef-blp_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}