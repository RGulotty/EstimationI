{
  "hash": "c7ef95091cad6f7c5c0f3f5faefd2765",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"13. Fixed Effects and Modern DiD\"\nsubtitle: \"Random effects, Hausman test, CRE, dynamic panels, and new DiD methods\"\n---\n\nThis chapter covers the fixed vs. random effects choice, correlated random effects (Mundlak), dynamic panel GMM, and modern difference-in-differences methods that address the problems with two-way fixed effects under staggered treatment adoption. We use `plm` for panel estimation, `did` for Callaway--Sant'Anna group-time ATTs, and `fect` for counterfactual imputation estimators.\n\n**Questions this chapter answers:**\n\n1. When should you use fixed effects vs. random effects, and what does the Hausman test tell you?\n2. How does the TWFE estimator fail under staggered treatment with heterogeneous effects?\n3. What modern alternatives (Callaway-Sant'Anna, fect, matrix completion) fix the negative weighting problem?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(plm)\nlibrary(fixest)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(did)\nlibrary(fect)\n```\n:::\n\n\n## Random effects as GLS\n\nIn Chapter 12 we estimated fixed effects by demeaning within each unit. The **random effects** (RE) estimator instead treats $\\alpha_i$ as part of a composite error $\\nu_{it} = \\alpha_i + \\varepsilon_{it}$ and applies GLS to exploit the known correlation structure:\n\n$$\\text{Var}(\\nu_i) = \\sigma_\\varepsilon^2 I_T + \\sigma_\\alpha^2 \\iota\\iota' = \\Omega$$ {#eq-re-variance}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"Produc\", package = \"plm\")\npdata <- pdata.frame(Produc, index = c(\"state\", \"year\"))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fixed effects\nfe_fit <- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n              data = pdata, model = \"within\")\n\n# Random effects (GLS with error components)\nre_fit <- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n              data = pdata, model = \"random\")\n\n# Compare slope coefficients\ncoef_comp <- data.frame(\n  Variable = names(coef(fe_fit)),\n  FE = round(coef(fe_fit), 4),\n  RE = round(coef(re_fit)[-1], 4)\n)\nrownames(coef_comp) <- NULL\ncoef_comp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Variable      FE      RE\n1 log(pcap) -0.0261  0.0044\n2   log(pc)  0.2920  0.3105\n3  log(emp)  0.7682  0.7297\n4     unemp -0.0053 -0.0062\n```\n\n\n:::\n:::\n\n\nRE is more efficient than FE when the random effects assumption ($\\alpha_i \\perp X_{it}$) holds, because it uses both within-unit and between-unit variation. But if $\\alpha_i$ is correlated with the regressors, RE is inconsistent.\n\n### Variance components\n\nThe RE estimator decomposes total variance into unit-level ($\\sigma_\\alpha^2$) and idiosyncratic ($\\sigma_\\varepsilon^2$) components:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract variance components from plm\nercomp <- ercomp(re_fit)\ncat(\"sigma_alpha (unit):\", round(sqrt(ercomp$sigma2[\"id\"]), 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nsigma_alpha (unit): 0.0827 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"sigma_eps (idios):\", round(sqrt(ercomp$sigma2[\"idios\"]), 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nsigma_eps (idios): 0.0381 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"theta (shrinkage):\", round(ercomp$theta, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntheta (shrinkage): 0.8888 \n```\n\n\n:::\n:::\n\n\nThe shrinkage parameter $\\theta$ controls how much RE pulls toward the within estimator versus the between estimator. When $\\theta$ is close to 1, RE is close to FE; when $\\theta$ is close to 0, RE is close to pooled OLS.\n\n### Partial pooling: RE as a weighted average\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Between estimator\nbe_fit <- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n              data = pdata, model = \"between\")\n\ncat(\"Between: \", round(coef(be_fit)[\"log(pcap)\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBetween:  0.1794 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Within:  \", round(coef(fe_fit)[\"log(pcap)\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWithin:   -0.0261 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"RE:      \", round(coef(re_fit)[\"log(pcap)\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRE:       0.0044 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"(RE is between the within and between estimates)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(RE is between the within and between estimates)\n```\n\n\n:::\n:::\n\n\n## The Hausman test\n\nThe Hausman test compares FE (consistent under both $H_0$ and $H_1$) with RE (efficient under $H_0$ but inconsistent under $H_1$):\n\n$$H = (\\hat\\beta_{FE} - \\hat\\beta_{RE})' [\\text{Var}(\\hat\\beta_{FE}) - \\text{Var}(\\hat\\beta_{RE})]^{-1} (\\hat\\beta_{FE} - \\hat\\beta_{RE}) \\xrightarrow{d} \\chi^2_K$$ {#eq-hausman}\n\n::: {#thm-hausman}\n## Hausman Test\nThe Hausman statistic $H = (\\hat\\beta_{FE} - \\hat\\beta_{RE})'[\\text{Var}(\\hat\\beta_{FE}) - \\text{Var}(\\hat\\beta_{RE})]^{-1}(\\hat\\beta_{FE} - \\hat\\beta_{RE}) \\xrightarrow{d} \\chi^2_K$ tests whether FE and RE estimates differ systematically. Rejection means $\\alpha_i$ is correlated with $X_{it}$, favoring FE.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nphtest(fe_fit, re_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tHausman Test\n\ndata:  log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp\nchisq = 9.5254, df = 4, p-value = 0.04923\nalternative hypothesis: one model is inconsistent\n```\n\n\n:::\n:::\n\n\nA small p-value rejects the RE assumption---state fixed effects are correlated with the regressors, so FE is preferred.\n\n**Caveat:** Using the Hausman test to *choose* between FE and RE creates a pretest estimator with distorted coverage. The research design should determine the estimator; report the Hausman test as a diagnostic. The Hausman test can also be viewed as a [GMM overidentification test](ch11-gmm.qmd#sec-j-test).\n\n## Correlated random effects (Mundlak) {#sec-mundlak}\n\nMundlak (1978) proposed a middle ground: include the group means $\\bar{x}_i$ as additional regressors in the RE model. This \"soaks up\" the correlation between $\\alpha_i$ and $x_{it}$:\n\n$$\\alpha_i = \\delta_0 + \\bar{x}_i' \\delta + \\zeta_i, \\quad \\mathbb{E}[\\zeta_i \\mid x_{it}, z_i] = 0$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add group means of time-varying regressors\npdata$lpcap_bar <- Between(log(pdata$pcap))\npdata$lpc_bar   <- Between(log(pdata$pc))\npdata$lemp_bar  <- Between(log(pdata$emp))\npdata$unemp_bar <- Between(pdata$unemp)\n\n# CRE = RE + group means\ncre_fit <- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp +\n                 lpcap_bar + lpc_bar + lemp_bar + unemp_bar,\n               data = pdata, model = \"random\")\n\n# CRE slopes = FE slopes (by FWL)\ncat(\"CRE slopes: \", round(coef(cre_fit)[2:5], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCRE slopes:  -0.0261 0.292 0.7682 -0.0053 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"FE slopes:  \", round(coef(fe_fit), 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFE slopes:   -0.0261 0.292 0.7682 -0.0053 \n```\n\n\n:::\n:::\n\n\nThe CRE slopes on time-varying regressors match FE exactly. This follows from the [FWL theorem](ch04-sensitivity.qmd#thm-fwl): partialing out $\\bar{x}_i$ from $x_{it}$ gives the within-demeaned data.\n\n::: {#thm-mundlak}\n## Correlated Random Effects (Mundlak)\nAdding group means $\\bar{x}_i$ to the RE model gives CRE: $y_{it} = x_{it}'\\beta + \\bar{x}_i'\\delta + \\alpha_i + \\varepsilon_{it}$. The slope $\\hat\\beta_{CRE}$ equals $\\hat\\beta_{FE}$ by FWL. Testing $\\delta = 0$ is equivalent to the Hausman test but works with robust SEs and allows estimation of time-invariant variable effects.\n:::\n\n::: {.callout-note}\n## CRE = FE + Time-Invariant Variables\nThe Mundlak/CRE approach gives identical slope coefficients to FE while also allowing estimation of effects of time-invariant regressors (region, sex, ethnicity). It also provides a robust version of the Hausman test through an F-test on the group means.\n:::\n\n### The Mundlak test = Hausman test\n\nTesting $H_0: \\delta = 0$ (the group means have no additional explanatory power) is equivalent to the Hausman test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# F-test on the group means\nlibrary(car)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: carData\n```\n\n\n:::\n\n```{.r .cell-code}\nlinearHypothesis(cre_fit,\n  c(\"lpcap_bar = 0\", \"lpc_bar = 0\", \"lemp_bar = 0\", \"unemp_bar = 0\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\nlpcap_bar = 0\nlpc_bar = 0\nlemp_bar = 0\nunemp_bar = 0\n\nModel 1: restricted model\nModel 2: log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp + lpcap_bar + \n    lpc_bar + lemp_bar + unemp_bar\n\n  Res.Df Df  Chisq Pr(>Chisq)  \n1    811                       \n2    807  4 9.7181    0.04545 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### CRE identifies effects of time-invariant variables\n\nA key advantage of CRE over FE: it can estimate effects of time-invariant regressors. FE sweeps them out along with $\\alpha_i$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add a time-invariant variable: region\npdata$region_ne <- as.numeric(pdata$region == \"1\")\npdata$region_s  <- as.numeric(pdata$region == \"3\")\npdata$region_w  <- as.numeric(pdata$region == \"4\")\n\ncre_region <- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp +\n                    lpcap_bar + lpc_bar + lemp_bar + unemp_bar +\n                    region_ne + region_s + region_w,\n                  data = pdata, model = \"random\")\n\n# Region coefficients (relative to North Central)\ncat(\"Region effects (CRE):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRegion effects (CRE):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Northeast:\", round(coef(cre_region)[\"region_ne\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Northeast: 0.1225 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  South:    \", round(coef(cre_region)[\"region_s\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  South:     -0.0089 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  West:     \", round(coef(cre_region)[\"region_w\"], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  West:      -0.102 \n```\n\n\n:::\n:::\n\n\n## The problem with TWFE under staggered treatment {#sec-twfe-bias}\n\nStandard two-way fixed effects (TWFE) with staggered treatment can produce severely biased estimates when treatment effects are heterogeneous. TWFE implicitly compares newly-treated units to already-treated units, creating negative weights on some group-time ATTs.\n\n::: {#thm-twfe-bias}\n## TWFE Bias Under Staggered Treatment\nWith staggered adoption and heterogeneous treatment effects, TWFE assigns negative weights to some group-time ATTs — using already-treated units as controls for newly-treated units. The resulting estimate can be far from any meaningful causal parameter, including the wrong sign.\n:::\n\n::: {.callout-warning}\n## TWFE Can Produce Negative Weights\nUnder staggered treatment with dynamic or heterogeneous effects, TWFE regression assigns negative weights to some group-time treatment effects. This means the TWFE coefficient is not a convex average of individual treatment effects — it can even have the wrong sign. Use Callaway-Sant'Anna, Sun-Abraham, or fect instead.\n:::\n\n### Simulation: TWFE bias\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nN <- 200; T_max <- 10\n\n# Staggered adoption: 3 cohorts + never-treated\ncohorts <- c(4, 6, 8)  # treatment start times\nsim_panel <- data.frame()\n\nfor (i in 1:N) {\n  # Assign to cohort (25% each)\n  g <- sample(c(cohorts, 0), 1)  # 0 = never treated\n  alpha_i <- rnorm(1, sd = 2)\n\n  for (t in 1:T_max) {\n    treated <- (g > 0) & (t >= g)\n    # Treatment effect grows with exposure AND varies by cohort\n    te <- if (treated) 1.0 + 0.3 * (t - g) + 0.5 * (g == 4) else 0\n    y <- 2 + alpha_i + 0.2 * t + te + rnorm(1)\n\n    sim_panel <- rbind(sim_panel,\n      data.frame(id = i, time = t, y = y,\n                 treat = as.integer(treated),\n                 cohort = g,\n                 first_treat = ifelse(g == 0, 0, g)))\n  }\n}\n\n# TWFE regression\ntwfe <- feols(y ~ treat | id + time, data = sim_panel)\ncat(\"TWFE estimate:\", round(coef(twfe)[\"treat\"], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTWFE estimate: 1.417 \n```\n\n\n:::\n\n```{.r .cell-code}\n# True average ATT among treated observations\ntrue_att <- mean(sim_panel$y[sim_panel$treat == 1]) -\n  mean(2 + ave(rnorm(N), rep(1:N, each = T_max)) + 0.2 * sim_panel$time[sim_panel$treat == 1])\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in split.default(x, g): data length is not a multiple of split variable\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in split.default(seq_along(x), f, drop = drop, ...): data length is not\na multiple of split variable\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in 2 + ave(rnorm(N), rep(1:N, each = T_max)) + 0.2 *\nsim_panel$time[sim_panel$treat == : longer object length is not a multiple of\nshorter object length\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"(The true ATT varies by cohort and exposure time -- TWFE averages with potentially negative weights)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(The true ATT varies by cohort and exposure time -- TWFE averages with potentially negative weights)\n```\n\n\n:::\n:::\n\n\n## Callaway--Sant'Anna: group-time ATTs\n\nThe `did` package (Callaway & Sant'Anna, 2021) avoids the TWFE problem by estimating separate ATTs for each group-time pair, then aggregating. The `mpdta` dataset tracks teen employment across 500 U.S. counties, with staggered minimum wage increases.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(mpdta)\ncat(\"Counties:\", length(unique(mpdta$countyreal)),\n    \" Years:\", paste(range(mpdta$year), collapse = \"-\"),\n    \" Treatment cohorts:\", paste(sort(unique(mpdta$first.treat[mpdta$first.treat > 0])),\n                                  collapse = \", \"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCounties: 500  Years: 2003-2007  Treatment cohorts: 2004, 2006, 2007 \n```\n\n\n:::\n\n```{.r .cell-code}\nhead(mpdta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    year countyreal     lpop     lemp first.treat treat\n866 2003       8001 5.896761 8.461469        2007     1\n841 2004       8001 5.896761 8.336870        2007     1\n842 2005       8001 5.896761 8.340217        2007     1\n819 2006       8001 5.896761 8.378161        2007     1\n827 2007       8001 5.896761 8.487352        2007     1\n937 2003       8019 2.232377 4.997212        2007     1\n```\n\n\n:::\n:::\n\n\n### Estimating group-time ATTs\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate all group-time ATTs\ncs_out <- att_gt(\n  yname = \"lemp\",\n  gname = \"first.treat\",\n  idname = \"countyreal\",\n  tname = \"year\",\n  xformla = ~1,              # unconditional\n  data = mpdta,\n  control_group = \"nevertreated\",\n  est_method = \"dr\"          # doubly robust (default)\n)\n\nsummary(cs_out)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\natt_gt(yname = \"lemp\", tname = \"year\", idname = \"countyreal\", \n    gname = \"first.treat\", xformla = ~1, data = mpdta, control_group = \"nevertreated\", \n    est_method = \"dr\")\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. <https://doi.org/10.1016/j.jeconom.2020.12.001>, <https://arxiv.org/abs/1803.09015> \n\nGroup-Time Average Treatment Effects:\n Group Time ATT(g,t) Std. Error [95% Simult.  Conf. Band]  \n  2004 2004  -0.0105     0.0237       -0.0747      0.0537  \n  2004 2005  -0.0704     0.0313       -0.1553      0.0145  \n  2004 2006  -0.1373     0.0362       -0.2354     -0.0391 *\n  2004 2007  -0.1008     0.0379       -0.2036      0.0020  \n  2006 2004   0.0065     0.0235       -0.0571      0.0702  \n  2006 2005  -0.0028     0.0193       -0.0552      0.0497  \n  2006 2006  -0.0046     0.0167       -0.0499      0.0407  \n  2006 2007  -0.0412     0.0195       -0.0940      0.0116  \n  2007 2004   0.0305     0.0144       -0.0084      0.0695  \n  2007 2005  -0.0027     0.0165       -0.0474      0.0419  \n  2007 2006  -0.0311     0.0181       -0.0801      0.0179  \n  2007 2007  -0.0261     0.0161       -0.0698      0.0177  \n---\nSignif. codes: `*' confidence band does not cover 0\n\nP-value for pre-test of parallel trends assumption:  0.16812\nControl Group:  Never Treated,  Anticipation Periods:  0\nEstimation Method:  Doubly Robust\n```\n\n\n:::\n:::\n\n\nEach row is an ATT(g, t): the average treatment effect for group $g$ (defined by when they were first treated) at time $t$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggdid(cs_out, ylim = c(-0.3, 0.3))\n```\n\n::: {.cell-output-display}\n![](ch13-fixed-effects_files/figure-html/cs-attgt-plot-1.png){width=672}\n:::\n:::\n\n\n### Aggregation: event study\n\nThe many group-time ATTs can be aggregated into an event study (dynamic effects by exposure time):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncs_dyn <- aggte(cs_out, type = \"dynamic\")\nsummary(cs_dyn)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\naggte(MP = cs_out, type = \"dynamic\")\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. <https://doi.org/10.1016/j.jeconom.2020.12.001>, <https://arxiv.org/abs/1803.09015> \n\n\nOverall summary of ATT's based on event-study/dynamic aggregation:  \n     ATT    Std. Error     [ 95%  Conf. Int.]  \n -0.0772        0.0221    -0.1206     -0.0338 *\n\n\nDynamic Effects:\n Event time Estimate Std. Error [95% Simult.  Conf. Band]  \n         -3   0.0305     0.0150       -0.0069      0.0679  \n         -2  -0.0006     0.0138       -0.0350      0.0338  \n         -1  -0.0245     0.0132       -0.0574      0.0085  \n          0  -0.0199     0.0115       -0.0486      0.0088  \n          1  -0.0510     0.0170       -0.0934     -0.0085 *\n          2  -0.1373     0.0375       -0.2310     -0.0435 *\n          3  -0.1008     0.0345       -0.1871     -0.0145 *\n---\nSignif. codes: `*' confidence band does not cover 0\n\nControl Group:  Never Treated,  Anticipation Periods:  0\nEstimation Method:  Doubly Robust\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggdid(cs_dyn, ylim = c(-0.3, 0.3))\n```\n\n::: {.cell-output-display}\n![Callaway-Sant'Anna event study: pre-treatment estimates near zero support parallel trends](ch13-fixed-effects_files/figure-html/cs-event-plot-1.png){width=672}\n:::\n:::\n\n\nPre-treatment estimates near zero support the parallel trends assumption. Post-treatment estimates show the dynamic treatment effect.\n\n### Simple aggregation: overall ATT\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncs_simple <- aggte(cs_out, type = \"simple\")\nsummary(cs_simple)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\naggte(MP = cs_out, type = \"simple\")\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. <https://doi.org/10.1016/j.jeconom.2020.12.001>, <https://arxiv.org/abs/1803.09015> \n\n\n   ATT    Std. Error     [ 95%  Conf. Int.]  \n -0.04        0.0118    -0.0632     -0.0167 *\n\n\n---\nSignif. codes: `*' confidence band does not cover 0\n\nControl Group:  Never Treated,  Anticipation Periods:  0\nEstimation Method:  Doubly Robust\n```\n\n\n:::\n:::\n\n\n### Group-specific effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncs_group <- aggte(cs_out, type = \"group\")\nsummary(cs_group)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\naggte(MP = cs_out, type = \"group\")\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. <https://doi.org/10.1016/j.jeconom.2020.12.001>, <https://arxiv.org/abs/1803.09015> \n\n\nOverall summary of ATT's based on group/cohort aggregation:  \n    ATT    Std. Error     [ 95%  Conf. Int.]  \n -0.031        0.0127    -0.0559     -0.0062 *\n\n\nGroup Effects:\n Group Estimate Std. Error [95% Simult.  Conf. Band]  \n  2004  -0.0797     0.0299       -0.1447     -0.0148 *\n  2006  -0.0229     0.0164       -0.0586      0.0128  \n  2007  -0.0261     0.0167       -0.0623      0.0101  \n---\nSignif. codes: `*' confidence band does not cover 0\n\nControl Group:  Never Treated,  Anticipation Periods:  0\nEstimation Method:  Doubly Robust\n```\n\n\n:::\n:::\n\n\nDifferent cohorts may experience different effects---early adopters vs. late adopters of the minimum wage increase.\n\n### With covariates\n\nParallel trends can be conditioned on covariates. Here we control for log population:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncs_cov <- att_gt(\n  yname = \"lemp\",\n  gname = \"first.treat\",\n  idname = \"countyreal\",\n  tname = \"year\",\n  xformla = ~lpop,           # conditional on log population\n  data = mpdta,\n  control_group = \"nevertreated\"\n)\n\ncs_cov_dyn <- aggte(cs_cov, type = \"dynamic\")\nggdid(cs_cov_dyn, ylim = c(-0.3, 0.3))\n```\n\n::: {.cell-output-display}\n![](ch13-fixed-effects_files/figure-html/cs-covariates-1.png){width=672}\n:::\n:::\n\n\n## Counterfactual estimators with fect\n\nThe `fect` package (Liu, Wang & Xu, 2024) takes a different approach: it imputes counterfactual outcomes for treated observations by fitting a model on untreated observations only, then computes treatment effects as the difference between observed and imputed outcomes.\n\nThe `simdata` dataset has 200 units over 35 periods with treatment that can switch on and off:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(fect)\ncat(\"Units:\", length(unique(simdata$id)),\n    \" Periods:\", paste(range(simdata$time), collapse = \"-\"),\n    \" Treated obs:\", sum(simdata$D), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnits: 200  Periods: 1-35  Treated obs: 1503 \n```\n\n\n:::\n:::\n\n\n### Two-way FE counterfactual\n\nThe simplest `fect` estimator uses two-way fixed effects to impute counterfactuals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_fe <- fect(Y ~ D + X1 + X2, data = simdata,\n               index = c(\"id\", \"time\"),\n               method = \"fe\", force = \"two-way\",\n               se = TRUE, nboots = 200,\n               parallel = FALSE)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"ATT estimate:\", round(out_fe$est.avg[1, 1], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nATT estimate: 5.121 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"SE:          \", round(out_fe$est.avg[1, 2], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSE:           0.3 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"95% CI:      [\", round(out_fe$est.avg[1, 3], 3), \",\",\n    round(out_fe$est.avg[1, 4], 3), \"]\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% CI:      [ 4.532 , 5.709 ]\n```\n\n\n:::\n:::\n\n\n### Event study (gap) plot\n\nThe default plot shows period-by-period ATTs relative to treatment onset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(out_fe, main = \"FE counterfactual: ATT by periods since treatment\")\n```\n\n::: {.cell-output-display}\n![](ch13-fixed-effects_files/figure-html/fect-fe-plot-1.png){width=672}\n:::\n:::\n\n\n### Interactive fixed effects (IFE)\n\nWhen parallel trends may not hold, interactive fixed effects allow for unit-specific responses to common latent factors:\n\n$$Y_{it}(0) = \\alpha_i + \\xi_t + f_t' \\lambda_i + X_{it}'\\beta + \\varepsilon_{it}$$\n\nThe number of factors $r$ is selected by cross-validation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_ife <- fect(Y ~ D + X1 + X2, data = simdata,\n                index = c(\"id\", \"time\"),\n                method = \"ife\", force = \"two-way\",\n                CV = TRUE, r = c(0, 5),\n                se = TRUE, nboots = 200,\n                parallel = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCross-validating ...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCriterion: Mean Squared Prediction Error\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nInteractive fixed effects model...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nr = 0; sigma2 = 6.35460; IC = 2.21891; PC = 6.08178; MSPE = 6.89894\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nr = 1; sigma2 = 4.52698; IC = 2.24325; PC = 5.26760; MSPE = 5.05627\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nr = 2; sigma2 = 3.89603; IC = 2.45349; PC = 5.33953; MSPE = 4.62321\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n*\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nr = 3; sigma2 = 3.79056; IC = 2.78325; PC = 5.98062; MSPE = 4.96778\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nr = 4; sigma2 = 3.67967; IC = 3.10762; PC = 6.56967; MSPE = 5.65206\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nr = 5; sigma2 = 3.57625; IC = 3.43005; PC = 7.12886; MSPE = 6.07420\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n r* = 2\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"IFE ATT:\", round(out_ife$est.avg[1, 1], 3),\n    \" (selected r =\", out_ife$r.cv, \")\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIFE ATT: 3.073  (selected r = 2 )\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(out_ife, main = \"IFE counterfactual: ATT by periods since treatment\")\n```\n\n::: {.cell-output-display}\n![](ch13-fixed-effects_files/figure-html/fect-ife-plot-1.png){width=672}\n:::\n:::\n\n\n### Matrix completion\n\nMatrix completion treats the $N \\times T$ matrix of untreated potential outcomes as approximately low-rank and uses nuclear norm regularization to impute missing entries---no need to specify the number of factors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_mc <- fect(Y ~ D + X1 + X2, data = simdata,\n               index = c(\"id\", \"time\"),\n               method = \"mc\", force = \"two-way\",\n               CV = TRUE,\n               se = TRUE, nboots = 200,\n               parallel = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCross-validating ...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCriterion: Mean Squared Prediction Error\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nMatrix completion method...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nlambda.norm = 1.00000; MSPE = 7.10032; MSPTATT = 0.28561; MSE = 5.80999\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nlambda.norm = 0.42170; MSPE = 5.52641; MSPTATT = 0.12218; MSE = 4.15356\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nlambda.norm = 0.17783; MSPE = 5.25167; MSPTATT = 0.04007; MSE = 1.57548\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n*\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nlambda.norm = 0.07499; MSPE = 5.47297; MSPTATT = 0.00815; MSE = 0.28590\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nlambda.norm = 0.03162; MSPE = 5.76475; MSPTATT = 0.00170; MSE = 0.05133\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nlambda.norm = 0.01334; MSPE = 6.68770; MSPTATT = 0.00034; MSE = 0.00914\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n lambda.norm* = 0.177827941003892\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(\"MC ATT:\", round(out_mc$est.avg[1, 1], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMC ATT: 4.262 \n```\n\n\n:::\n:::\n\n\n### Comparing methods\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmethods_comp <- data.frame(\n  Method = c(\"FE\", \"IFE\", \"MC\"),\n  ATT = round(c(out_fe$est.avg[1, 1], out_ife$est.avg[1, 1], out_mc$est.avg[1, 1]), 3),\n  SE = round(c(out_fe$est.avg[1, 2], out_ife$est.avg[1, 2], out_mc$est.avg[1, 2]), 3),\n  CI_lower = round(c(out_fe$est.avg[1, 3], out_ife$est.avg[1, 3], out_mc$est.avg[1, 3]), 3),\n  CI_upper = round(c(out_fe$est.avg[1, 4], out_ife$est.avg[1, 4], out_mc$est.avg[1, 4]), 3)\n)\nmethods_comp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Method   ATT    SE CI_lower CI_upper\n1     FE 5.121 0.300    4.532    5.709\n2    IFE 3.073 0.282    2.521    3.626\n3     MC 4.262 0.327    3.622    4.902\n```\n\n\n:::\n:::\n\n\n### Placebo test for pre-trends\n\nThe placebo test checks whether there are \"treatment effects\" in the pre-treatment period---evidence against parallel trends:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_placebo <- fect(Y ~ D + X1 + X2, data = simdata,\n                    index = c(\"id\", \"time\"),\n                    method = \"fe\", force = \"two-way\",\n                    placeboTest = TRUE,\n                    placebo.period = c(-2, 0),\n                    se = TRUE, nboots = 200,\n                    parallel = FALSE)\nplot(out_placebo, main = \"Placebo test: pre-treatment effects should be zero\")\n```\n\n::: {.cell-output-display}\n![](ch13-fixed-effects_files/figure-html/fect-placebo-1.png){width=672}\n:::\n:::\n\n\n### Equivalence test\n\nThe equivalence test provides a more formal assessment: are pre-treatment ATTs close enough to zero?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(out_fe, type = \"equiv\",\n     main = \"Equivalence test for pre-treatment periods\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the fect package.\n  Please report the issue at <https://github.com/xuyiqing/fect/issues>.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](ch13-fixed-effects_files/figure-html/fect-equiv-1.png){width=672}\n:::\n:::\n\n\n## Matrix completion for panel data (conceptual)\n\nAthey, Bayati, Doudchenko, Imbens & Khosravi (2021) formalize the connection between DID, synthetic control, and matrix completion. The key insight is that all three are special cases of the same framework, differing in what structure they impose on the matrix of untreated potential outcomes $Y(0)$:\n\n| Method | Assumption on $Y(0)$ | Best when |\n|--------|---------------------|-----------|\n| DID | Additive: $Y_{it}(0) = \\alpha_i + \\xi_t$ (rank 1) | Strong parallel trends |\n| Synthetic control | Hard rank constraint on columns | Few treated units, many controls |\n| Matrix completion | Soft low-rank via nuclear norm penalty | General (robust across settings) |\n\nThe `MCPanel` package (available from GitHub: `susanathey/MCPanel`) implements nuclear norm minimized matrix completion. The idea is to find a low-rank matrix $L$ plus unit and time effects that best approximate the observed (untreated) entries:\n\n$$\\min_{L, \\gamma, \\delta} \\frac{1}{|\\mathcal{O}|} \\sum_{(i,t) \\in \\mathcal{O}} (Y_{it} - L_{it} - \\gamma_i - \\delta_t)^2 + \\lambda \\|L\\|_*$$\n\nwhere $\\|L\\|_*$ is the nuclear norm (sum of singular values) and $\\lambda$ is chosen by cross-validation. When $\\lambda$ is very large, $L \\to 0$ and the method reduces to DID. When $\\lambda$ is small, it approximates interactive fixed effects.\n\nWe can demonstrate the idea with a simple simulation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(99)\nN_mc <- 30; T_mc <- 20; R_true <- 2\n\n# Low-rank structure\nA <- matrix(rnorm(N_mc * R_true), N_mc, R_true)\nB <- matrix(rnorm(T_mc * R_true), T_mc, R_true)\nL_true <- A %*% t(B)\n\n# Unit and time effects\ngamma <- rnorm(N_mc, sd = 0.5)\ndelta <- rnorm(T_mc, sd = 0.5)\nY0 <- L_true + outer(gamma, rep(1, T_mc)) + outer(rep(1, N_mc), delta) +\n  matrix(rnorm(N_mc * T_mc, sd = 0.3), N_mc, T_mc)\n\n# Treatment: units 1-10 treated from period 15 onward\ntau_true <- 2.0  # constant treatment effect\nY_obs <- Y0\nY_obs[1:10, 15:T_mc] <- Y_obs[1:10, 15:T_mc] + tau_true\n\n# What DID would estimate\ndid_control_pre <- mean(Y_obs[11:N_mc, 1:14])\ndid_control_post <- mean(Y_obs[11:N_mc, 15:T_mc])\ndid_treat_pre <- mean(Y_obs[1:10, 1:14])\ndid_treat_post <- mean(Y_obs[1:10, 15:T_mc])\ndid_att <- (did_treat_post - did_treat_pre) - (did_control_post - did_control_pre)\n\n# SVD-based imputation (simplified matrix completion)\n# Use control rows + pre-treatment treated rows to impute\nmask <- matrix(1, N_mc, T_mc)\nmask[1:10, 15:T_mc] <- 0  # missing = treated\n\n# Impute using low-rank SVD of observed entries\nY_masked <- Y_obs * mask\nY_masked[1:10, 15:T_mc] <- NA\n\n# Simple imputation: use SVD on complete rows to predict treated post\nY_control <- Y_obs[11:N_mc, ]\nsvd_control <- svd(Y_control)\n# Project treated pre-period onto control space\nY_treat_pre <- Y_obs[1:10, 1:14]\n# Use first R_true factors\nV_pre <- svd_control$v[1:14, 1:R_true]\nV_post <- svd_control$v[15:T_mc, 1:R_true]\nU_hat <- Y_treat_pre %*% V_pre %*% solve(t(V_pre) %*% V_pre)\nY0_hat_post <- U_hat %*% t(V_post)\n\n# Adjust for level (add mean)\nmc_att <- mean(Y_obs[1:10, 15:T_mc] - Y0_hat_post) -\n  mean(Y_obs[1:10, 1:14] - U_hat %*% t(V_pre))\n\ncat(\"True ATT:     \", tau_true, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue ATT:      2 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"DID estimate: \", round(did_att, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDID estimate:  1.914 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"MC estimate:  \", round(mc_att, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMC estimate:   1.733 \n```\n\n\n:::\n:::\n\n\nWhen the data have a low-rank structure that violates simple parallel trends, matrix completion can outperform DID by exploiting the factor structure.\n\n## When to use which method\n\nThe landscape of DiD and panel methods has expanded. Here is a practical guide:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nModern DiD / Panel Method Guide:\n==================================\n\nSTANDARD PANEL (no staggered treatment):\n  - alpha_i correlated with X?\n    Yes --> Fixed Effects or CRE (Mundlak)\n    No  --> Random Effects (rare in observational data)\n  - Lagged dependent variable?\n    Yes --> Arellano-Bond or System GMM (see Ch. 12)\n\nSTAGGERED DiD (treatment adopted at different times):\n  - Homogeneous treatment effects?\n    Yes --> Standard TWFE is fine\n    No  --> Use modern methods:\n      * Callaway-Sant'Anna (did package):\n        - Group-time ATTs, flexible aggregation\n        - Supports covariates, doubly robust estimation\n        - Requires staggered adoption (no treatment reversal)\n      * fect package:\n        - FE, IFE, or matrix completion counterfactuals\n        - Handles treatment reversal\n        - Built-in placebo and equivalence tests\n        - Also wraps other methods (CS, Sun-Abraham, etc.)\n      * MCPanel:\n        - Matrix completion (nuclear norm)\n        - Unifies DID, synthetic control, and IFE\n        - Best when outcome matrix is approximately low-rank\n\nDIAGNOSTICS:\n  - Always check pre-trends (event study / placebo test)\n  - For GMM: Sargan/J-test + AR(2) test\n  - For RE: Hausman test (as diagnostic, not decision rule)\n  - For CRE: F-test on group means = robust Hausman test\n```\n\n\n:::\n:::\n\n\n## Summary\n\n- **Random effects** assumes $\\alpha_i \\perp X_{it}$ and applies GLS with the composite error structure. It is more efficient than FE when valid, but inconsistent when violated.\n- **The Hausman test** compares FE and RE; use it as a diagnostic, not a model-selection tool.\n- **Correlated random effects (Mundlak)** nests both FE and RE: it recovers FE slopes while also estimating effects of time-invariant variables. The Mundlak test ($\\delta = 0$) equals the Hausman test but works with robust SEs.\n- **TWFE fails under staggered treatment** with heterogeneous effects due to negative weighting.\n- **Callaway--Sant'Anna** (`did` package) estimates group-time ATTs and aggregates them into event studies, group-specific effects, or an overall ATT---avoiding the negative weighting problem.\n- **fect** provides counterfactual imputation estimators: FE, interactive fixed effects, and matrix completion. It handles treatment reversal and includes built-in diagnostics.\n- **Matrix completion** (Athey et al.) unifies DID and synthetic control as special cases of nuclear norm minimization, providing a flexible middle ground.\n- All panel estimators connect to the **GMM framework**: FE uses within-group moments, RE adds between-group moments, CRE augments RE to nest FE, and dynamic panels use lagged instruments.\n",
    "supporting": [
      "ch13-fixed-effects_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}