{
  "hash": "d5bb04f27c165bc4f0d9c893f0947ee2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"8. Asymptotics\"\nsubtitle: \"Large-sample theory, the delta method, and the bootstrap\"\n---\n\nChapters 5--7 used sandwich standard errors and asymptotic normality without fully justifying them. This chapter provides the justification --- and then immediately asks: can we do better? The **bootstrap** offers a simulation-based alternative that avoids distributional assumptions and derivative calculations entirely. We develop both approaches side by side so you can see when they agree, when they diverge, and which to trust.\n\nThe chapter is organized around three practical questions:\n\n1. **Why are my standard errors approximately right?** (CLT, sandwich)\n2. **How do I get SEs for nonlinear functions of parameters?** (Delta method vs. bootstrap)\n3. **How do I know my sample is large enough?** (Coverage simulations, bootstrap diagnostics)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(estimatr)\nlibrary(car)\nlibrary(carData)\nlibrary(boot)\noptions(digits = 4)\n```\n:::\n\n\n## Consistency: what more data buys you {#sec-consistency}\n\nAn estimator is **consistent** if it converges to the truth as the sample grows. Formally, $\\hat{\\theta}_n \\overset{p}{\\to} \\theta$ means $P(|\\hat{\\theta}_n - \\theta| > \\varepsilon) \\to 0$ for all $\\varepsilon > 0$.\n\nConsistency is a stronger guarantee than unbiasedness. The \"first observation\" estimator $\\hat{\\mu} = X_1$ is unbiased but useless --- its variance never shrinks. The sample mean is both unbiased and consistent.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nB <- 5000\n\ndf_list <- list()\nfor (n in c(10, 50, 200, 1000)) {\n  xbar <- replicate(B, mean(rnorm(n, mean = 24, sd = sqrt(430))))\n  x1 <- rnorm(B, mean = 24, sd = sqrt(430))\n  df_list[[length(df_list) + 1]] <- data.frame(\n    estimate = c(xbar, x1),\n    estimator = rep(c(\"Sample mean\", \"First observation\"), each = B),\n    n = n\n  )\n}\ndf_consist <- do.call(rbind, df_list)\n\nggplot(df_consist, aes(estimate, fill = estimator)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.5,\n                 position = \"identity\") +\n  geom_vline(xintercept = 24, linetype = \"dashed\") +\n  facet_wrap(~ n, scales = \"free_y\", labeller = label_both) +\n  scale_fill_manual(values = c(\"steelblue\", \"coral\")) +\n  labs(title = \"Consistent vs. merely unbiased\",\n       subtitle = \"Both are unbiased, but only the sample mean concentrates on μ\",\n       x = \"Estimate of μ = 24\", fill = NULL)\n```\n\n::: {.cell-output-display}\n![](ch08-asymptotics_files/figure-html/consistency-motivation-1.png){width=768}\n:::\n:::\n\n\n**OLS is consistent** for the projection coefficient $\\beta$ whenever $E[X_i e_i] = 0$. The proof is short: OLS is a continuous function of sample moments ($\\hat{Q}_{XX}^{-1} \\hat{Q}_{XY}$), the WLLN says sample moments converge to population moments, and the continuous mapping theorem (CMT) says continuous functions preserve convergence. When $E[Xe] \\neq 0$ --- endogeneity --- OLS is inconsistent regardless of sample size. This motivates the [IV estimator](ch10-iv.qmd#sec-iv-estimator).\n\n::: {#thm-clt-ols}\n## OLS Consistency\nIf $\\mathbb{E}[X_i e_i] = 0$ and $Q_{XX} = \\mathbb{E}[X_i X_i']$ is positive definite, then $\\hat\\beta_{OLS} \\xrightarrow{p} \\beta$. OLS is a continuous function of sample moments; the WLLN and continuous mapping theorem deliver consistency.\n:::\n\n::: {.callout-note}\n## Consistency Requires $\\mathbb{E}[Xe] = 0$\nOLS is consistent if and only if regressors are uncorrelated with errors. When $\\mathbb{E}[Xe] \\neq 0$ — from omitted variables, measurement error, or simultaneity — OLS converges to the wrong value no matter how large the sample. This is the fundamental motivation for IV estimation (Chapter 10).\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nB <- 3000\n\n# Non-normal, skewed errors: OLS is still consistent\nsim_ols <- function(n) {\n  x <- rnorm(n)\n  e <- rexp(n) - 1  # skewed, mean 0\n  y <- 2 + 3 * x + e\n  coef(lm(y ~ x))[\"x\"]\n}\n\ndf_ols <- rbind(\n  data.frame(b = replicate(B, sim_ols(20)),   n = \"n = 20\"),\n  data.frame(b = replicate(B, sim_ols(200)),  n = \"n = 200\"),\n  data.frame(b = replicate(B, sim_ols(2000)), n = \"n = 2000\")\n)\n\nggplot(df_ols, aes(b)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.5) +\n  geom_vline(xintercept = 3, color = \"blue\", linewidth = 1) +\n  facet_wrap(~ n, scales = \"free_y\") +\n  labs(title = \"OLS is consistent even with non-normal errors\",\n       subtitle = \"Errors ~ Exp(1) - 1 (skewed). True β = 3 (blue). Distribution concentrates as n grows.\",\n       x = expression(hat(beta)))\n```\n\n::: {.cell-output-display}\n![](ch08-asymptotics_files/figure-html/ols-consistency-1.png){width=768}\n:::\n:::\n\n\n## The CLT and why standard errors work\n\nThe **central limit theorem** says that standardized sample means converge to a normal distribution, regardless of the underlying data distribution:\n\n$$\\sqrt{n}(\\bar{X}_n - \\mu) \\overset{d}{\\to} N(0, \\sigma^2)$$ {#eq-clt}\n\nApplied to OLS, this gives:\n\n$$\\sqrt{n}(\\hat{\\beta} - \\beta) \\overset{d}{\\to} N\\!\\left(0,\\; Q_{XX}^{-1}\\, \\Omega\\, Q_{XX}^{-1}\\right)$$ {#eq-clt-ols}\n\n::: {#thm-clt}\n## Lindeberg-Levy CLT\nIf $X_1, \\ldots, X_n$ are iid with mean $\\mu$ and variance $\\sigma^2 < \\infty$, then $\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2)$. Applied to OLS: $\\sqrt{n}(\\hat\\beta - \\beta) \\xrightarrow{d} N(0, Q_{XX}^{-1}\\Omega Q_{XX}^{-1})$.\n:::\n\nwhere $\\Omega = E[X_i X_i' e_i^2]$. This is the [sandwich variance estimator](ch05-gls.qmd#def-sandwich), now justified by asymptotic theory. The convergence rate depends on the shape of the data --- symmetric data need $n \\approx 30$, skewed data may need $n \\geq 100$, and heavy-tailed data may need much more.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nB <- 2000\n\n# How fast does the CLT kick in? Depends on the error distribution.\nsim_tstat <- function(n, rdist_e) {\n  replicate(B, {\n    x <- rnorm(n)\n    e <- rdist_e(n)\n    y <- 2 + 3 * x + e\n    fit <- lm_robust(y ~ x, se_type = \"HC2\")\n    (coef(fit)[\"x\"] - 3) / fit$std.error[\"x\"]\n  })\n}\n\ndf_clt <- rbind(\n  data.frame(t = sim_tstat(30,  function(n) rnorm(n)),\n             n = \"n = 30\",  errors = \"Normal\"),\n  data.frame(t = sim_tstat(30,  function(n) rexp(n) - 1),\n             n = \"n = 30\",  errors = \"Exponential (skewed)\"),\n  data.frame(t = sim_tstat(30,  function(n) (rt(n, df = 3) / sqrt(3))),\n             n = \"n = 30\",  errors = \"t(3) (heavy-tailed)\"),\n  data.frame(t = sim_tstat(200, function(n) rnorm(n)),\n             n = \"n = 200\", errors = \"Normal\"),\n  data.frame(t = sim_tstat(200, function(n) rexp(n) - 1),\n             n = \"n = 200\", errors = \"Exponential (skewed)\"),\n  data.frame(t = sim_tstat(200, function(n) (rt(n, df = 3) / sqrt(3))),\n             n = \"n = 200\", errors = \"t(3) (heavy-tailed)\")\n)\n\nggplot(df_clt, aes(t)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.5) +\n  stat_function(fun = dnorm, color = \"red\", linewidth = 0.8) +\n  facet_grid(errors ~ n) +\n  coord_cartesian(xlim = c(-4, 4)) +\n  labs(title = \"How fast does the CLT kick in?\",\n       subtitle = \"Red = N(0,1). Normal errors: fast. Skewed: OK by n=200. Heavy tails: slowest.\",\n       x = \"t-statistic (HC2)\")\n```\n\n::: {.cell-output-display}\n![How fast does the CLT kick in? Depends on the error distribution.](ch08-asymptotics_files/figure-html/clt-convergence-1.png){width=768}\n:::\n:::\n\n\nWith normal errors, the CLT approximation is excellent even at $n = 30$. With skewed errors (Exponential), it improves by $n = 200$. With heavy-tailed errors ($t_3$), it remains imperfect even at $n = 200$. This is why alternatives like the bootstrap matter.\n\n## The delta method {#sec-delta-method}\n\nOften we care about a **nonlinear function** of the estimated parameters, not $\\hat{\\beta}$ itself. The delta method uses a first-order Taylor expansion:\n\n$$\\sqrt{n}(h(\\hat{\\theta}) - h(\\theta)) \\overset{d}{\\to} N(0, \\; \\nabla h' \\, V \\, \\nabla h)$$ {#eq-delta-method}\n\n::: {#thm-delta-method}\n## Delta Method\nIf $\\sqrt{n}(\\hat\\theta - \\theta) \\xrightarrow{d} N(0, V)$ and $h$ is continuously differentiable with $\\nabla h(\\theta) \\neq 0$, then $\\sqrt{n}(h(\\hat\\theta) - h(\\theta)) \\xrightarrow{d} N(0, \\nabla h' V \\nabla h)$.\n:::\n\n::: {.callout-warning}\n## Delta Method Fails at Boundaries\nThe delta method relies on a first-order Taylor expansion. It fails when $\\nabla h(\\theta) = 0$ (e.g., testing $h(\\theta) = \\theta^2$ at $\\theta = 0$) or when the function is not differentiable. In such cases, the bootstrap (or higher-order expansions) is more reliable.\n:::\n\nIn practice: compute the gradient $\\nabla h$, sandwich it around the covariance matrix, take the square root. R's `car::deltaMethod()` does this automatically.\n\n### Applied example: long-run elasticity\n\nA model with a lagged dependent variable:\n\n$$y_t = \\beta_0 + \\beta_1 x_t + \\gamma y_{t-1} + \\varepsilon_t$$\n\nThe short-run effect of $x$ is $\\beta_1$. The long-run effect (after the dynamics play out) is:\n\n$$\\theta = \\frac{\\beta_1}{1 - \\gamma}$$\n\nThis is a ratio of regression coefficients --- the delta method gives its standard error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nn <- 200\nbeta0 <- 1; beta1 <- 0.5; gamma_true <- 0.7\ny <- numeric(n)\nx <- rnorm(n)\ny[1] <- rnorm(1)\nfor (t in 2:n) {\n  y[t] <- beta0 + beta1 * x[t] + gamma_true * y[t - 1] + rnorm(1, sd = 0.5)\n}\n\ndta <- data.frame(y = y[-1], x = x[-1], y_lag = y[-n])\nfit_dyn <- lm(y ~ x + y_lag, data = dta)\n\n# Delta method (car package)\ndeltaMethod(fit_dyn, \"x / (1 - y_lag)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Estimate    SE 2.5 % 97.5 %\nx/(1 - y_lag)    1.332 0.177 0.985   1.68\n```\n\n\n:::\n:::\n\n\nThe true long-run effect is $0.5/(1 - 0.7) = 1.67$.\n\n### Delta method by hand\n\nUnder the hood, `deltaMethod()` computes the gradient vector and applies $\\text{SE} = \\sqrt{\\nabla h' \\hat{V} \\nabla h}$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb <- coef(fit_dyn)\n\n# Gradient of h(beta) = beta1 / (1 - gamma) w.r.t. (intercept, x, y_lag)\ngrad <- c(0,\n          1 / (1 - b[\"y_lag\"]),\n          b[\"x\"] / (1 - b[\"y_lag\"])^2)\n\nse_delta <- as.numeric(sqrt(t(grad) %*% vcov(fit_dyn) %*% grad))\nlr_hat <- b[\"x\"] / (1 - b[\"y_lag\"])\n\nc(estimate = lr_hat, se = se_delta,\n  ci_lo = lr_hat - 1.96 * se_delta, ci_hi = lr_hat + 1.96 * se_delta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nestimate.x         se    ci_lo.x    ci_hi.x \n    1.3318     0.1768     0.9853     1.6783 \n```\n\n\n:::\n:::\n\n\nThe delta method requires computing derivatives. For simple functions this is fine; for complicated transformations it becomes tedious. The bootstrap avoids derivatives entirely.\n\n## The bootstrap\n\nThe **bootstrap** (Efron, 1979) replaces analytical derivations with simulation. The idea: if the sample is a good stand-in for the population, then resampling from the sample mimics sampling from the population. The variation across resamples estimates sampling variability.\n\n### Nonparametric bootstrap for OLS\n\nThe **pairs bootstrap** resamples rows $(Y_i, X_i)$ with replacement, re-estimates OLS on each resample, and uses the distribution of $\\hat{\\beta}^*$ across resamples to estimate the sampling distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(Prestige)\nmod <- lm(prestige ~ income + education + women, data = Prestige)\n\n# Bootstrap by hand\nset.seed(42)\nB <- 5000\nn <- nrow(Prestige)\nboot_coefs <- matrix(NA, B, length(coef(mod)))\n\nfor (b in 1:B) {\n  idx <- sample(n, n, replace = TRUE)\n  boot_coefs[b, ] <- coef(lm(prestige ~ income + education + women,\n                               data = Prestige[idx, ]))\n}\ncolnames(boot_coefs) <- names(coef(mod))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare bootstrap SEs to classical and sandwich SEs\nse_classical <- sqrt(diag(vcov(mod)))\nse_hc2 <- sqrt(diag(vcovHC(mod, type = \"HC2\")))\nse_boot <- apply(boot_coefs, 2, sd)\n\ndata.frame(\n  Variable = names(coef(mod)),\n  Classical = round(se_classical, 4),\n  HC2 = round(se_hc2, 4),\n  Bootstrap = round(se_boot, 4)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Variable Classical    HC2 Bootstrap\n(Intercept) (Intercept)    3.2391 3.2167    3.2067\nincome           income    0.0003 0.0004    0.0004\neducation     education    0.3887 0.4469    0.4759\nwomen             women    0.0304 0.0355    0.0369\n```\n\n\n:::\n:::\n\n\nThe bootstrap SEs are close to the sandwich (HC2) SEs --- both account for heteroskedasticity without assuming a specific form. The classical SEs assume homoskedasticity, which may be wrong.\n\n### Visualizing the bootstrap distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_boot <- data.frame(\n  income = boot_coefs[, \"income\"],\n  education = boot_coefs[, \"education\"]\n)\n\nggplot(df_boot, aes(income)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.5) +\n  geom_vline(xintercept = coef(mod)[\"income\"], color = \"blue\", linewidth = 1) +\n  stat_function(fun = dnorm,\n                args = list(mean = coef(mod)[\"income\"], sd = se_hc2[\"income\"]),\n                color = \"red\", linewidth = 0.8) +\n  labs(title = \"Bootstrap distribution of income coefficient\",\n       subtitle = \"Blue = point estimate, Red = normal approximation (sandwich SE)\",\n       x = expression(hat(beta)[income]^\"*\"))\n```\n\n::: {.cell-output-display}\n![](ch08-asymptotics_files/figure-html/bootstrap-distribution-1.png){width=768}\n:::\n:::\n\n\n### Using the `boot` package\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The boot package standardizes this workflow\nboot_fn <- function(data, indices) {\n  d <- data[indices, ]\n  coef(lm(prestige ~ income + education + women, data = d))\n}\n\nboot_out <- boot(Prestige, boot_fn, R = 5000)\nboot_out\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Prestige, statistic = boot_fn, R = 5000)\n\n\nBootstrap Statistics :\n     original     bias    std. error\nt1* -6.794334  0.0571783   3.2173023\nt2*  0.001314  0.0001055   0.0004326\nt3*  4.186637 -0.0831645   0.4685039\nt4* -0.008905  0.0055652   0.0366501\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap confidence intervals for education coefficient\nboot.ci(boot_out, index = 3, type = c(\"norm\", \"perc\", \"bca\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 5000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_out, type = c(\"norm\", \"perc\", \"bca\"), \n    index = 3)\n\nIntervals : \nLevel      Normal             Percentile            BCa          \n95%   ( 3.352,  5.188 )   ( 3.043,  4.935 )   ( 3.261,  5.020 )  \nCalculations and Intervals on Original Scale\n```\n\n\n:::\n:::\n\n\nThe `boot.ci` function provides three types of intervals:\n\n- **Normal**: $\\hat{\\theta} \\pm z_{0.025} \\cdot \\text{SE}_{\\text{boot}}$ (same logic as asymptotic, just uses bootstrap SE)\n- **Percentile**: the 2.5th and 97.5th percentiles of $\\hat{\\theta}^*$ (no normal assumption)\n- **BCa** (bias-corrected, accelerated): adjusts for bias and skewness in the bootstrap distribution (generally preferred)\n\n## Delta method vs. bootstrap: a direct comparison\n\nFor the long-run elasticity $\\theta = \\beta_1/(1 - \\gamma)$, we can compare three approaches:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nB <- 5000\n\n# Bootstrap the long-run effect\nboot_lr <- replicate(B, {\n  idx <- sample(nrow(dta), nrow(dta), replace = TRUE)\n  b_star <- coef(lm(y ~ x + y_lag, data = dta[idx, ]))\n  b_star[\"x\"] / (1 - b_star[\"y_lag\"])\n})\n\n# Delta method (from above)\ndm <- deltaMethod(fit_dyn, \"x / (1 - y_lag)\")\n\ndata.frame(\n  Method = c(\"Delta method\", \"Bootstrap\"),\n  Estimate = c(dm$Estimate, mean(boot_lr)),\n  SE = c(dm$SE, sd(boot_lr)),\n  CI_lo = c(dm$`2.5 %`, quantile(boot_lr, 0.025)),\n  CI_hi = c(dm$`97.5 %`, quantile(boot_lr, 0.975))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Method Estimate     SE  CI_lo CI_hi\n     Delta method    1.332 0.1768 0.9853 1.678\n2.5%    Bootstrap    1.350 0.1881 1.0229 1.756\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_lr <- data.frame(lr = boot_lr)\nggplot(df_lr, aes(lr)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 60, alpha = 0.5) +\n  stat_function(fun = dnorm,\n                args = list(mean = dm$Estimate, sd = dm$SE),\n                color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = 0.5 / (1 - 0.7), linetype = \"dashed\") +\n  labs(title = \"Bootstrap vs. delta method for long-run elasticity\",\n       subtitle = \"Red = normal approximation (delta method). Dashed = true value 1.67.\",\n       x = expression(hat(beta)[x] / (1 - hat(gamma))))\n```\n\n::: {.cell-output-display}\n![](ch08-asymptotics_files/figure-html/delta-vs-bootstrap-plot-1.png){width=672}\n:::\n:::\n\n\nWhen the bootstrap distribution is close to normal, the two approaches agree. When it is skewed (common for ratios, especially when the denominator can be near zero), the bootstrap percentile interval is more reliable.\n\n## When does each approach work?\n\n### Coverage simulation\n\nThe acid test for any inference method: does a nominal 95% interval actually contain the true parameter 95% of the time?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nB <- 500\nn_vals <- c(20, 50, 200)\nB_boot <- 200  # bootstrap replicates per simulation\n\nresults <- list()\nfor (n in n_vals) {\n  cover_analytic <- cover_sandwich <- cover_boot <- logical(B)\n  for (b in 1:B) {\n    x <- rnorm(n)\n    sigma_i <- 0.5 + abs(x)\n    e <- rt(n, df = 3) / sqrt(3) * sigma_i  # heavy-tailed + heteroskedastic\n    y <- 2 + 3 * x + e\n    dat <- data.frame(y = y, x = x)\n    fit <- lm(y ~ x, data = dat)\n\n    # Analytic (classical)\n    ci_a <- confint(fit)[\"x\", ]\n    cover_analytic[b] <- ci_a[1] < 3 & 3 < ci_a[2]\n\n    # Sandwich (HC2)\n    fit_r <- lm_robust(y ~ x, data = dat, se_type = \"HC2\")\n    ci_s <- c(fit_r$conf.low[\"x\"], fit_r$conf.high[\"x\"])\n    cover_sandwich[b] <- ci_s[1] < 3 & 3 < ci_s[2]\n\n    # Bootstrap (percentile)\n    boot_b <- replicate(B_boot, {\n      idx <- sample(n, n, replace = TRUE)\n      coef(lm(y ~ x, data = dat[idx, ]))[\"x\"]\n    })\n    ci_boot <- quantile(boot_b, c(0.025, 0.975))\n    cover_boot[b] <- ci_boot[1] < 3 & 3 < ci_boot[2]\n  }\n  results[[length(results) + 1]] <- data.frame(\n    n = n,\n    Classical = mean(cover_analytic),\n    HC2 = mean(cover_sandwich),\n    Bootstrap = mean(cover_boot)\n  )\n}\ndo.call(rbind, results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    n Classical   HC2 Bootstrap\n1  20     0.862 0.914     0.892\n2  50     0.838 0.958     0.932\n3 200     0.776 0.928     0.906\n```\n\n\n:::\n:::\n\n\nKey patterns:\n\n- **Classical SEs** undercover at every sample size because they assume homoskedasticity\n- **HC2 sandwich SEs** maintain near-nominal coverage across all sample sizes — the analytical leverage correction works even at $n = 20$\n- **Bootstrap percentile** does well once $n$ is moderate, but at $n = 20$ its coverage is nearly as bad as the classical — with so few observations and heavy-tailed heteroskedastic errors, resampling cannot reliably capture the tail behavior\n\n### When the bootstrap breaks down\n\nThe bootstrap is not a panacea. It fails or becomes unreliable when:\n\n1. **The data are not iid** (time series, clustered data) --- you need a block bootstrap or cluster bootstrap instead\n2. **The estimator is not smooth** (e.g., the maximum of the data) --- the bootstrap distribution can be inconsistent\n3. **The sample is very small** ($n < 20$) --- there aren't enough distinct resamples to represent the population\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bootstrap fails for the sample maximum (non-smooth estimator)\nset.seed(42)\nB <- 5000\nn <- 50\ntrue_max <- 1  # support of Uniform[0,1]\n\nboot_max <- replicate(B, {\n  x <- runif(n)\n  x_star <- sample(x, n, replace = TRUE)\n  max(x_star)\n})\n\ndf_max <- data.frame(max_star = boot_max)\nggplot(df_max, aes(max_star)) +\n  geom_histogram(bins = 50, alpha = 0.5) +\n  geom_vline(xintercept = true_max, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Bootstrap failure: sample maximum of Uniform[0,1]\",\n       subtitle = \"The bootstrap cannot exceed the sample max, so it cannot cover the true max = 1.\",\n       x = expression(max(X^\"*\")))\n```\n\n::: {.cell-output-display}\n![](ch08-asymptotics_files/figure-html/bootstrap-failure-1.png){width=672}\n:::\n:::\n\n\nThe bootstrap distribution of the sample maximum is bounded by the observed maximum --- it can never reach the true parameter boundary. For smooth functions of sample means (like regression coefficients), this pathology does not arise.\n\n## Applied workflow: Prestige data\n\nLet's put everything together on a real example, comparing all approaches.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- lm(prestige ~ income + education + women, data = Prestige)\n```\n:::\n\n\n### Standard errors: three ways\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Classical (assumes homoskedasticity + normality)\nse_class <- sqrt(diag(vcov(mod)))\n\n# 2. Sandwich (asymptotic, no distributional assumptions)\nse_hc2 <- sqrt(diag(vcovHC(mod, type = \"HC2\")))\n\n# 3. Bootstrap\nset.seed(42)\nboot_out <- boot(Prestige,\n                  function(d, i) coef(lm(prestige ~ income + education + women, data = d[i, ])),\n                  R = 5000)\nse_boot <- apply(boot_out$t, 2, sd)\n\ndata.frame(\n  Variable = names(coef(mod)),\n  Estimate = round(coef(mod), 4),\n  SE_classical = round(se_class, 4),\n  SE_HC2 = round(se_hc2, 4),\n  SE_bootstrap = round(se_boot, 4)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Variable Estimate SE_classical SE_HC2 SE_bootstrap\n(Intercept) (Intercept)  -6.7943       3.2391 3.2167       3.1653\nincome           income   0.0013       0.0003 0.0004       0.0004\neducation     education   4.1866       0.3887 0.4469       0.4724\nwomen             women  -0.0089       0.0304 0.0355       0.0372\n```\n\n\n:::\n:::\n\n\n### Confidence intervals: three ways\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Focus on the education coefficient (index 3)\nci_class <- confint(mod)[\"education\", ]\nci_hc2 <- coefci(mod, vcov = vcovHC(mod, type = \"HC2\"))[\"education\", ]\nci_boot <- boot.ci(boot_out, index = 3, type = \"perc\")$percent[4:5]\n\ndata.frame(\n  Method = c(\"Classical\", \"HC2 sandwich\", \"Bootstrap percentile\"),\n  Lower = round(c(ci_class[1], ci_hc2[1], ci_boot[1]), 3),\n  Upper = round(c(ci_class[2], ci_hc2[2], ci_boot[2]), 3),\n  Width = round(c(diff(ci_class), diff(ci_hc2), diff(ci_boot)), 3)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                Method Lower Upper Width\n1            Classical 3.415 4.958 1.543\n2         HC2 sandwich 3.300 5.073 1.774\n3 Bootstrap percentile 3.025 4.881 1.856\n```\n\n\n:::\n:::\n\n\n### Nonlinear function: income-to-education ratio\n\nSuppose we want to compare the magnitude of the income and education effects. The ratio $\\theta = \\beta_{\\text{income}} / \\beta_{\\text{education}}$ tells us how many prestige points a unit of income buys relative to a year of education.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Delta method\ndm_ratio <- deltaMethod(mod, \"income / education\")\ndm_ratio\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 Estimate       SE    2.5 % 97.5 %\nincome/education 3.14e-04 8.87e-05 1.40e-04      0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Bootstrap\nboot_ratio <- boot_out$t[, 2] / boot_out$t[, 3]  # income / education\n\nc(delta_method_se = dm_ratio$SE,\n  bootstrap_se = sd(boot_ratio))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndelta_method_se    bootstrap_se \n      8.866e-05       1.747e-04 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_ratio <- data.frame(ratio = boot_ratio)\nggplot(df_ratio, aes(ratio)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.5) +\n  stat_function(fun = dnorm,\n                args = list(mean = dm_ratio$Estimate, sd = dm_ratio$SE),\n                color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = dm_ratio$Estimate, linetype = \"dashed\") +\n  labs(title = \"Bootstrap vs. delta method for β_income / β_education\",\n       subtitle = \"Red = normal approximation (delta method). Bootstrap is slightly skewed.\",\n       x = expression(hat(beta)[income] / hat(beta)[education]))\n```\n\n::: {.cell-output-display}\n![](ch08-asymptotics_files/figure-html/prestige-ratio-plot-1.png){width=672}\n:::\n:::\n\n\n### Wald test with robust and bootstrap covariance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Classical F-test\nlinearHypothesis(mod, c(\"education = 0\", \"women = 0\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\neducation = 0\nwomen = 0\n\nModel 1: restricted model\nModel 2: prestige ~ income + education + women\n\n  Res.Df   RSS Df Sum of Sq    F Pr(>F)    \n1    100 14616                             \n2     98  6034  2      8583 69.7 <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Robust Wald test (sandwich)\nlinearHypothesis(mod, c(\"education = 0\", \"women = 0\"),\n                 vcov = vcovHC(mod, type = \"HC2\"), test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\neducation = 0\nwomen = 0\n\nModel 1: restricted model\nModel 2: prestige ~ income + education + women\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df Chisq Pr(>Chisq)    \n1    100                        \n2     98  2   118     <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n## The asymptotic toolkit in one picture\n\nEvery estimator in this course follows the same pattern:\n\n| Step | Tool | What it does |\n|------|------|-------------|\n| 1. Consistency | WLLN + CMT | Sample moments → population moments; $\\hat{\\theta} \\overset{p}{\\to} \\theta$ |\n| 2. Asymptotic normality | CLT + Slutsky | $\\sqrt{n}(\\hat{\\theta} - \\theta) \\overset{d}{\\to} N(0, V)$ |\n| 3. Variance estimation | Sandwich | $\\hat{V} \\overset{p}{\\to} V$ (robust to misspecification) |\n| 4. Nonlinear functions | Delta method or bootstrap | SEs for $h(\\hat{\\theta})$ |\n\nThis applies identically to OLS, probit MLE (Chapter 7), IV (Chapter 10), and GMM (Chapter 11) --- only the moment condition changes:\n\n| Estimator | Moment condition |\n|-----------|-----------------|\n| OLS | $E[X_i(Y_i - X_i'\\beta)] = 0$ |\n| Probit MLE | $E[s_i(\\beta)] = 0$ (score) |\n| IV | $E[Z_i(Y_i - X_i'\\beta)] = 0$ |\n| GMM | $E[g(W_i, \\theta)] = 0$ (general) |\n\n## Summary\n\n| Method | How it works | Advantages | Limitations |\n|--------|-------------|------------|-------------|\n| Classical SE | Assumes $e \\sim N(0, \\sigma^2)$ | Exact under normality | Wrong under heteroskedasticity |\n| Sandwich (HC2) | Estimates $E[X_i X_i' e_i^2]$ | No distributional assumptions | Needs moderate $n$ |\n| Delta method | Taylor expansion of $h(\\hat{\\beta})$ | Fast, analytical | Requires derivatives; assumes normality of $h(\\hat{\\beta})$ |\n| Bootstrap | Resample rows, re-estimate | No derivatives; captures skewness | Needs iid; slow; fails for non-smooth estimators |\n\n| Task | R code |\n|------|--------|\n| Sandwich SEs | `vcovHC(mod, type = \"HC2\")` or `lm_robust(..., se_type = \"HC2\")` |\n| Delta method | `car::deltaMethod(mod, \"expression\")` |\n| Bootstrap SEs | `boot(data, statistic, R = 5000)` |\n| Bootstrap CIs | `boot.ci(boot_out, type = \"perc\")` or `type = \"bca\"` |\n| Robust Wald test | `linearHypothesis(mod, ..., vcov = vcovHC, test = \"Chisq\")` |\n\n**Key takeaways.**\n\n- The CLT justifies using normal-based inference for OLS, probit, IV, and GMM --- but convergence speed depends on the error distribution. Heavy tails and small samples require caution.\n- The sandwich SE and the bootstrap SE are both robust to heteroskedasticity. They usually agree for linear models with moderate $n$.\n- The delta method and the bootstrap both handle nonlinear functions. Use the delta method when you can compute derivatives easily; use the bootstrap when the function is complicated or the normal approximation is suspect.\n- When in doubt about whether $n$ is \"large enough,\" compare all three approaches. If they disagree substantially, trust the bootstrap percentile or BCa interval.\n",
    "supporting": [
      "ch08-asymptotics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}