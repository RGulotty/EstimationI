{
  "hash": "0705cd5ad92488044e37b07450b9189d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"7. Probit and MLE\"\nsubtitle: \"Binary outcomes and maximum likelihood estimation\"\n---\n\nChapter 6 developed the likelihood for the normal linear model, where MLE turned out to be OLS. This chapter applies the same MLE toolkit to a genuinely nonlinear problem: modeling binary outcomes. We start with the linear probability model (LPM), motivate the probit link, write the probit likelihood and score by hand, implement Newton--Raphson, and compare everything to R's built-in `glm()`. The running theme is that the score equation $\\sum s_i(\\beta) = 0$ is just another moment condition --- the same logic that gave us OLS, but now for a nonlinear model.\n\n**Questions this chapter answers:**\n\n1. Why does the linear probability model predict outside $[0, 1]$, and how does probit fix this?\n2. What is the probit model, and how does it arise from a latent variable representation?\n3. How do we interpret probit coefficients — and why are marginal effects needed?\n4. How does Newton-Raphson find the MLE, and why does it converge so fast?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(estimatr)\nlibrary(carData)\noptions(digits = 4)\n```\n:::\n\n\nWe use the Mroz (1987) dataset: 753 married women, with labor force participation (`lfp`) as the binary outcome.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(Mroz)\nMroz$lfp_bin <- as.integer(Mroz$lfp == \"yes\")\nmean(Mroz$lfp_bin)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5684\n```\n\n\n:::\n:::\n\n\nAbout 57% of women in the sample participate in the labor force.\n\n## The linear probability model\n\nWhen $Y \\in \\{0,1\\}$, the conditional expectation is a probability:\n\n$$E[Y \\mid X] = P(Y = 1 \\mid X)$$\n\nThe best linear predictor (Chapter 2) still exists --- OLS estimates it by solving the usual normal equations. This is the **linear probability model** (LPM):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlpm <- lm_robust(lfp_bin ~ k5 + k618 + age + wc + inc, data = Mroz, se_type = \"HC2\")\nsummary(lpm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm_robust(formula = lfp_bin ~ k5 + k618 + age + wc + inc, data = Mroz, \n    se_type = \"HC2\")\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|) CI Lower CI Upper  DF\n(Intercept)  1.28867    0.11527   11.18 6.12e-27  1.06239  1.51495 747\nk5          -0.30255    0.03297   -9.18 4.28e-19 -0.36728 -0.23783 747\nk618        -0.01670    0.01462   -1.14 2.54e-01 -0.04540  0.01200 747\nage         -0.01320    0.00243   -5.43 7.77e-08 -0.01798 -0.00843 747\nwcyes        0.22065    0.03707    5.95 4.05e-09  0.14788  0.29341 747\ninc         -0.00627    0.00153   -4.10 4.50e-05 -0.00927 -0.00327 747\n\nMultiple R-squared:  0.131 ,\tAdjusted R-squared:  0.125 \nF-statistic: 28.8 on 5 and 747 DF,  p-value: <2e-16\n```\n\n\n:::\n:::\n\n\nBinary outcomes have built-in heteroskedasticity: $\\text{Var}(Y \\mid X) = p(X)(1 - p(X))$ depends on $X$ by construction, so we use HC2 standard errors throughout.\n\n### Predictions outside $[0,1]$\n\nThe LPM is a linear function --- nothing stops it from predicting probabilities below 0 or above 1:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlpm_ols <- lm(lfp_bin ~ k5 + k618 + age + wc + inc, data = Mroz)\np_hat_lpm <- fitted(lpm_ols)\n\nc(min = min(p_hat_lpm), max = max(p_hat_lpm),\n  outside_01 = sum(p_hat_lpm < 0 | p_hat_lpm > 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       min        max outside_01 \n    -0.286      1.024     11.000 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_lpm <- data.frame(fitted = p_hat_lpm, age = Mroz$age, y = Mroz$lfp_bin)\nggplot(df_lpm, aes(age, fitted)) +\n  geom_point(alpha = 0.3) +\n  geom_hline(yintercept = c(0, 1), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"LPM fitted values vs. age\",\n       subtitle = \"Dashed lines mark 0 and 1 --- LPM can exceed these bounds\",\n       y = \"P̂(lfp = 1)\")\n```\n\n::: {.cell-output-display}\n![LPM fitted values vs. age — dashed lines mark the [0, 1] bounds](ch07-probit_files/figure-html/lpm-plot-1.png){width=672}\n:::\n:::\n\n\nThis motivates models that constrain predictions to $[0, 1]$.\n\n::: {.callout-warning}\n## LPM Predictions Outside [0, 1]\nThe linear probability model can predict probabilities below 0 or above 1 because it fits a line through binary data. While this doesn't affect the interpretation of coefficients as average marginal effects, it limits the model's use for prediction.\n:::\n\n## The probit model\n\n### Latent variable representation\n\nSuppose there is an unobserved continuous variable:\n\n$$Y_i^* = X_i'\\beta + \\varepsilon_i, \\qquad \\varepsilon_i \\sim N(0, 1)$$\n\nWe observe $Y_i = \\mathbf{1}\\{Y_i^* > 0\\}$. Then:\n\n$$P(Y_i = 1 \\mid X_i) = P(\\varepsilon_i > -X_i'\\beta) = \\Phi(X_i'\\beta)$$\n\nwhere $\\Phi$ is the standard normal CDF. This is the **probit model** --- a single-index model with link function $G = \\Phi$.\n\n::: {#def-probit}\n## Probit Model\nThe probit model specifies $P(Y_i = 1 | X_i) = \\Phi(X_i'\\beta)$, where $\\Phi$ is the standard normal CDF. It arises from a latent variable $Y_i^* = X_i'\\beta + \\varepsilon_i$ with $\\varepsilon_i \\sim N(0,1)$ and $Y_i = \\mathbf{1}\\{Y_i^* > 0\\}$.\n:::\n\n### Fitting probit with `glm()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprobit <- glm(lfp_bin ~ k5 + k618 + age + wc + inc,\n              data = Mroz, family = binomial(link = \"probit\"))\nsummary(probit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = lfp_bin ~ k5 + k618 + age + wc + inc, family = binomial(link = \"probit\"), \n    data = Mroz)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.28263    0.36748    6.21  5.2e-10 ***\nk5          -0.87865    0.11331   -7.75  8.9e-15 ***\nk618        -0.05185    0.04055   -1.28      0.2    \nage         -0.03814    0.00749   -5.09  3.5e-07 ***\nwcyes        0.63741    0.11742    5.43  5.7e-08 ***\ninc         -0.01850    0.00457   -4.05  5.1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1029.75  on 752  degrees of freedom\nResidual deviance:  923.04  on 747  degrees of freedom\nAIC: 935\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n`glm()` uses Fisher scoring (iteratively reweighted least squares), which is a variant of Newton--Raphson. Below, we implement this from scratch.\n\n### Identification and scale\n\nAn important difference from OLS: probit coefficients are identified only up to scale. If we allowed $\\text{Var}(\\varepsilon) = \\sigma^2$ instead of normalizing to 1, then $P(Y = 1 \\mid X) = \\Phi(X'\\beta/\\sigma)$ --- only $\\beta/\\sigma$ is identified. The normalization $\\sigma = 1$ pins down the scale, but it means $\\beta_j$ does **not** have the same interpretation as in OLS. We return to this in the marginal effects section.\n\n## Writing the probit likelihood by hand\n\nThe probit log-likelihood is:\n\n$$\\ell_n(\\beta) = \\sum_{i=1}^n \\left[ Y_i \\log \\Phi(X_i'\\beta) + (1 - Y_i) \\log(1 - \\Phi(X_i'\\beta)) \\right]$$ {#eq-probit-loglik}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- Mroz$lfp_bin\nX <- model.matrix(~ k5 + k618 + age + wc + inc, data = Mroz)\nn <- nrow(X)\nK <- ncol(X)\n\n# Log-likelihood\nprobit_loglik <- function(beta) {\n  Xb <- as.numeric(X %*% beta)\n  Phi <- pnorm(Xb)\n  # Clamp to avoid log(0)\n  Phi <- pmax(pmin(Phi, 1 - 1e-10), 1e-10)\n  sum(y * log(Phi) + (1 - y) * log(1 - Phi))\n}\n\n# Evaluate at glm's estimate\nprobit_loglik(coef(probit))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -461.5\n```\n\n\n:::\n\n```{.r .cell-code}\nlogLik(probit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'log Lik.' -461.5 (df=6)\n```\n\n\n:::\n:::\n\n\n### Profile likelihood\n\nWe can visualize the likelihood as a function of a single coefficient, holding the others at their MLE values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_hat <- coef(probit)\nj <- 2  # k5 coefficient\n\nb_grid <- seq(beta_hat[j] - 1, beta_hat[j] + 1, length.out = 200)\nll_profile <- numeric(length(b_grid))\nfor (i in seq_along(b_grid)) {\n  beta_try <- beta_hat\n  beta_try[j] <- b_grid[i]\n  ll_profile[i] <- probit_loglik(beta_try)\n}\n\ndf_profile <- data.frame(beta_k5 = b_grid, loglik = ll_profile)\nggplot(df_profile, aes(beta_k5, loglik)) +\n  geom_line(linewidth = 1) +\n  geom_vline(xintercept = beta_hat[j], linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Profile log-likelihood for β_k5\",\n       subtitle = paste(\"MLE =\", round(beta_hat[j], 3)),\n       x = expression(beta[k5]), y = \"Log-likelihood\")\n```\n\n::: {.cell-output-display}\n![](ch07-probit_files/figure-html/profile-loglik-1.png){width=672}\n:::\n:::\n\n\n## The score function\n\nRecall the [score and Fisher information](ch06-small-sample.qmd#def-score-information) from Chapter 6. The individual score contribution for the probit model is:\n\n$$s_i(\\beta) = \\frac{\\phi(X_i'\\beta)}{\\Phi(X_i'\\beta)(1 - \\Phi(X_i'\\beta))} (Y_i - \\Phi(X_i'\\beta)) X_i$$ {#eq-probit-score}\n\nwhere $\\phi$ is the standard normal pdf. The weight $w_i = \\phi/({\\Phi(1 - \\Phi)})$ upweights observations near $X'\\beta = 0$ (where the likelihood is most informative) and downweights observations deep in the tails.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Score function (returns a K-vector)\nprobit_score <- function(beta) {\n  Xb <- as.numeric(X %*% beta)\n  Phi <- pnorm(Xb)\n  phi <- dnorm(Xb)\n  Phi <- pmax(pmin(Phi, 1 - 1e-10), 1e-10)\n  w <- phi / (Phi * (1 - Phi))\n  resid_w <- w * (y - Phi)\n  as.numeric(t(X) %*% resid_w)\n}\n\n# At the MLE, the score should be zero\nprobit_score(coef(probit))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.798e-04 7.338e-05 6.640e-04 6.994e-03 1.101e-04 7.178e-03\n```\n\n\n:::\n:::\n\n\nAll elements are numerically zero --- confirming that `glm()` found the score root.\n\n### Comparing estimating equations\n\nSetting the score to zero defines the MLE. Compare this to the OLS normal equation:\n\n| Model | Estimating equation | Weights |\n|-------|-------------------|---------|\n| OLS | $\\sum_i X_i(Y_i - X_i'\\beta) = 0$ | Equal |\n| Probit MLE | $\\sum_i w_i(Y_i - \\Phi(X_i'\\beta)) X_i = 0$ | $w_i = \\phi/(\\Phi(1-\\Phi))$ |\n| General | $\\sum_i g_i(\\theta) = 0$ | $\\to$ GMM |\n\nBoth OLS and probit MLE are **method-of-moments estimators** --- they set sample moment conditions to zero. GMM (Chapter 13) is the general framework.\n\n::: {.callout-tip}\n## The Probit Score Is a Moment Condition\nSetting the probit score to zero, $\\sum w_i(Y_i - \\Phi(X_i'\\beta))X_i = 0$, is a method-of-moments estimating equation — just like OLS normal equations but with observation-specific weights $w_i$. This connection to GMM (Chapter 11) unifies all the estimators in this course.\n:::\n\n## Newton--Raphson from scratch\n\nThe Hessian (second derivative of the log-likelihood) is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Hessian function (returns a K x K matrix)\nprobit_hessian <- function(beta) {\n  Xb <- as.numeric(X %*% beta)\n  Phi <- pnorm(Xb)\n  phi <- dnorm(Xb)\n  Phi <- pmax(pmin(Phi, 1 - 1e-10), 1e-10)\n\n  # Weight for the Hessian\n  lambda <- phi / (Phi * (1 - Phi))\n  d <- lambda * (lambda + Xb)  # diagonal weights\n  # Negative Hessian contribution from each obs\n  w_hess <- -(phi^2) / (Phi * (1 - Phi))^2 *\n            ((y - Phi) * (-Xb * Phi * (1 - Phi) - phi * (1 - 2 * Phi)) -\n             phi^2) / (1)\n\n  # Simpler: use the expected Hessian (Fisher scoring)\n  w_fisher <- phi^2 / (Phi * (1 - Phi))\n  -t(X) %*% diag(w_fisher) %*% X\n}\n```\n:::\n\n\nNewton--Raphson iterates: $\\beta^{(t+1)} = \\beta^{(t)} - [H_n(\\beta^{(t)})]^{-1} S_n(\\beta^{(t)})$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Newton-Raphson (using Fisher scoring: expected Hessian)\nnewton_raphson_probit <- function(X, y, tol = 1e-8, max_iter = 50) {\n  n <- nrow(X)\n  K <- ncol(X)\n\n  # Initialize with OLS\n  beta <- solve(crossprod(X)) %*% crossprod(X, y)\n  beta <- as.numeric(beta)\n\n  for (iter in 1:max_iter) {\n    Xb <- as.numeric(X %*% beta)\n    Phi <- pnorm(Xb)\n    phi <- dnorm(Xb)\n    Phi <- pmax(pmin(Phi, 1 - 1e-10), 1e-10)\n\n    # Score\n    w <- phi / (Phi * (1 - Phi))\n    S <- as.numeric(t(X) %*% (w * (y - Phi)))\n\n    # Expected Hessian (Fisher scoring)\n    W <- phi^2 / (Phi * (1 - Phi))\n    H <- -t(X) %*% diag(W) %*% X\n\n    # Update\n    delta <- -solve(H) %*% S\n    beta <- beta + as.numeric(delta)\n\n    if (max(abs(delta)) < tol) {\n      return(list(coefficients = beta, iterations = iter,\n                  converged = TRUE, hessian = H))\n    }\n  }\n  list(coefficients = beta, iterations = max_iter, converged = FALSE, hessian = H)\n}\n\nfit_nr <- newton_raphson_probit(X, y)\nfit_nr$iterations\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare our Newton-Raphson to glm()\ndata.frame(\n  Variable = colnames(X),\n  NR = round(fit_nr$coefficients, 6),\n  glm = round(coef(probit), 6),\n  diff = round(fit_nr$coefficients - coef(probit), 10)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Variable       NR      glm       diff\n(Intercept) (Intercept)  2.28263  2.28263 -2.450e-06\nk5                   k5 -0.87865 -0.87865  3.073e-07\nk618               k618 -0.05185 -0.05185  5.738e-07\nage                 age -0.03814 -0.03814  1.780e-08\nwcyes             wcyes  0.63741  0.63741  3.525e-07\ninc                 inc -0.01850 -0.01850  5.800e-08\n```\n\n\n:::\n:::\n\n\nOur implementation matches `glm()` to many decimal places, converging in just a handful of iterations.\n\n### Visualizing convergence\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Track convergence\nbeta_path <- matrix(NA, 20, K)\nbeta <- as.numeric(solve(crossprod(X)) %*% crossprod(X, y))\nbeta_path[1, ] <- beta\n\nfor (iter in 2:20) {\n  Xb <- as.numeric(X %*% beta)\n  Phi <- pnorm(Xb)\n  phi <- dnorm(Xb)\n  Phi <- pmax(pmin(Phi, 1 - 1e-10), 1e-10)\n\n  w <- phi / (Phi * (1 - Phi))\n  S <- as.numeric(t(X) %*% (w * (y - Phi)))\n  W <- phi^2 / (Phi * (1 - Phi))\n  H <- -t(X) %*% diag(W) %*% X\n\n  delta <- -solve(H) %*% S\n  beta <- beta + as.numeric(delta)\n  beta_path[iter, ] <- beta\n  if (max(abs(delta)) < 1e-10) break\n}\n\n# Distance from MLE at each iteration\ndist_to_mle <- apply(beta_path, 1, function(b) {\n  if (any(is.na(b))) return(NA)\n  sqrt(sum((b - coef(probit))^2))\n})\ndist_to_mle <- dist_to_mle[!is.na(dist_to_mle)]\n\ndf_conv <- data.frame(iteration = seq_along(dist_to_mle), distance = dist_to_mle)\nggplot(df_conv, aes(iteration, distance)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  scale_y_log10() +\n  labs(title = \"Newton-Raphson convergence\",\n       subtitle = \"Distance to MLE on log scale --- quadratic convergence\",\n       y = \"||β(t) - β̂|| (log scale)\")\n```\n\n::: {.cell-output-display}\n![](ch07-probit_files/figure-html/convergence-path-1.png){width=672}\n:::\n:::\n\n\nNewton--Raphson converges quadratically: the number of correct digits roughly doubles each iteration.\n\n::: {.callout-note}\n## Newton-Raphson Exploits Concavity\nThe probit log-likelihood is globally concave, guaranteeing that Newton-Raphson (or Fisher scoring) converges to the unique maximum from any starting point. Quadratic convergence means the number of correct digits roughly doubles each iteration.\n:::\n\n## Marginal effects {#sec-marginal-effects}\n\nIn OLS, $\\partial E[Y \\mid X] / \\partial x_j = \\beta_j$ --- the coefficient **is** the marginal effect. In probit:\n\n$$\\frac{\\partial P(Y = 1 \\mid X)}{\\partial x_j} = \\phi(X'\\beta) \\cdot \\beta_j$$\n\nThe marginal effect depends on $X$ through $\\phi(X'\\beta)$. Two standard summaries:\n\n::: {#def-marginal-effects}\n## Marginal Effects\nIn the probit model, $\\partial P(Y=1|X)/\\partial x_j = \\phi(X'\\beta) \\cdot \\beta_j$. The **marginal effect at the mean** (MEM) evaluates at $\\bar{X}$; the **average marginal effect** (AME) averages over all observations. AME is comparable to the LPM coefficient.\n:::\n\n1. **Marginal effect at the mean** (MEM): evaluate at $\\bar{X}$\n2. **Average marginal effect** (AME): average the marginal effect over all observations\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXb_hat <- as.numeric(X %*% coef(probit))\n\n# Marginal effect at the mean\nphi_at_mean <- dnorm(mean(Xb_hat))\nmem <- phi_at_mean * coef(probit)\n\n# Average marginal effect\nphi_each <- dnorm(Xb_hat)\name <- colMeans(outer(phi_each, coef(probit)))\n\n# Compare to LPM coefficients\ndata.frame(\n  Variable = names(coef(probit)),\n  LPM = round(coef(lpm_ols), 4),\n  Probit = round(coef(probit), 4),\n  MEM = round(mem, 4),\n  AME = round(ame, 4)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Variable     LPM  Probit     MEM     AME\n(Intercept) (Intercept)  1.2887  2.2826  0.8946  0.7986\nk5                   k5 -0.3026 -0.8787 -0.3444 -0.3074\nk618               k618 -0.0167 -0.0519 -0.0203 -0.0181\nage                 age -0.0132 -0.0381 -0.0149 -0.0133\nwcyes             wcyes  0.2206  0.6374  0.2498  0.2230\ninc                 inc -0.0063 -0.0185 -0.0073 -0.0065\n```\n\n\n:::\n:::\n\n\nThe AME is directly comparable to the LPM coefficient: both estimate the average change in $P(Y=1)$ per unit change in $X_j$. They are generally similar, with differences reflecting the curvature of $\\Phi$.\n\n### Where marginal effects differ most\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Marginal effect of age as a function of the index\nxb_grid <- seq(-3, 3, length.out = 200)\nme_age <- dnorm(xb_grid) * coef(probit)[\"age\"]\n\ndf_me <- data.frame(index = xb_grid, me = me_age)\nggplot(df_me, aes(index, me)) +\n  geom_line(linewidth = 1) +\n  geom_hline(yintercept = coef(lpm_ols)[\"age\"], linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Marginal effect of age on P(lfp = 1)\",\n       subtitle = \"Probit ME (black) vs. LPM constant effect (red dashed)\",\n       x = \"X'β (probit index)\", y = \"∂P/∂age\")\n```\n\n::: {.cell-output-display}\n![](ch07-probit_files/figure-html/marginal-effects-plot-1.png){width=672}\n:::\n:::\n\n\nThe probit marginal effect is largest near $X'\\beta = 0$ (where the CDF is steepest) and vanishes in the tails. The LPM assumes a constant effect everywhere.\n\n## Fisher information and standard errors\n\n### Model-based standard errors\n\nUnder correct specification, the information matrix equality holds:\n\n$$\\mathcal{I}(\\beta) = E[s_i s_i'] = -E\\left[\\frac{\\partial^2 \\ell_i}{\\partial \\beta \\partial \\beta'}\\right]$$\n\nThe asymptotic variance of $\\hat{\\beta}$ is $\\mathcal{I}(\\beta)^{-1}/n$, estimated by inverting the negative Hessian:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model-based variance: inverse of negative Hessian\nV_model <- solve(-fit_nr$hessian)\nse_model <- sqrt(diag(V_model))\n\n# Compare to glm's reported SEs\ndata.frame(\n  Variable = colnames(X),\n  SE_manual = round(se_model, 5),\n  SE_glm = round(summary(probit)$coefficients[, \"Std. Error\"], 5)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Variable SE_manual  SE_glm\n(Intercept) (Intercept)   0.36748 0.36748\nk5                   k5   0.11331 0.11331\nk618               k618   0.04055 0.04055\nage                 age   0.00749 0.00749\nwcyes             wcyes   0.11742 0.11742\ninc                 inc   0.00457 0.00457\n```\n\n\n:::\n:::\n\n\n### Sandwich standard errors\n\nIf the probit model is misspecified (wrong link function), the information matrix equality fails and we need the sandwich:\n\n$$V_{\\text{sandwich}} = H^{-1} \\left(\\sum_i s_i s_i'\\right) H^{-1}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Robust SEs\ncoeftest(probit)                                       # model-based\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.28263    0.36748    6.21  5.2e-10 ***\nk5          -0.87865    0.11331   -7.75  8.9e-15 ***\nk618        -0.05185    0.04055   -1.28      0.2    \nage         -0.03814    0.00749   -5.09  3.5e-07 ***\nwcyes        0.63741    0.11742    5.43  5.7e-08 ***\ninc         -0.01850    0.00457   -4.05  5.1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\ncoeftest(probit, vcov = vcovHC(probit, type = \"HC1\"))  # sandwich\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nz test of coefficients:\n\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.28263    0.37312    6.12  9.5e-10 ***\nk5          -0.87865    0.11725   -7.49  6.7e-14 ***\nk618        -0.05185    0.04359   -1.19  0.23425    \nage         -0.03814    0.00755   -5.05  4.4e-07 ***\nwcyes        0.63741    0.11764    5.42  6.0e-08 ***\ninc         -0.01850    0.00493   -3.75  0.00018 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nse_classical <- summary(probit)$coefficients[, \"Std. Error\"]\nse_robust <- sqrt(diag(vcovHC(probit, type = \"HC1\")))\n\ndata.frame(\n  Variable = names(coef(probit)),\n  Model_based = round(se_classical, 4),\n  Sandwich = round(se_robust, 4),\n  Ratio = round(se_robust / se_classical, 3)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Variable Model_based Sandwich Ratio\n(Intercept) (Intercept)      0.3675   0.3731 1.015\nk5                   k5      0.1133   0.1172 1.035\nk618               k618      0.0406   0.0436 1.075\nage                 age      0.0075   0.0076 1.009\nwcyes             wcyes      0.1174   0.1176 1.002\ninc                 inc      0.0046   0.0049 1.079\n```\n\n\n:::\n:::\n\n\nIf model-based and sandwich SEs differ substantially, this signals possible misspecification of the link function. Here they are similar, which is reassuring.\n\n## LPM vs. probit: applied comparison\n\n### Fitted probabilities\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_probit <- predict(probit, type = \"response\")\n\ndf_compare <- data.frame(\n  lpm = p_hat_lpm,\n  probit = p_probit,\n  y = Mroz$lfp_bin\n)\n\nggplot(df_compare, aes(lpm, probit)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"LPM vs. Probit fitted probabilities\",\n       subtitle = \"Close agreement in the interior, diverge near 0 and 1\",\n       x = \"P̂ (LPM)\", y = \"P̂ (Probit)\")\n```\n\n::: {.cell-output-display}\n![](ch07-probit_files/figure-html/lpm-vs-probit-fitted-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary comparison\ndata.frame(\n  Model = c(\"LPM\", \"Probit\"),\n  LogLik = c(sum(dbinom(y, 1, pmax(pmin(p_hat_lpm, 1-1e-10), 1e-10), log = TRUE)),\n             as.numeric(logLik(probit))),\n  AIC = c(NA, AIC(probit)),\n  Predictions_outside_01 = c(sum(p_hat_lpm < 0 | p_hat_lpm > 1), 0)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Model LogLik AIC Predictions_outside_01\n1    LPM -462.6  NA                     11\n2 Probit -461.5 935                      0\n```\n\n\n:::\n:::\n\n\n### When does the choice matter?\n\nThe LPM and probit give nearly identical results when predicted probabilities are far from 0 and 1. They diverge most when:\n\n1. The outcome is rare (or nearly universal)\n2. Predictions reach the boundaries\n3. You need to interpret the functional form (e.g., for counterfactual predictions)\n\nFor estimating average marginal effects, the LPM is a robust baseline that doesn't require correct specification of the link function.\n\n## Logit: the main alternative\n\nThe logit model uses the logistic CDF $\\Lambda(z) = e^z/(1 + e^z)$ instead of $\\Phi$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit <- glm(lfp_bin ~ k5 + k618 + age + wc + inc,\n             data = Mroz, family = binomial(link = \"logit\"))\n\n# Probit and logit fitted values are nearly identical\nmax(abs(predict(probit, type = \"response\") - predict(logit, type = \"response\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01162\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Coefficients differ by approximately a factor of 1.6\ndata.frame(\n  Variable = names(coef(probit)),\n  Probit = round(coef(probit), 4),\n  Logit = round(coef(logit), 4),\n  Ratio = round(coef(logit) / coef(probit), 3)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Variable  Probit   Logit Ratio\n(Intercept) (Intercept)  2.2826  3.7999 1.665\nk5                   k5 -0.8787 -1.4631 1.665\nk618               k618 -0.0519 -0.0876 1.689\nage                 age -0.0381 -0.0637 1.669\nwcyes             wcyes  0.6374  1.0623 1.667\ninc                 inc -0.0185 -0.0308 1.663\n```\n\n\n:::\n:::\n\n\nThe ratio of logit to probit coefficients is approximately $\\pi/\\sqrt{3} \\approx 1.81$, because the logistic distribution has variance $\\pi^2/3$ while the standard normal has variance 1. The fitted probabilities are nearly indistinguishable.\n\n## Simulation: verifying asymptotic normality\n\nThe MLE theory from Chapter 6 says $\\sqrt{n}(\\hat{\\beta} - \\beta_0) \\to N(0, \\mathcal{I}^{-1})$. Let's verify with a controlled simulation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nB <- 2000\nn_sim <- 500\nbeta_true <- c(0.3, -0.5, 0.8)  # intercept, x1, x2\nK_sim <- length(beta_true)\n\nb_sim <- matrix(NA, B, K_sim)\nfor (b in 1:B) {\n  x1 <- rnorm(n_sim)\n  x2 <- rnorm(n_sim)\n  X_sim <- cbind(1, x1, x2)\n  p_sim <- pnorm(X_sim %*% beta_true)\n  y_sim <- rbinom(n_sim, 1, p_sim)\n  fit <- glm(y_sim ~ x1 + x2, family = binomial(link = \"probit\"))\n  b_sim[b, ] <- coef(fit)\n}\n\n# Focus on x1 coefficient\nj_sim <- 2\nc(true = beta_true[j_sim],\n  mean_bhat = mean(b_sim[, j_sim]),\n  sd_bhat = sd(b_sim[, j_sim]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     true mean_bhat   sd_bhat \n -0.50000  -0.50775   0.07339 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_sim <- data.frame(b_hat = b_sim[, j_sim])\nggplot(df_sim, aes(b_hat)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.5) +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(b_sim[, j_sim]),\n                            sd = sd(b_sim[, j_sim])),\n                color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = beta_true[j_sim], linetype = \"dashed\") +\n  labs(title = \"Sampling distribution of probit MLE\",\n       subtitle = paste(\"n =\", n_sim, \"— centered on true β, approximately normal\"),\n       x = expression(hat(beta)[x1]))\n```\n\n::: {.cell-output-display}\n![](ch07-probit_files/figure-html/asymptotic-sim-plot-1.png){width=672}\n:::\n:::\n\n\nThe MLE is centered on the true value (unbiased in large samples) and approximately normal, as the theory predicts.\n\n## Prediction and classification\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predicted probabilities\np_pred <- predict(probit, type = \"response\")\n\n# Classify at threshold 0.5\ny_pred <- as.integer(p_pred > 0.5)\n\n# Confusion matrix\ntable(Predicted = y_pred, Actual = Mroz$lfp_bin)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Actual\nPredicted   0   1\n        0 166  83\n        1 159 345\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Accuracy\nmean(y_pred == Mroz$lfp_bin)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6786\n```\n\n\n:::\n:::\n\n\nA simple 50% threshold gives reasonable classification. But the primary goal of probit in econometrics is estimating the effect of covariates on $P(Y=1)$, not prediction.\n\n## Connecting to GMM\n\nThe probit score equation, the OLS normal equation, and the GLS estimating equation are all special cases of:\n\n$$\\frac{1}{n} \\sum_{i=1}^n g(W_i, \\theta) = 0$$\n\n| Estimator | Moment condition $g(W_i, \\theta)$ |\n|-----------|----------------------------------|\n| OLS | $X_i(Y_i - X_i'\\beta)$ |\n| WLS | $W_i^{1/2} X_i(Y_i - X_i'\\beta)$ |\n| Probit MLE | $w_i(Y_i - \\Phi(X_i'\\beta)) X_i$ |\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# All three are sample moment conditions that equal zero at the estimate\n\n# OLS\nmax(abs(t(X) %*% (y - X %*% coef(lpm_ols))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.976e-10\n```\n\n\n:::\n\n```{.r .cell-code}\n# Probit score\nmax(abs(probit_score(coef(probit))))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.007178\n```\n\n\n:::\n:::\n\n\nWhen there are more moment conditions than parameters, the **generalized method of moments** (Chapter 13) provides the efficient way to combine them.\n\n## Summary\n\n| Concept | Formula | R code |\n|---------|---------|--------|\n| Linear probability model | $P(Y=1\\mid X) = X'\\beta$ | `lm_robust(y ~ x, se_type = \"HC2\")` |\n| Probit model | $P(Y=1\\mid X) = \\Phi(X'\\beta)$ | `glm(y ~ x, family = binomial(link = \"probit\"))` |\n| Log-likelihood | $\\sum[Y\\log\\Phi + (1-Y)\\log(1-\\Phi)]$ | `logLik(probit)` |\n| Score | $\\sum w_i(Y_i - \\Phi(X_i'\\beta))X_i = 0$ | manual (see above) |\n| Newton--Raphson | $\\beta^{(t+1)} = \\beta^{(t)} - H^{-1}S$ | `glm()` does this internally |\n| Average marginal effect | $\\frac{1}{n}\\sum \\phi(X_i'\\hat\\beta)\\hat\\beta_j$ | `mean(dnorm(Xb)) * coef(probit)` |\n| Model-based SEs | $\\sqrt{[-H_n(\\hat\\beta)]^{-1}_{jj}}$ | `summary(probit)` |\n| Sandwich SEs | $\\sqrt{[H^{-1}(\\sum s_is_i')H^{-1}]_{jj}}$ | `vcovHC(probit, type = \"HC1\")` |\n| Logit | $P(Y=1\\mid X) = \\Lambda(X'\\beta)$ | `family = binomial(link = \"logit\")` |\n\n**Key takeaway.** The probit model is the natural extension of MLE to binary outcomes. The coefficients are not marginal effects (compute AME instead), the score equation is a moment condition just like OLS normal equations, and sandwich SEs protect against misspecification of the link function. The LPM remains a robust baseline for estimating average effects.\n",
    "supporting": [
      "ch07-probit_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}