{
  "hash": "f680b8b53c3c13668a1ca58cafff2f70",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"6. Small Sample Inference\"\nsubtitle: \"Likelihood, the normal linear model, and exact tests\"\n---\n\nThe previous chapters derived the OLS estimator using only moment conditions — no distributional assumptions required. This chapter adds the assumption that errors are normal, which unlocks exact finite-sample distributions for $\\hat{\\beta}$, $s^2$, and the $t$- and $F$-statistics. We build everything computationally: write the likelihood, verify MLE = OLS, simulate the distributional results, and apply them to real data.\n\n**Questions this chapter answers:**\n\n1. How does the likelihood function connect to OLS, and why does MLE give the same $\\hat\\beta$?\n2. What exact sampling distributions do $\\hat\\beta$, $s^2$, and the $t$-statistic follow under normality?\n3. How do we conduct applied inference — $t$-tests, $F$-tests, and confidence intervals — in R?\n4. When do exact tests break down, and what alternatives exist?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(estimatr)\nlibrary(car)\noptions(digits = 3)\n```\n:::\n\n\n## Likelihood\n\nA parametric model specifies a probability density $f(x \\mid \\theta)$ for the data. The **likelihood** reverses the role of data and parameters: given observed data, how probable is each parameter value?\n\n### Example: binomial likelihood\n\nSuppose we observe the number of terms served by 5 legislators: $x = \\{1, 0, 1, 2, 0\\}$, each out of 2 possible terms. The binomial probability is $P(x_i \\mid p) = \\binom{2}{x_i} p^{x_i}(1-p)^{2-x_i}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_obs <- c(1, 0, 1, 2, 0)\nn_trials <- 2\n\n# Likelihood as a function of p\nlik <- function(p) {\n  prod(dbinom(x_obs, size = n_trials, prob = p))\n}\n\np_grid <- seq(0.01, 0.99, by = 0.01)\nlik_vals <- sapply(p_grid, lik)\n\ndf_lik <- data.frame(p = p_grid, likelihood = lik_vals)\nggplot(df_lik, aes(p, likelihood)) +\n  geom_line(linewidth = 1) +\n  geom_vline(xintercept = p_grid[which.max(lik_vals)], linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Binomial likelihood for x = {1, 0, 1, 2, 0}\",\n       subtitle = paste(\"MLE: p̂ =\", p_grid[which.max(lik_vals)]),\n       x = \"p\", y = \"L(p | x)\")\n```\n\n::: {.cell-output-display}\n![](ch06-small-sample_files/figure-html/binomial-likelihood-1.png){width=672}\n:::\n:::\n\n\nThe MLE is $\\hat{p} = \\sum x_i / (n \\cdot 2) = 4/10 = 0.4$ — the sample proportion.\n\n### The log-likelihood\n\nWe almost always work with the **log-likelihood** $\\ell(\\theta) = \\sum \\log f(x_i \\mid \\theta)$, which turns products into sums and is easier to optimize:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloglik <- function(p) sum(dbinom(x_obs, size = n_trials, prob = p, log = TRUE))\n\n# Optimize\nopt <- optimize(loglik, interval = c(0, 1), maximum = TRUE)\nopt$maximum\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4\n```\n\n\n:::\n:::\n\n\n### Score, Hessian, and Fisher information\n\nThe **score** is the derivative of the log-likelihood — it tells you the slope at any parameter value. At the MLE, the score equals zero:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Score for binomial: d/dp [sum(x*log(p) + (2-x)*log(1-p))]\n# = sum(x)/p - sum(2-x)/(1-p)\nscore <- function(p) sum(x_obs) / p - sum(n_trials - x_obs) / (1 - p)\nscore(0.4)  # zero at the MLE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Hessian (negative second derivative): observed information\nhessian <- function(p) sum(x_obs) / p^2 + sum(n_trials - x_obs) / (1 - p)^2\nobserved_info <- hessian(0.4)\n\n# Cramér-Rao bound: variance >= 1 / information\nc(observed_information = observed_info,\n  CR_bound_variance    = 1 / observed_info,\n  CR_bound_se          = 1 / sqrt(observed_info))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nobserved_information    CR_bound_variance          CR_bound_se \n              41.667                0.024                0.155 \n```\n\n\n:::\n:::\n\n\nThe Fisher information measures how sharply the likelihood peaks — more information means more precise estimation.\n\n## The normal linear model\n\nThe **classic normal regression model** adds a distributional assumption to OLS:\n\n$$y \\mid X \\sim N(X\\beta, \\, \\sigma^2 I)$$\n\nThis gives us the likelihood:\n\n$$\\mathcal{L}(\\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\!\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i'\\beta)^2\\right)$$ {#eq-normal-loglik}\n\n### Writing the log-likelihood in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(swiss)\ny <- swiss$Fertility\nX <- model.matrix(~ Education + Agriculture + Catholic + Infant.Mortality,\n                  data = swiss)\nn <- nrow(X)\nK <- ncol(X)\n\n# Log-likelihood as a function of beta and sigma^2\nnormal_loglik <- function(beta, sigma2) {\n  resid <- y - X %*% beta\n  -n/2 * log(2 * pi * sigma2) - sum(resid^2) / (2 * sigma2)\n}\n```\n:::\n\n\n### MLE = OLS\n\nMaximizing the normal log-likelihood with respect to $\\beta$ gives $\\hat{\\beta}_{MLE} = (X'X)^{-1}X'y$ — exactly OLS. For $\\sigma^2$, the MLE is $\\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\sum \\hat{e}_i^2$ (biased, unlike $s^2 = \\frac{1}{n-K}\\sum \\hat{e}_i^2$).\n\n::: {#thm-mle-ols}\n## MLE Equals OLS\nUnder the normal linear model $y|X \\sim N(X\\beta, \\sigma^2 I)$, the MLE of $\\beta$ is identical to the OLS estimator: $\\hat\\beta_{MLE} = (X'X)^{-1}X'y$. The MLE of $\\sigma^2$ is $\\hat\\sigma^2_{MLE} = \\frac{1}{n}\\sum\\hat{e}_i^2$ (biased downward).\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- lm(Fertility ~ Education + Agriculture + Catholic + Infant.Mortality,\n          data = swiss)\n\n# OLS coefficients\nbeta_ols <- coef(mod)\n\n# MLE sigma^2 (biased) vs s^2 (unbiased)\nsigma2_mle <- sum(resid(mod)^2) / n\nsigma2_s2 <- sum(resid(mod)^2) / (n - K)\n\nc(sigma2_MLE = sigma2_mle, s2 = sigma2_s2, sigma_R = sigma(mod))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nsigma2_MLE         s2    sigma_R \n     45.92      51.38       7.17 \n```\n\n\n:::\n:::\n\n\nR's `sigma()` returns $\\sqrt{s^2}$, the bias-corrected RMSE.\n\n### `logLik()` and model comparison\n\nR computes the maximized log-likelihood directly:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# R's logLik uses s^2 (REML-style), but logLik.lm uses MLE sigma^2\nlogLik(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'log Lik.' -157 (df=6)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Verify by hand\nnormal_loglik(beta_ols, sigma2_mle)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -157\n```\n\n\n:::\n:::\n\n\nThe log-likelihood is useful for comparing nested models via **AIC** and **BIC**:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(Fertility ~ Education, data = swiss)\nmod2 <- lm(Fertility ~ Education + Agriculture, data = swiss)\nmod3 <- lm(Fertility ~ Education + Agriculture + Catholic + Infant.Mortality,\n           data = swiss)\n\ndata.frame(\n  Model = c(\"Education only\", \"+ Agriculture\", \"+ Catholic + Infant.Mortality\"),\n  logLik = sapply(list(mod1, mod2, mod3), logLik),\n  AIC    = sapply(list(mod1, mod2, mod3), AIC),\n  BIC    = sapply(list(mod1, mod2, mod3), BIC),\n  K      = sapply(list(mod1, mod2, mod3), function(m) length(coef(m)))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                          Model logLik AIC BIC K\n1                Education only   -171 348 354 2\n2                 + Agriculture   -171 350 357 3\n3 + Catholic + Infant.Mortality   -157 325 336 5\n```\n\n\n:::\n:::\n\n\nAIC = $-2\\ell + 2K$ penalizes complexity; BIC = $-2\\ell + K \\log n$ penalizes it more heavily. Lower is better for both.\n\n## Scores as moment conditions\n\nThe score of the normal regression model for $\\beta$ is:\n\n$$\\frac{\\partial}{\\partial \\beta} \\ell(\\beta, \\sigma^2) = \\frac{1}{\\sigma^2} X'(y - X\\beta)$$\n\nSetting this to zero gives $X'(y - X\\hat{\\beta}) = 0$ — the OLS normal equations. This means:\n\n- **MLE** solves: score = 0\n- **OLS** solves: $X'e = 0$ (normal equations)\n- **Method of moments** solves: $\\frac{1}{n}\\sum x_i(y_i - x_i'\\hat{\\beta}) = 0$\n\nAll three are the same equation.\n\n::: {#def-score-information}\n## Score and Fisher Information\nThe **score** is $S(\\theta) = \\partial \\ell / \\partial \\theta$, the gradient of the log-likelihood. Setting $S(\\hat\\theta) = 0$ defines the MLE. The **Fisher information** $\\mathcal{I}(\\theta) = -\\mathbb{E}[\\partial^2 \\ell / \\partial\\theta\\partial\\theta']$ measures how sharply the likelihood peaks. Under correct specification, $\\text{Var}(\\hat\\theta) \\approx \\mathcal{I}(\\theta)^{-1}/n$.\n:::\n\nThe likelihood-based view adds two things: (1) the **information matrix** tells us the variance of $\\hat{\\beta}$, and (2) under correct specification, the information equals the Hessian, giving us the classical formula $\\sigma^2(X'X)^{-1}$. Under misspecification, $\\mathcal{I} \\neq \\mathcal{H}$, and we need the sandwich $\\mathcal{H}^{-1}\\mathcal{I}\\mathcal{H}^{-1}$ — exactly the robust variance from Chapter 5.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Score at the MLE = 0 = normal equations\nscore_at_mle <- t(X) %*% resid(mod)\nscore_at_mle\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                      [,1]\n(Intercept)       3.02e-14\nEducation         5.68e-14\nAgriculture       7.96e-13\nCatholic          9.09e-13\nInfant.Mortality -3.98e-13\n```\n\n\n:::\n:::\n\n\n## Exact distributions under normality {#sec-exact-distributions}\n\nThe normality assumption gives us **exact** finite-sample distributions — no asymptotics needed.\n\n::: {#thm-exact-distributions}\n## Exact Sampling Distributions\nUnder $e|X \\sim N(0, \\sigma^2 I)$: (i) $\\hat\\beta|X \\sim N(\\beta, \\sigma^2(X'X)^{-1})$; (ii) $(n-K)s^2/\\sigma^2 \\sim \\chi^2_{n-K}$; (iii) $\\hat\\beta$ and $s^2$ are independent; (iv) $t_j = (\\hat\\beta_j - \\beta_j)/\\text{SE}(\\hat\\beta_j) \\sim t_{n-K}$.\n:::\n\n::: {.callout-note}\n## Exact Means No Asymptotics Required\nThese distributions hold for any sample size $n$ — no \"large $n$\" approximation needed. The $t$-distribution automatically accounts for the extra uncertainty from estimating $\\sigma^2$, with heavier tails when $n - K$ is small.\n:::\n\n### $\\hat{\\beta}$ is normal\n\nSince $\\hat{\\beta} = \\beta + (X'X)^{-1}X'e$ is a linear function of the normal vector $e$:\n\n$$\\hat{\\beta} \\mid X \\sim N(\\beta, \\, \\sigma^2 (X'X)^{-1})$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nB <- 10000\nbeta_true <- coef(mod)\nsigma_true <- sigma(mod)\n\n# Simulate under the normal model using the actual Swiss X matrix\nb_sim <- matrix(NA, B, K)\nfor (b in 1:B) {\n  y_sim <- X %*% beta_true + rnorm(n, 0, sigma_true)\n  b_sim[b, ] <- as.numeric(solve(crossprod(X)) %*% crossprod(X, y_sim))\n}\n\n# Focus on Education coefficient (column 2)\nj <- 2\nse_theory <- sigma_true * sqrt(solve(crossprod(X))[j, j])\n\ndf_beta <- data.frame(b_hat = b_sim[, j])\nggplot(df_beta, aes(b_hat)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.5) +\n  stat_function(fun = dnorm, args = list(mean = beta_true[j], sd = se_theory),\n                color = \"red\", linewidth = 1) +\n  labs(title = \"Sampling distribution of β̂_Education\",\n       subtitle = paste(\"Simulated vs. N(β, σ²[(X'X)⁻¹]_jj), SE =\", round(se_theory, 3)),\n       x = expression(hat(beta)[Education]))\n```\n\n::: {.cell-output-display}\n![](ch06-small-sample_files/figure-html/beta-normal-sim-1.png){width=672}\n:::\n:::\n\n\nThe histogram of simulated $\\hat{\\beta}$ matches the theoretical normal density perfectly.\n\n### $(n-K)s^2/\\sigma^2 \\sim \\chi^2_{n-K}$\n\nThe residual sum of squares, scaled by $\\sigma^2$, follows a chi-squared distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscaled_s2 <- numeric(B)\nfor (b in 1:B) {\n  y_sim <- X %*% beta_true + rnorm(n, 0, sigma_true)\n  e_hat <- resid(lm(y_sim ~ X - 1))\n  scaled_s2[b] <- sum(e_hat^2) / sigma_true^2\n}\n\n# Should match chi-squared(n-K)\ndf_chi <- n - K\nc(simulated_mean = mean(scaled_s2), theory_mean = df_chi,\n  simulated_var  = var(scaled_s2),  theory_var  = 2 * df_chi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nsimulated_mean    theory_mean  simulated_var     theory_var \n          41.9           42.0           82.8           84.0 \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_s2 <- data.frame(scaled_s2 = scaled_s2)\nggplot(df_s2, aes(scaled_s2)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.5) +\n  stat_function(fun = dchisq, args = list(df = df_chi),\n                color = \"red\", linewidth = 1) +\n  labs(title = expression((n-K)*s^2/sigma^2 ~ \"follows\" ~ chi[n-K]^2),\n       subtitle = paste(\"df =\", df_chi),\n       x = expression((n-K)*s^2/sigma^2))\n```\n\n::: {.cell-output-display}\n![](ch06-small-sample_files/figure-html/chi-sq-plot-1.png){width=672}\n:::\n:::\n\n\n### Independence of $\\hat{\\beta}$ and $s^2$\n\nA crucial fact: $\\hat{\\beta}$ and $\\hat{e}$ are **independent** under normality (because their joint covariance is zero and they are jointly normal). This is what allows us to form the $t$-statistic.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Correlation between beta_hat and s^2 across simulations\ns2_sim <- numeric(B)\nfor (b in 1:B) {\n  y_sim <- X %*% beta_true + rnorm(n, 0, sigma_true)\n  fit <- lm(y_sim ~ X - 1)\n  s2_sim[b] <- sum(resid(fit)^2) / (n - K)\n}\n\ncor(b_sim[, 2], s2_sim)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.0182\n```\n\n\n:::\n:::\n\n\n### The $t$-statistic\n\nSince $\\hat{\\beta}_j$ is normal and $s^2$ is independent chi-squared, their ratio has an **exact** $t$-distribution:\n\n$$T = \\frac{\\hat{\\beta}_j - \\beta_j}{\\text{SE}(\\hat{\\beta}_j)} = \\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{s^2 [(X'X)^{-1}]_{jj}}} \\sim t_{n-K}$$ {#eq-t-stat}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute t-statistics across simulations\nt_sim <- numeric(B)\nfor (b in 1:B) {\n  y_sim <- X %*% beta_true + rnorm(n, 0, sigma_true)\n  fit <- lm(y_sim ~ X - 1)\n  # t-stat for Education coefficient\n  se_b <- sqrt(vcov(fit)[j, j])\n  t_sim[b] <- (coef(fit)[j] - beta_true[j]) / se_b\n}\n\ndf_t <- data.frame(t_stat = t_sim)\nggplot(df_t, aes(t_stat)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 60, alpha = 0.5) +\n  stat_function(fun = dt, args = list(df = n - K),\n                color = \"red\", linewidth = 1) +\n  stat_function(fun = dnorm, color = \"blue\", linewidth = 0.8, linetype = \"dashed\") +\n  labs(title = expression(\"Simulated t-statistics follow \" * t[n-K]),\n       subtitle = paste(\"Red = t(\", n-K, \"), blue dashed = N(0,1). Heavier tails with small df.\"),\n       x = \"t-statistic\")\n```\n\n::: {.cell-output-display}\n![Simulated t-statistics follow the t distribution with n-K degrees of freedom](ch06-small-sample_files/figure-html/t-distribution-sim-1.png){width=672}\n:::\n:::\n\n\nWith $n - K = 42$ degrees of freedom, the $t$-distribution is close to the normal but has heavier tails — the extra uncertainty from estimating $\\sigma^2$.\n\n## Applied inference with the Swiss data\n\n### The regression table\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Fertility ~ Education + Agriculture + Catholic + \n    Infant.Mortality, data = swiss)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.676  -6.052   0.751   3.166  16.142 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       62.1013     9.6049    6.47  8.5e-08 ***\nEducation         -0.9803     0.1481   -6.62  5.1e-08 ***\nAgriculture       -0.1546     0.0682   -2.27   0.0286 *  \nCatholic           0.1247     0.0289    4.31  9.5e-05 ***\nInfant.Mortality   1.0784     0.3819    2.82   0.0072 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.17 on 42 degrees of freedom\nMultiple R-squared:  0.699,\tAdjusted R-squared:  0.671 \nF-statistic: 24.4 on 4 and 42 DF,  p-value: 1.72e-10\n```\n\n\n:::\n:::\n\n\nEach row reports $\\hat{\\beta}_j$, $\\text{SE}(\\hat{\\beta}_j)$, the $t$-statistic $\\hat{\\beta}_j / \\text{SE}(\\hat{\\beta}_j)$ (testing $H_0: \\beta_j = 0$), and the two-sided $p$-value.\n\n### Reading the output\n\nThe Education coefficient is $-0.98$ with SE $= 0.15$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_hat <- coef(mod)[\"Education\"]\nse_hat <- sqrt(vcov(mod)[\"Education\", \"Education\"])\nt_stat <- beta_hat / se_hat\np_val <- 2 * (1 - pt(abs(t_stat), df = n - K))\n\nc(estimate = beta_hat, se = se_hat, t = t_stat, p = p_val)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nestimate.Education                 se        t.Education        p.Education \n         -9.80e-01           1.48e-01          -6.62e+00           5.14e-08 \n```\n\n\n:::\n:::\n\n\nFor each additional percentage point of post-primary education, fertility is about 1 point lower, and this is highly significant.\n\n### Confidence intervals\n\nA $95\\%$ confidence interval is $\\hat{\\beta}_j \\pm t_{n-K, 0.975} \\cdot \\text{SE}(\\hat{\\beta}_j)$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Critical value from t-distribution\nc_val <- qt(0.975, df = n - K)\nc(critical_value = c_val, normal_approx = qnorm(0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ncritical_value  normal_approx \n          2.02           1.96 \n```\n\n\n:::\n\n```{.r .cell-code}\n# confint() does this automatically\nconfint(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   2.5 % 97.5 %\n(Intercept)      42.7179 81.485\nEducation        -1.2792 -0.681\nAgriculture      -0.2922 -0.017\nCatholic          0.0664  0.183\nInfant.Mortality  0.3078  1.849\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# By hand for Education\nci_lo <- beta_hat - c_val * se_hat\nci_hi <- beta_hat + c_val * se_hat\nc(lower = ci_lo, upper = ci_hi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlower.Education upper.Education \n         -1.279          -0.681 \n```\n\n\n:::\n:::\n\n\nThe correct interpretation: if we repeated this study many times, 95% of the computed intervals would contain the true $\\beta$.\n\n### Robust confidence intervals\n\nUnder heteroskedasticity, the $t$-distribution is only approximate. Use `lm_robust()` or `coeftest()` with sandwich SEs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Robust SEs\nmod_r <- lm_robust(Fertility ~ Education + Agriculture + Catholic + Infant.Mortality,\n                    data = swiss, se_type = \"HC2\")\n\n# Compare classical vs. robust CIs\nci_classical <- confint(mod)\nci_robust <- confint(mod_r)\n\ndata.frame(\n  Variable = rownames(ci_classical),\n  Classical_lo = ci_classical[, 1],\n  Classical_hi = ci_classical[, 2],\n  Robust_lo = ci_robust[, 1],\n  Robust_hi = ci_robust[, 2]\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                         Variable Classical_lo Classical_hi Robust_lo Robust_hi\n(Intercept)           (Intercept)      42.7179       81.485   44.6132   79.5894\nEducation               Education      -1.2792       -0.681   -1.2837   -0.6769\nAgriculture           Agriculture      -0.2922       -0.017   -0.2888   -0.0204\nCatholic                 Catholic       0.0664        0.183    0.0681    0.1813\nInfant.Mortality Infant.Mortality       0.3078        1.849    0.2865    1.8704\n```\n\n\n:::\n:::\n\n\n### Visualizing confidence intervals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nci_df <- data.frame(\n  term = rep(names(coef(mod))[-1], 2),\n  estimate = rep(coef(mod)[-1], 2),\n  lo = c(ci_classical[-1, 1], ci_robust[-1, 1]),\n  hi = c(ci_classical[-1, 2], ci_robust[-1, 2]),\n  type = rep(c(\"Classical\", \"HC2 Robust\"), each = K - 1)\n)\n\nggplot(ci_df, aes(x = estimate, y = term, color = type)) +\n  geom_point(position = position_dodge(0.3)) +\n  geom_errorbarh(aes(xmin = lo, xmax = hi),\n                 height = 0.2, position = position_dodge(0.3)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Swiss fertility: classical vs. robust confidence intervals\",\n       x = \"Coefficient estimate\", y = NULL, color = NULL)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: `geom_errorbarh()` was deprecated in ggplot2 4.0.0.\nℹ Please use the `orientation` argument of `geom_errorbar()` instead.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`height` was translated to `width`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](ch06-small-sample_files/figure-html/coefplot-1.png){width=672}\n:::\n:::\n\n\n## The $F$-test\n\nThe $t$-test handles one coefficient at a time. The $F$-test tests **joint** restrictions — for instance, \"do Agriculture and Catholic together predict fertility, controlling for the other variables?\"\n\n### $F$-test as a likelihood ratio\n\nThe $F$-statistic compares the restricted (null) and unrestricted models:\n\n$$F = \\frac{(\\tilde{\\sigma}^2 - \\hat{\\sigma}^2) / q}{\\hat{\\sigma}^2 / (n - K)} \\sim F_{q, \\, n-K}$$ {#eq-f-stat}\n\nwhere $\\tilde{\\sigma}^2$ is the residual variance from the restricted model and $q$ is the number of restrictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Unrestricted model\nmod_full <- lm(Fertility ~ Education + Agriculture + Catholic + Infant.Mortality,\n               data = swiss)\n\n# Restricted model: drop Agriculture and Catholic\nmod_restricted <- lm(Fertility ~ Education + Infant.Mortality, data = swiss)\n\n# F-statistic by hand\nRSS_full <- sum(resid(mod_full)^2)\nRSS_restr <- sum(resid(mod_restricted)^2)\nq <- 2  # number of restrictions\ndf_full <- n - K\n\nF_stat <- ((RSS_restr - RSS_full) / q) / (RSS_full / df_full)\np_F <- 1 - pf(F_stat, q, df_full)\n\nc(F_statistic = F_stat, df1 = q, df2 = df_full, p_value = p_F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nF_statistic         df1         df2     p_value \n   9.40e+00    2.00e+00    4.20e+01    4.23e-04 \n```\n\n\n:::\n:::\n\n\n### Using `anova()` and `linearHypothesis()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# anova() compares nested models\nanova(mod_restricted, mod_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: Fertility ~ Education + Infant.Mortality\nModel 2: Fertility ~ Education + Agriculture + Catholic + Infant.Mortality\n  Res.Df  RSS Df Sum of Sq   F  Pr(>F)    \n1     44 3124                             \n2     42 2158  2       966 9.4 0.00042 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# linearHypothesis() tests restrictions directly\nlinearHypothesis(mod_full, c(\"Agriculture = 0\", \"Catholic = 0\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\nAgriculture = 0\nCatholic = 0\n\nModel 1: restricted model\nModel 2: Fertility ~ Education + Agriculture + Catholic + Infant.Mortality\n\n  Res.Df  RSS Df Sum of Sq   F  Pr(>F)    \n1     44 3124                             \n2     42 2158  2       966 9.4 0.00042 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### The overall $F$-test\n\nThe $F$-statistic at the bottom of `summary(lm)` tests whether *all* slope coefficients are jointly zero:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Overall F-test: H0: all slopes = 0\nmod_null <- lm(Fertility ~ 1, data = swiss)\nanova(mod_null, mod_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: Fertility ~ 1\nModel 2: Fertility ~ Education + Agriculture + Catholic + Infant.Mortality\n  Res.Df  RSS Df Sum of Sq    F  Pr(>F)    \n1     46 7178                              \n2     42 2158  4      5020 24.4 1.7e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# When q = 1, F = t^2\nt_education <- summary(mod_full)$coefficients[\"Education\", \"t value\"]\nF_education <- linearHypothesis(mod_full, \"Education = 0\")$F[2]\n\nc(t_squared = t_education^2, F_stat = F_education)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nt_squared    F_stat \n     43.8      43.8 \n```\n\n\n:::\n:::\n\n\n### Robust $F$-tests\n\nUnder heteroskedasticity, use `linearHypothesis()` with a robust covariance matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Classical F-test\nlinearHypothesis(mod_full, c(\"Agriculture = 0\", \"Catholic = 0\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\nAgriculture = 0\nCatholic = 0\n\nModel 1: restricted model\nModel 2: Fertility ~ Education + Agriculture + Catholic + Infant.Mortality\n\n  Res.Df  RSS Df Sum of Sq   F  Pr(>F)    \n1     44 3124                             \n2     42 2158  2       966 9.4 0.00042 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Robust F-test (HC2)\nlinearHypothesis(mod_full, c(\"Agriculture = 0\", \"Catholic = 0\"),\n                 vcov = vcovHC(mod_full, type = \"HC2\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\nAgriculture = 0\nCatholic = 0\n\nModel 1: restricted model\nModel 2: Fertility ~ Education + Agriculture + Catholic + Infant.Mortality\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df    F  Pr(>F)    \n1     44                    \n2     42  2 10.3 0.00023 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n## Prediction intervals\n\nA **confidence interval** for $E[y \\mid x_0]$ captures uncertainty about the regression line. A **prediction interval** for a new observation $y_0$ also includes the error variance $\\sigma^2$:\n\n$$\\hat{y}_0 \\pm t_{n-K, 1-\\alpha/2} \\cdot \\sqrt{s^2 (1 + x_0'(X'X)^{-1}x_0)}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict fertility for a district with median characteristics\nnew_data <- data.frame(\n  Education  = median(swiss$Education),\n  Agriculture = median(swiss$Agriculture),\n  Catholic   = median(swiss$Catholic),\n  Infant.Mortality = median(swiss$Infant.Mortality)\n)\n\n# Confidence interval for E[y|x]\npredict(mod_full, newdata = new_data, interval = \"confidence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   fit  lwr  upr\n1 69.4 66.6 72.1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Prediction interval for a new y\npredict(mod_full, newdata = new_data, interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   fit  lwr  upr\n1 69.4 54.6 84.1\n```\n\n\n:::\n:::\n\n\nThe prediction interval is much wider — it accounts for the irreducible noise $\\sigma^2$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple model for visualization\nmod_simple <- lm(Fertility ~ Education, data = swiss)\npred_grid <- data.frame(Education = seq(1, 55, length.out = 100))\npred_ci <- predict(mod_simple, newdata = pred_grid, interval = \"confidence\")\npred_pi <- predict(mod_simple, newdata = pred_grid, interval = \"prediction\")\n\ndf_pred <- data.frame(\n  Education = pred_grid$Education,\n  fit = pred_ci[, 1],\n  ci_lo = pred_ci[, 2], ci_hi = pred_ci[, 3],\n  pi_lo = pred_pi[, 2], pi_hi = pred_pi[, 3]\n)\n\nggplot() +\n  geom_ribbon(data = df_pred, aes(Education, ymin = pi_lo, ymax = pi_hi),\n              alpha = 0.15, fill = \"steelblue\") +\n  geom_ribbon(data = df_pred, aes(Education, ymin = ci_lo, ymax = ci_hi),\n              alpha = 0.3, fill = \"steelblue\") +\n  geom_line(data = df_pred, aes(Education, fit), color = \"steelblue\", linewidth = 1) +\n  geom_point(data = swiss, aes(Education, Fertility), alpha = 0.6) +\n  labs(title = \"Swiss fertility: confidence and prediction intervals\",\n       subtitle = \"Dark band = CI for E[y|x], light band = PI for new observation\",\n       y = \"Fertility\")\n```\n\n::: {.cell-output-display}\n![](ch06-small-sample_files/figure-html/prediction-plot-1.png){width=672}\n:::\n:::\n\n\n## When do exact tests break down?\n\nThe $t$- and $F$-distributions are **exact** only under:\n\n1. Normality of errors\n2. Homoskedasticity\n3. Known, correct model specification\n\nWhat happens when normality fails? Let's simulate with heavy-tailed errors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nB <- 5000\ncover_normal <- cover_t5 <- logical(B)\nbeta_ed <- coef(mod)[\"Education\"]\n\nfor (b in 1:B) {\n  # Normal errors\n  e_norm <- rnorm(n, 0, sigma(mod))\n  y_norm <- X %*% coef(mod) + e_norm\n  fit_norm <- lm(y_norm ~ X - 1)\n  ci_norm <- confint(fit_norm)[j, ]\n  cover_normal[b] <- ci_norm[1] < beta_ed & beta_ed < ci_norm[2]\n\n  # t(5) errors (heavy tails, same variance)\n  e_t5 <- rt(n, df = 5) * sigma(mod) / sqrt(5/3)\n  y_t5 <- X %*% coef(mod) + e_t5\n  fit_t5 <- lm(y_t5 ~ X - 1)\n  ci_t5 <- confint(fit_t5)[j, ]\n  cover_t5[b] <- ci_t5[1] < beta_ed & beta_ed < ci_t5[2]\n}\n\nc(normal_errors = mean(cover_normal),\n  t5_errors     = mean(cover_t5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnormal_errors     t5_errors \n        0.949         0.950 \n```\n\n\n:::\n:::\n\n\nWith normal errors, coverage is close to 95%. With $t_5$ errors (heavier tails), coverage degrades — the exact $t$-distribution no longer applies. In practice, this is where robust SEs and asymptotic approximations (Chapters 9–11) take over.\n\n::: {.callout-warning}\n## Heavy Tails Break Exact Tests\nThe $t$- and $F$-distributions are exact only under normality. With heavy-tailed errors (e.g., $t_5$), coverage of confidence intervals degrades noticeably. In practice, use robust SEs and asymptotic theory (Chapters 8-9) when normality is suspect. The [CLT](ch08-asymptotics.qmd#thm-clt) will justify this asymptotically in Chapter 8.\n:::\n\n## Summary\n\n| Concept | Formula | R code |\n|---------|---------|--------|\n| Log-likelihood | $\\ell = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum e_i^2$ | `logLik(mod)` |\n| MLE of $\\beta$ | $(X'X)^{-1}X'y$ (= OLS) | `coef(lm(...))` |\n| MLE of $\\sigma^2$ | $\\frac{1}{n}\\sum \\hat{e}_i^2$ (biased) | `sum(resid(mod)^2) / n` |\n| AIC / BIC | $-2\\ell + 2K$ / $-2\\ell + K\\log n$ | `AIC(mod)`, `BIC(mod)` |\n| $t$-statistic | $\\hat{\\beta}_j / \\text{SE}(\\hat{\\beta}_j) \\sim t_{n-K}$ | `summary(mod)$coefficients` |\n| Confidence interval | $\\hat{\\beta}_j \\pm t_{n-K,\\,0.975} \\cdot \\text{SE}$ | `confint(mod)` |\n| $F$-test | $\\frac{(RSS_R - RSS_U)/q}{RSS_U/(n-K)} \\sim F_{q,n-K}$ | `anova(mod_r, mod_u)` |\n| Joint hypothesis | $R\\beta = r$ | `linearHypothesis(mod, ...)` |\n| Robust $F$-test | — | `linearHypothesis(mod, ..., vcov = vcovHC)` |\n| Prediction interval | $\\hat{y}_0 \\pm t \\cdot s\\sqrt{1 + x_0'(X'X)^{-1}x_0}$ | `predict(mod, interval = \"prediction\")` |\n\n**Key takeaway.** The normal linear model gives us exact finite-sample distributions: $\\hat{\\beta}$ is normal, $s^2$ is chi-squared, their ratio gives $t$, and nested model comparisons give $F$. These are the foundation of every regression table. But they rely on normality and homoskedasticity — when those fail, use robust SEs and asymptotic theory instead.\n",
    "supporting": [
      "ch06-small-sample_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}