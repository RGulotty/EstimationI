[
  {
    "objectID": "ch08-asymptotics.html",
    "href": "ch08-asymptotics.html",
    "title": "8. Asymptotics",
    "section": "",
    "text": "Chapters 5–7 used sandwich standard errors and asymptotic normality without fully justifying them. This chapter provides the justification — and then immediately asks: can we do better? The bootstrap offers a simulation-based alternative that avoids distributional assumptions and derivative calculations entirely. We develop both approaches side by side so you can see when they agree, when they diverge, and which to trust.\nThe chapter is organized around three practical questions:\nlibrary(ggplot2)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(estimatr)\nlibrary(car)\nlibrary(carData)\nlibrary(boot)\noptions(digits = 4)"
  },
  {
    "objectID": "ch08-asymptotics.html#sec-consistency",
    "href": "ch08-asymptotics.html#sec-consistency",
    "title": "8. Asymptotics",
    "section": "1 Consistency: what more data buys you",
    "text": "1 Consistency: what more data buys you\nAn estimator is consistent if it converges to the truth as the sample grows. Formally, \\(\\hat{\\theta}_n \\overset{p}{\\to} \\theta\\) means \\(P(|\\hat{\\theta}_n - \\theta| &gt; \\varepsilon) \\to 0\\) for all \\(\\varepsilon &gt; 0\\).\nConsistency is a stronger guarantee than unbiasedness. The “first observation” estimator \\(\\hat{\\mu} = X_1\\) is unbiased but useless — its variance never shrinks. The sample mean is both unbiased and consistent.\n\nset.seed(42)\nB &lt;- 5000\n\ndf_list &lt;- list()\nfor (n in c(10, 50, 200, 1000)) {\n  xbar &lt;- replicate(B, mean(rnorm(n, mean = 24, sd = sqrt(430))))\n  x1 &lt;- rnorm(B, mean = 24, sd = sqrt(430))\n  df_list[[length(df_list) + 1]] &lt;- data.frame(\n    estimate = c(xbar, x1),\n    estimator = rep(c(\"Sample mean\", \"First observation\"), each = B),\n    n = n\n  )\n}\ndf_consist &lt;- do.call(rbind, df_list)\n\nggplot(df_consist, aes(estimate, fill = estimator)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.5,\n                 position = \"identity\") +\n  geom_vline(xintercept = 24, linetype = \"dashed\") +\n  facet_wrap(~ n, scales = \"free_y\", labeller = label_both) +\n  scale_fill_manual(values = c(\"steelblue\", \"coral\")) +\n  labs(title = \"Consistent vs. merely unbiased\",\n       subtitle = \"Both are unbiased, but only the sample mean concentrates on μ\",\n       x = \"Estimate of μ = 24\", fill = NULL)\n\n\n\n\n\n\n\n\nOLS is consistent for the projection coefficient \\(\\beta\\) whenever \\(E[X_i e_i] = 0\\). The proof is short: OLS is a continuous function of sample moments (\\(\\hat{Q}_{XX}^{-1} \\hat{Q}_{XY}\\)), the WLLN says sample moments converge to population moments, and the continuous mapping theorem (CMT) says continuous functions preserve convergence. When \\(E[Xe] \\neq 0\\) — endogeneity — OLS is inconsistent regardless of sample size. This motivates the IV estimator.\n\nTheorem 1 (OLS Consistency) If \\(\\mathbb{E}[X_i e_i] = 0\\) and \\(Q_{XX} = \\mathbb{E}[X_i X_i']\\) is positive definite, then \\(\\hat\\beta_{OLS} \\xrightarrow{p} \\beta\\). OLS is a continuous function of sample moments; the WLLN and continuous mapping theorem deliver consistency.\n\n\n\n\n\n\n\nNoteConsistency Requires \\(\\mathbb{E}[Xe] = 0\\)\n\n\n\nOLS is consistent if and only if regressors are uncorrelated with errors. When \\(\\mathbb{E}[Xe] \\neq 0\\) — from omitted variables, measurement error, or simultaneity — OLS converges to the wrong value no matter how large the sample. This is the fundamental motivation for IV estimation (Chapter 10).\n\n\n\nset.seed(42)\nB &lt;- 3000\n\n# Non-normal, skewed errors: OLS is still consistent\nsim_ols &lt;- function(n) {\n  x &lt;- rnorm(n)\n  e &lt;- rexp(n) - 1  # skewed, mean 0\n  y &lt;- 2 + 3 * x + e\n  coef(lm(y ~ x))[\"x\"]\n}\n\ndf_ols &lt;- rbind(\n  data.frame(b = replicate(B, sim_ols(20)),   n = \"n = 20\"),\n  data.frame(b = replicate(B, sim_ols(200)),  n = \"n = 200\"),\n  data.frame(b = replicate(B, sim_ols(2000)), n = \"n = 2000\")\n)\n\nggplot(df_ols, aes(b)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.5) +\n  geom_vline(xintercept = 3, color = \"blue\", linewidth = 1) +\n  facet_wrap(~ n, scales = \"free_y\") +\n  labs(title = \"OLS is consistent even with non-normal errors\",\n       subtitle = \"Errors ~ Exp(1) - 1 (skewed). True β = 3 (blue). Distribution concentrates as n grows.\",\n       x = expression(hat(beta)))"
  },
  {
    "objectID": "ch08-asymptotics.html#the-clt-and-why-standard-errors-work",
    "href": "ch08-asymptotics.html#the-clt-and-why-standard-errors-work",
    "title": "8. Asymptotics",
    "section": "2 The CLT and why standard errors work",
    "text": "2 The CLT and why standard errors work\nThe central limit theorem says that standardized sample means converge to a normal distribution, regardless of the underlying data distribution:\n\\[\\sqrt{n}(\\bar{X}_n - \\mu) \\overset{d}{\\to} N(0, \\sigma^2) \\tag{1}\\]\nApplied to OLS, this gives:\n\\[\\sqrt{n}(\\hat{\\beta} - \\beta) \\overset{d}{\\to} N\\!\\left(0,\\; Q_{XX}^{-1}\\, \\Omega\\, Q_{XX}^{-1}\\right) \\tag{2}\\]\n\nTheorem 2 (Lindeberg-Levy CLT) If \\(X_1, \\ldots, X_n\\) are iid with mean \\(\\mu\\) and variance \\(\\sigma^2 &lt; \\infty\\), then \\(\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2)\\). Applied to OLS: \\(\\sqrt{n}(\\hat\\beta - \\beta) \\xrightarrow{d} N(0, Q_{XX}^{-1}\\Omega Q_{XX}^{-1})\\).\n\nwhere \\(\\Omega = E[X_i X_i' e_i^2]\\). This is the sandwich variance estimator, now justified by asymptotic theory. The convergence rate depends on the shape of the data — symmetric data need \\(n \\approx 30\\), skewed data may need \\(n \\geq 100\\), and heavy-tailed data may need much more.\n\nset.seed(42)\nB &lt;- 2000\n\n# How fast does the CLT kick in? Depends on the error distribution.\nsim_tstat &lt;- function(n, rdist_e) {\n  replicate(B, {\n    x &lt;- rnorm(n)\n    e &lt;- rdist_e(n)\n    y &lt;- 2 + 3 * x + e\n    fit &lt;- lm_robust(y ~ x, se_type = \"HC2\")\n    (coef(fit)[\"x\"] - 3) / fit$std.error[\"x\"]\n  })\n}\n\ndf_clt &lt;- rbind(\n  data.frame(t = sim_tstat(30,  function(n) rnorm(n)),\n             n = \"n = 30\",  errors = \"Normal\"),\n  data.frame(t = sim_tstat(30,  function(n) rexp(n) - 1),\n             n = \"n = 30\",  errors = \"Exponential (skewed)\"),\n  data.frame(t = sim_tstat(30,  function(n) (rt(n, df = 3) / sqrt(3))),\n             n = \"n = 30\",  errors = \"t(3) (heavy-tailed)\"),\n  data.frame(t = sim_tstat(200, function(n) rnorm(n)),\n             n = \"n = 200\", errors = \"Normal\"),\n  data.frame(t = sim_tstat(200, function(n) rexp(n) - 1),\n             n = \"n = 200\", errors = \"Exponential (skewed)\"),\n  data.frame(t = sim_tstat(200, function(n) (rt(n, df = 3) / sqrt(3))),\n             n = \"n = 200\", errors = \"t(3) (heavy-tailed)\")\n)\n\nggplot(df_clt, aes(t)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.5) +\n  stat_function(fun = dnorm, color = \"red\", linewidth = 0.8) +\n  facet_grid(errors ~ n) +\n  coord_cartesian(xlim = c(-4, 4)) +\n  labs(title = \"How fast does the CLT kick in?\",\n       subtitle = \"Red = N(0,1). Normal errors: fast. Skewed: OK by n=200. Heavy tails: slowest.\",\n       x = \"t-statistic (HC2)\")\n\n\n\n\nHow fast does the CLT kick in? Depends on the error distribution.\n\n\n\n\nWith normal errors, the CLT approximation is excellent even at \\(n = 30\\). With skewed errors (Exponential), it improves by \\(n = 200\\). With heavy-tailed errors (\\(t_3\\)), it remains imperfect even at \\(n = 200\\). This is why alternatives like the bootstrap matter."
  },
  {
    "objectID": "ch08-asymptotics.html#sec-delta-method",
    "href": "ch08-asymptotics.html#sec-delta-method",
    "title": "8. Asymptotics",
    "section": "3 The delta method",
    "text": "3 The delta method\nOften we care about a nonlinear function of the estimated parameters, not \\(\\hat{\\beta}\\) itself. The delta method uses a first-order Taylor expansion:\n\\[\\sqrt{n}(h(\\hat{\\theta}) - h(\\theta)) \\overset{d}{\\to} N(0, \\; \\nabla h' \\, V \\, \\nabla h) \\tag{3}\\]\n\nTheorem 3 (Delta Method) If \\(\\sqrt{n}(\\hat\\theta - \\theta) \\xrightarrow{d} N(0, V)\\) and \\(h\\) is continuously differentiable with \\(\\nabla h(\\theta) \\neq 0\\), then \\(\\sqrt{n}(h(\\hat\\theta) - h(\\theta)) \\xrightarrow{d} N(0, \\nabla h' V \\nabla h)\\).\n\n\n\n\n\n\n\nWarningDelta Method Fails at Boundaries\n\n\n\nThe delta method relies on a first-order Taylor expansion. It fails when \\(\\nabla h(\\theta) = 0\\) (e.g., testing \\(h(\\theta) = \\theta^2\\) at \\(\\theta = 0\\)) or when the function is not differentiable. In such cases, the bootstrap (or higher-order expansions) is more reliable.\n\n\nIn practice: compute the gradient \\(\\nabla h\\), sandwich it around the covariance matrix, take the square root. R’s car::deltaMethod() does this automatically.\n\n3.1 Applied example: long-run elasticity\nA model with a lagged dependent variable:\n\\[y_t = \\beta_0 + \\beta_1 x_t + \\gamma y_{t-1} + \\varepsilon_t\\]\nThe short-run effect of \\(x\\) is \\(\\beta_1\\). The long-run effect (after the dynamics play out) is:\n\\[\\theta = \\frac{\\beta_1}{1 - \\gamma}\\]\nThis is a ratio of regression coefficients — the delta method gives its standard error.\n\nset.seed(42)\nn &lt;- 200\nbeta0 &lt;- 1; beta1 &lt;- 0.5; gamma_true &lt;- 0.7\ny &lt;- numeric(n)\nx &lt;- rnorm(n)\ny[1] &lt;- rnorm(1)\nfor (t in 2:n) {\n  y[t] &lt;- beta0 + beta1 * x[t] + gamma_true * y[t - 1] + rnorm(1, sd = 0.5)\n}\n\ndta &lt;- data.frame(y = y[-1], x = x[-1], y_lag = y[-n])\nfit_dyn &lt;- lm(y ~ x + y_lag, data = dta)\n\n# Delta method (car package)\ndeltaMethod(fit_dyn, \"x / (1 - y_lag)\")\n\n              Estimate    SE 2.5 % 97.5 %\nx/(1 - y_lag)    1.332 0.177 0.985   1.68\n\n\nThe true long-run effect is \\(0.5/(1 - 0.7) = 1.67\\).\n\n\n3.2 Delta method by hand\nUnder the hood, deltaMethod() computes the gradient vector and applies \\(\\text{SE} = \\sqrt{\\nabla h' \\hat{V} \\nabla h}\\):\n\nb &lt;- coef(fit_dyn)\n\n# Gradient of h(beta) = beta1 / (1 - gamma) w.r.t. (intercept, x, y_lag)\ngrad &lt;- c(0,\n          1 / (1 - b[\"y_lag\"]),\n          b[\"x\"] / (1 - b[\"y_lag\"])^2)\n\nse_delta &lt;- as.numeric(sqrt(t(grad) %*% vcov(fit_dyn) %*% grad))\nlr_hat &lt;- b[\"x\"] / (1 - b[\"y_lag\"])\n\nc(estimate = lr_hat, se = se_delta,\n  ci_lo = lr_hat - 1.96 * se_delta, ci_hi = lr_hat + 1.96 * se_delta)\n\nestimate.x         se    ci_lo.x    ci_hi.x \n    1.3318     0.1768     0.9853     1.6783 \n\n\nThe delta method requires computing derivatives. For simple functions this is fine; for complicated transformations it becomes tedious. The bootstrap avoids derivatives entirely."
  },
  {
    "objectID": "ch08-asymptotics.html#the-bootstrap",
    "href": "ch08-asymptotics.html#the-bootstrap",
    "title": "8. Asymptotics",
    "section": "4 The bootstrap",
    "text": "4 The bootstrap\nThe bootstrap (Efron, 1979) replaces analytical derivations with simulation. The idea: if the sample is a good stand-in for the population, then resampling from the sample mimics sampling from the population. The variation across resamples estimates sampling variability.\n\n4.1 Nonparametric bootstrap for OLS\nThe pairs bootstrap resamples rows \\((Y_i, X_i)\\) with replacement, re-estimates OLS on each resample, and uses the distribution of \\(\\hat{\\beta}^*\\) across resamples to estimate the sampling distribution:\n\ndata(Prestige)\nmod &lt;- lm(prestige ~ income + education + women, data = Prestige)\n\n# Bootstrap by hand\nset.seed(42)\nB &lt;- 5000\nn &lt;- nrow(Prestige)\nboot_coefs &lt;- matrix(NA, B, length(coef(mod)))\n\nfor (b in 1:B) {\n  idx &lt;- sample(n, n, replace = TRUE)\n  boot_coefs[b, ] &lt;- coef(lm(prestige ~ income + education + women,\n                               data = Prestige[idx, ]))\n}\ncolnames(boot_coefs) &lt;- names(coef(mod))\n\n\n# Compare bootstrap SEs to classical and sandwich SEs\nse_classical &lt;- sqrt(diag(vcov(mod)))\nse_hc2 &lt;- sqrt(diag(vcovHC(mod, type = \"HC2\")))\nse_boot &lt;- apply(boot_coefs, 2, sd)\n\ndata.frame(\n  Variable = names(coef(mod)),\n  Classical = round(se_classical, 4),\n  HC2 = round(se_hc2, 4),\n  Bootstrap = round(se_boot, 4)\n)\n\n               Variable Classical    HC2 Bootstrap\n(Intercept) (Intercept)    3.2391 3.2167    3.2067\nincome           income    0.0003 0.0004    0.0004\neducation     education    0.3887 0.4469    0.4759\nwomen             women    0.0304 0.0355    0.0369\n\n\nThe bootstrap SEs are close to the sandwich (HC2) SEs — both account for heteroskedasticity without assuming a specific form. The classical SEs assume homoskedasticity, which may be wrong.\n\n\n4.2 Visualizing the bootstrap distribution\n\ndf_boot &lt;- data.frame(\n  income = boot_coefs[, \"income\"],\n  education = boot_coefs[, \"education\"]\n)\n\nggplot(df_boot, aes(income)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.5) +\n  geom_vline(xintercept = coef(mod)[\"income\"], color = \"blue\", linewidth = 1) +\n  stat_function(fun = dnorm,\n                args = list(mean = coef(mod)[\"income\"], sd = se_hc2[\"income\"]),\n                color = \"red\", linewidth = 0.8) +\n  labs(title = \"Bootstrap distribution of income coefficient\",\n       subtitle = \"Blue = point estimate, Red = normal approximation (sandwich SE)\",\n       x = expression(hat(beta)[income]^\"*\"))\n\n\n\n\n\n\n\n\n\n\n4.3 Using the boot package\n\n# The boot package standardizes this workflow\nboot_fn &lt;- function(data, indices) {\n  d &lt;- data[indices, ]\n  coef(lm(prestige ~ income + education + women, data = d))\n}\n\nboot_out &lt;- boot(Prestige, boot_fn, R = 5000)\nboot_out\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Prestige, statistic = boot_fn, R = 5000)\n\n\nBootstrap Statistics :\n     original     bias    std. error\nt1* -6.794334  0.0571783   3.2173023\nt2*  0.001314  0.0001055   0.0004326\nt3*  4.186637 -0.0831645   0.4685039\nt4* -0.008905  0.0055652   0.0366501\n\n\n\n# Bootstrap confidence intervals for education coefficient\nboot.ci(boot_out, index = 3, type = c(\"norm\", \"perc\", \"bca\"))\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 5000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_out, type = c(\"norm\", \"perc\", \"bca\"), \n    index = 3)\n\nIntervals : \nLevel      Normal             Percentile            BCa          \n95%   ( 3.352,  5.188 )   ( 3.043,  4.935 )   ( 3.261,  5.020 )  \nCalculations and Intervals on Original Scale\n\n\nThe boot.ci function provides three types of intervals:\n\nNormal: \\(\\hat{\\theta} \\pm z_{0.025} \\cdot \\text{SE}_{\\text{boot}}\\) (same logic as asymptotic, just uses bootstrap SE)\nPercentile: the 2.5th and 97.5th percentiles of \\(\\hat{\\theta}^*\\) (no normal assumption)\nBCa (bias-corrected, accelerated): adjusts for bias and skewness in the bootstrap distribution (generally preferred)"
  },
  {
    "objectID": "ch08-asymptotics.html#delta-method-vs.-bootstrap-a-direct-comparison",
    "href": "ch08-asymptotics.html#delta-method-vs.-bootstrap-a-direct-comparison",
    "title": "8. Asymptotics",
    "section": "5 Delta method vs. bootstrap: a direct comparison",
    "text": "5 Delta method vs. bootstrap: a direct comparison\nFor the long-run elasticity \\(\\theta = \\beta_1/(1 - \\gamma)\\), we can compare three approaches:\n\nset.seed(42)\nB &lt;- 5000\n\n# Bootstrap the long-run effect\nboot_lr &lt;- replicate(B, {\n  idx &lt;- sample(nrow(dta), nrow(dta), replace = TRUE)\n  b_star &lt;- coef(lm(y ~ x + y_lag, data = dta[idx, ]))\n  b_star[\"x\"] / (1 - b_star[\"y_lag\"])\n})\n\n# Delta method (from above)\ndm &lt;- deltaMethod(fit_dyn, \"x / (1 - y_lag)\")\n\ndata.frame(\n  Method = c(\"Delta method\", \"Bootstrap\"),\n  Estimate = c(dm$Estimate, mean(boot_lr)),\n  SE = c(dm$SE, sd(boot_lr)),\n  CI_lo = c(dm$`2.5 %`, quantile(boot_lr, 0.025)),\n  CI_hi = c(dm$`97.5 %`, quantile(boot_lr, 0.975))\n)\n\n           Method Estimate     SE  CI_lo CI_hi\n     Delta method    1.332 0.1768 0.9853 1.678\n2.5%    Bootstrap    1.350 0.1881 1.0229 1.756\n\n\n\ndf_lr &lt;- data.frame(lr = boot_lr)\nggplot(df_lr, aes(lr)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 60, alpha = 0.5) +\n  stat_function(fun = dnorm,\n                args = list(mean = dm$Estimate, sd = dm$SE),\n                color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = 0.5 / (1 - 0.7), linetype = \"dashed\") +\n  labs(title = \"Bootstrap vs. delta method for long-run elasticity\",\n       subtitle = \"Red = normal approximation (delta method). Dashed = true value 1.67.\",\n       x = expression(hat(beta)[x] / (1 - hat(gamma))))\n\n\n\n\n\n\n\n\nWhen the bootstrap distribution is close to normal, the two approaches agree. When it is skewed (common for ratios, especially when the denominator can be near zero), the bootstrap percentile interval is more reliable."
  },
  {
    "objectID": "ch08-asymptotics.html#when-does-each-approach-work",
    "href": "ch08-asymptotics.html#when-does-each-approach-work",
    "title": "8. Asymptotics",
    "section": "6 When does each approach work?",
    "text": "6 When does each approach work?\n\n6.1 Coverage simulation\nThe acid test for any inference method: does a nominal 95% interval actually contain the true parameter 95% of the time?\n\nset.seed(42)\nB &lt;- 500\nn_vals &lt;- c(20, 50, 200)\nB_boot &lt;- 200  # bootstrap replicates per simulation\n\nresults &lt;- list()\nfor (n in n_vals) {\n  cover_analytic &lt;- cover_sandwich &lt;- cover_boot &lt;- logical(B)\n  for (b in 1:B) {\n    x &lt;- rnorm(n)\n    sigma_i &lt;- 0.5 + abs(x)\n    e &lt;- rt(n, df = 3) / sqrt(3) * sigma_i  # heavy-tailed + heteroskedastic\n    y &lt;- 2 + 3 * x + e\n    dat &lt;- data.frame(y = y, x = x)\n    fit &lt;- lm(y ~ x, data = dat)\n\n    # Analytic (classical)\n    ci_a &lt;- confint(fit)[\"x\", ]\n    cover_analytic[b] &lt;- ci_a[1] &lt; 3 & 3 &lt; ci_a[2]\n\n    # Sandwich (HC2)\n    fit_r &lt;- lm_robust(y ~ x, data = dat, se_type = \"HC2\")\n    ci_s &lt;- c(fit_r$conf.low[\"x\"], fit_r$conf.high[\"x\"])\n    cover_sandwich[b] &lt;- ci_s[1] &lt; 3 & 3 &lt; ci_s[2]\n\n    # Bootstrap (percentile)\n    boot_b &lt;- replicate(B_boot, {\n      idx &lt;- sample(n, n, replace = TRUE)\n      coef(lm(y ~ x, data = dat[idx, ]))[\"x\"]\n    })\n    ci_boot &lt;- quantile(boot_b, c(0.025, 0.975))\n    cover_boot[b] &lt;- ci_boot[1] &lt; 3 & 3 &lt; ci_boot[2]\n  }\n  results[[length(results) + 1]] &lt;- data.frame(\n    n = n,\n    Classical = mean(cover_analytic),\n    HC2 = mean(cover_sandwich),\n    Bootstrap = mean(cover_boot)\n  )\n}\ndo.call(rbind, results)\n\n    n Classical   HC2 Bootstrap\n1  20     0.862 0.914     0.892\n2  50     0.838 0.958     0.932\n3 200     0.776 0.928     0.906\n\n\nKey patterns:\n\nClassical SEs undercover when errors are heteroskedastic (they assume homoskedasticity)\nHC2 sandwich SEs do well once \\(n\\) is moderate, but can undercover with very small \\(n\\) and heavy tails\nBootstrap tends to be robust across scenarios, especially with small \\(n\\)\n\n\n\n6.2 When the bootstrap breaks down\nThe bootstrap is not a panacea. It fails or becomes unreliable when:\n\nThe data are not iid (time series, clustered data) — you need a block bootstrap or cluster bootstrap instead\nThe estimator is not smooth (e.g., the maximum of the data) — the bootstrap distribution can be inconsistent\nThe sample is very small (\\(n &lt; 20\\)) — there aren’t enough distinct resamples to represent the population\n\n\n# Bootstrap fails for the sample maximum (non-smooth estimator)\nset.seed(42)\nB &lt;- 5000\nn &lt;- 50\ntrue_max &lt;- 1  # support of Uniform[0,1]\n\nboot_max &lt;- replicate(B, {\n  x &lt;- runif(n)\n  x_star &lt;- sample(x, n, replace = TRUE)\n  max(x_star)\n})\n\ndf_max &lt;- data.frame(max_star = boot_max)\nggplot(df_max, aes(max_star)) +\n  geom_histogram(bins = 50, alpha = 0.5) +\n  geom_vline(xintercept = true_max, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Bootstrap failure: sample maximum of Uniform[0,1]\",\n       subtitle = \"The bootstrap cannot exceed the sample max, so it cannot cover the true max = 1.\",\n       x = expression(max(X^\"*\")))\n\n\n\n\n\n\n\n\nThe bootstrap distribution of the sample maximum is bounded by the observed maximum — it can never reach the true parameter boundary. For smooth functions of sample means (like regression coefficients), this pathology does not arise."
  },
  {
    "objectID": "ch08-asymptotics.html#applied-workflow-prestige-data",
    "href": "ch08-asymptotics.html#applied-workflow-prestige-data",
    "title": "8. Asymptotics",
    "section": "7 Applied workflow: Prestige data",
    "text": "7 Applied workflow: Prestige data\nLet’s put everything together on a real example, comparing all approaches.\n\nmod &lt;- lm(prestige ~ income + education + women, data = Prestige)\n\n\n7.1 Standard errors: three ways\n\n# 1. Classical (assumes homoskedasticity + normality)\nse_class &lt;- sqrt(diag(vcov(mod)))\n\n# 2. Sandwich (asymptotic, no distributional assumptions)\nse_hc2 &lt;- sqrt(diag(vcovHC(mod, type = \"HC2\")))\n\n# 3. Bootstrap\nset.seed(42)\nboot_out &lt;- boot(Prestige,\n                  function(d, i) coef(lm(prestige ~ income + education + women, data = d[i, ])),\n                  R = 5000)\nse_boot &lt;- apply(boot_out$t, 2, sd)\n\ndata.frame(\n  Variable = names(coef(mod)),\n  Estimate = round(coef(mod), 4),\n  SE_classical = round(se_class, 4),\n  SE_HC2 = round(se_hc2, 4),\n  SE_bootstrap = round(se_boot, 4)\n)\n\n               Variable Estimate SE_classical SE_HC2 SE_bootstrap\n(Intercept) (Intercept)  -6.7943       3.2391 3.2167       3.1653\nincome           income   0.0013       0.0003 0.0004       0.0004\neducation     education   4.1866       0.3887 0.4469       0.4724\nwomen             women  -0.0089       0.0304 0.0355       0.0372\n\n\n\n\n7.2 Confidence intervals: three ways\n\n# Focus on the education coefficient (index 3)\nci_class &lt;- confint(mod)[\"education\", ]\nci_hc2 &lt;- coefci(mod, vcov = vcovHC(mod, type = \"HC2\"))[\"education\", ]\nci_boot &lt;- boot.ci(boot_out, index = 3, type = \"perc\")$percent[4:5]\n\ndata.frame(\n  Method = c(\"Classical\", \"HC2 sandwich\", \"Bootstrap percentile\"),\n  Lower = round(c(ci_class[1], ci_hc2[1], ci_boot[1]), 3),\n  Upper = round(c(ci_class[2], ci_hc2[2], ci_boot[2]), 3),\n  Width = round(c(diff(ci_class), diff(ci_hc2), diff(ci_boot)), 3)\n)\n\n                Method Lower Upper Width\n1            Classical 3.415 4.958 1.543\n2         HC2 sandwich 3.300 5.073 1.774\n3 Bootstrap percentile 3.025 4.881 1.856\n\n\n\n\n7.3 Nonlinear function: income-to-education ratio\nSuppose we want to compare the magnitude of the income and education effects. The ratio \\(\\theta = \\beta_{\\text{income}} / \\beta_{\\text{education}}\\) tells us how many prestige points a unit of income buys relative to a year of education.\n\n# Delta method\ndm_ratio &lt;- deltaMethod(mod, \"income / education\")\ndm_ratio\n\n                 Estimate       SE    2.5 % 97.5 %\nincome/education 3.14e-04 8.87e-05 1.40e-04      0\n\n# Bootstrap\nboot_ratio &lt;- boot_out$t[, 2] / boot_out$t[, 3]  # income / education\n\nc(delta_method_se = dm_ratio$SE,\n  bootstrap_se = sd(boot_ratio))\n\ndelta_method_se    bootstrap_se \n      8.866e-05       1.747e-04 \n\n\n\ndf_ratio &lt;- data.frame(ratio = boot_ratio)\nggplot(df_ratio, aes(ratio)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.5) +\n  stat_function(fun = dnorm,\n                args = list(mean = dm_ratio$Estimate, sd = dm_ratio$SE),\n                color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = dm_ratio$Estimate, linetype = \"dashed\") +\n  labs(title = \"Bootstrap vs. delta method for β_income / β_education\",\n       subtitle = \"Red = normal approximation (delta method). Bootstrap is slightly skewed.\",\n       x = expression(hat(beta)[income] / hat(beta)[education]))\n\n\n\n\n\n\n\n\n\n\n7.4 Wald test with robust and bootstrap covariance\n\n# Classical F-test\nlinearHypothesis(mod, c(\"education = 0\", \"women = 0\"))\n\n\nLinear hypothesis test:\neducation = 0\nwomen = 0\n\nModel 1: restricted model\nModel 2: prestige ~ income + education + women\n\n  Res.Df   RSS Df Sum of Sq    F Pr(&gt;F)    \n1    100 14616                             \n2     98  6034  2      8583 69.7 &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Robust Wald test (sandwich)\nlinearHypothesis(mod, c(\"education = 0\", \"women = 0\"),\n                 vcov = vcovHC(mod, type = \"HC2\"), test = \"Chisq\")\n\n\nLinear hypothesis test:\neducation = 0\nwomen = 0\n\nModel 1: restricted model\nModel 2: prestige ~ income + education + women\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df Chisq Pr(&gt;Chisq)    \n1    100                        \n2     98  2   118     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "ch08-asymptotics.html#the-asymptotic-toolkit-in-one-picture",
    "href": "ch08-asymptotics.html#the-asymptotic-toolkit-in-one-picture",
    "title": "8. Asymptotics",
    "section": "8 The asymptotic toolkit in one picture",
    "text": "8 The asymptotic toolkit in one picture\nEvery estimator in this course follows the same pattern:\n\n\n\n\n\n\n\n\nStep\nTool\nWhat it does\n\n\n\n\n1. Consistency\nWLLN + CMT\nSample moments → population moments; \\(\\hat{\\theta} \\overset{p}{\\to} \\theta\\)\n\n\n2. Asymptotic normality\nCLT + Slutsky\n\\(\\sqrt{n}(\\hat{\\theta} - \\theta) \\overset{d}{\\to} N(0, V)\\)\n\n\n3. Variance estimation\nSandwich\n\\(\\hat{V} \\overset{p}{\\to} V\\) (robust to misspecification)\n\n\n4. Nonlinear functions\nDelta method or bootstrap\nSEs for \\(h(\\hat{\\theta})\\)\n\n\n\nThis applies identically to OLS, probit MLE (Chapter 7), IV (Chapter 10), and GMM (Chapter 11) — only the moment condition changes:\n\n\n\nEstimator\nMoment condition\n\n\n\n\nOLS\n\\(E[X_i(Y_i - X_i'\\beta)] = 0\\)\n\n\nProbit MLE\n\\(E[s_i(\\beta)] = 0\\) (score)\n\n\nIV\n\\(E[Z_i(Y_i - X_i'\\beta)] = 0\\)\n\n\nGMM\n\\(E[g(W_i, \\theta)] = 0\\) (general)"
  },
  {
    "objectID": "ch08-asymptotics.html#summary",
    "href": "ch08-asymptotics.html#summary",
    "title": "8. Asymptotics",
    "section": "9 Summary",
    "text": "9 Summary\n\n\n\n\n\n\n\n\n\nMethod\nHow it works\nAdvantages\nLimitations\n\n\n\n\nClassical SE\nAssumes \\(e \\sim N(0, \\sigma^2)\\)\nExact under normality\nWrong under heteroskedasticity\n\n\nSandwich (HC2)\nEstimates \\(E[X_i X_i' e_i^2]\\)\nNo distributional assumptions\nNeeds moderate \\(n\\)\n\n\nDelta method\nTaylor expansion of \\(h(\\hat{\\beta})\\)\nFast, analytical\nRequires derivatives; assumes normality of \\(h(\\hat{\\beta})\\)\n\n\nBootstrap\nResample rows, re-estimate\nNo derivatives; captures skewness\nNeeds iid; slow; fails for non-smooth estimators\n\n\n\n\n\n\n\n\n\n\nTask\nR code\n\n\n\n\nSandwich SEs\nvcovHC(mod, type = \"HC2\") or lm_robust(..., se_type = \"HC2\")\n\n\nDelta method\ncar::deltaMethod(mod, \"expression\")\n\n\nBootstrap SEs\nboot(data, statistic, R = 5000)\n\n\nBootstrap CIs\nboot.ci(boot_out, type = \"perc\") or type = \"bca\"\n\n\nRobust Wald test\nlinearHypothesis(mod, ..., vcov = vcovHC, test = \"Chisq\")\n\n\n\nKey takeaways.\n\nThe CLT justifies using normal-based inference for OLS, probit, IV, and GMM — but convergence speed depends on the error distribution. Heavy tails and small samples require caution.\nThe sandwich SE and the bootstrap SE are both robust to heteroskedasticity. They usually agree for linear models with moderate \\(n\\).\nThe delta method and the bootstrap both handle nonlinear functions. Use the delta method when you can compute derivatives easily; use the bootstrap when the function is complicated or the normal approximation is suspect.\nWhen in doubt about whether \\(n\\) is “large enough,” compare all three approaches. If they disagree substantially, trust the bootstrap percentile or BCa interval."
  },
  {
    "objectID": "ch04-sensitivity.html",
    "href": "ch04-sensitivity.html",
    "title": "4. Sensitivity and Leverage",
    "section": "",
    "text": "library(ggplot2)\nlibrary(carData)\noptions(digits = 3)\ntr &lt;- function(M) sum(diag(M))\ndata(Prestige)\nIn applied work we rarely care about every regressor equally. We have a treatment or variable of interest (\\(X_1\\)) and controls we include to avoid omitted variable bias (\\(X_2\\)). Partitioning \\(X = [X_1 \\; X_2]\\) lets us answer three questions: What is the formula for \\(\\hat\\beta_1\\) holding \\(X_2\\) constant? What happens if we omit \\(X_2\\)? And how sensitive is \\(\\hat\\beta_1\\) to confounders we cannot observe?\nQuestions this chapter answers:"
  },
  {
    "objectID": "ch04-sensitivity.html#sec-fwl",
    "href": "ch04-sensitivity.html#sec-fwl",
    "title": "4. Sensitivity and Leverage",
    "section": "1 The Frisch-Waugh-Lovell theorem",
    "text": "1 The Frisch-Waugh-Lovell theorem\nThe FWL theorem says: the coefficient \\(\\hat\\beta_2\\) from the full regression \\(y = X_1\\beta_1 + X_2\\beta_2 + e\\) is identical to the coefficient from regressing the residualized outcome on the residualized treatment — after partialling out \\(X_1\\) from both.\nIn matrix terms, let \\(M_1 = I - X_1(X_1'X_1)^{-1}X_1'\\) be the annihilator for \\(X_1\\). Then:\n\\[\\hat\\beta_2 = (X_2'M_1 X_2)^{-1} X_2' M_1 y \\tag{1}\\]\nThis is just OLS on the residuals \\(M_1 y\\) and \\(M_1 X_2\\) — the parts of \\(y\\) and \\(X_2\\) that \\(X_1\\) cannot explain.\n\nTheorem 1 (Frisch-Waugh-Lovell Theorem) The coefficient \\(\\hat\\beta_2\\) from the full regression \\(y = X_1\\beta_1 + X_2\\beta_2 + e\\) equals the coefficient from regressing \\(M_1 y\\) on \\(M_1 X_2\\), where \\(M_1 = I - X_1(X_1'X_1)^{-1}X_1'\\). That is: partial out the controls from both sides, then run OLS.\n\n\n\n\n\n\n\nNoteFWL as a Bridge Between Chapters\n\n\n\nThe FWL theorem connects OLS geometry (Chapter 3) to applied causal inference. Partial regression plots — residualized \\(Y\\) vs. residualized \\(X\\) — visualize the multivariate coefficient in two dimensions. This same logic underlies fixed effects estimation (Chapter 12): demeaning within groups is FWL with group dummies as controls.\n\n\nLet’s verify with the Prestige data. We’ll show that the coefficient on education from a regression controlling for income and women is the same as the coefficient from the residual-on-residual regression:\n\n# Full regression\nmod_full &lt;- lm(prestige ~ education + income + women, data = Prestige)\n\n# Step 1: Residualize both y and education against (income, women)\ne_y &lt;- resid(lm(prestige ~ income + women, data = Prestige))\ne_educ &lt;- resid(lm(education ~ income + women, data = Prestige))\n\n# Step 2: Regress residuals on residuals\nmod_fwl &lt;- lm(e_y ~ e_educ)\n\nc(full_regression = coef(mod_full)[\"education\"],\n  FWL = coef(mod_fwl)[\"e_educ\"])\n\nfull_regression.education                FWL.e_educ \n                     4.19                      4.19 \n\n\nIdentical. FWL tells us that the coefficient on education reflects only the variation in education not explained by income and women.\n\n1.1 FWL with matrices\nLet’s do it with the projection and annihilator matrices directly:\n\ny &lt;- Prestige$prestige\nn &lt;- nrow(Prestige)\n\n# X1 = controls (intercept, income, women)\nX1 &lt;- cbind(1, Prestige$income, Prestige$women)\n# X2 = variable of interest (education)\nX2 &lt;- Prestige$education\n\n# Build the annihilator for X1\nP1 &lt;- X1 %*% solve(crossprod(X1)) %*% t(X1)\nM1 &lt;- diag(n) - P1\n\n# FWL formula: beta_2 = (X2'M1 X2)^{-1} X2'M1 y\nbeta_fwl &lt;- as.numeric(solve(t(X2) %*% M1 %*% X2) %*% t(X2) %*% M1 %*% y)\n\nc(matrix_FWL = beta_fwl, lm = coef(mod_full)[\"education\"])\n\n  matrix_FWL lm.education \n        4.19         4.19"
  },
  {
    "objectID": "ch04-sensitivity.html#plotting-partial-effects",
    "href": "ch04-sensitivity.html#plotting-partial-effects",
    "title": "4. Sensitivity and Leverage",
    "section": "2 Plotting partial effects",
    "text": "2 Plotting partial effects\nOne practical benefit of FWL: it lets us visualize relationships from a multivariate regression in two dimensions. After partialling out the controls, we can scatter the residualized \\(y\\) against the residualized \\(x\\) and draw the partial regression line.\n\ndf_partial &lt;- data.frame(educ_resid = e_educ, prestige_resid = e_y,\n                         job = rownames(Prestige))\n\nggplot(df_partial, aes(educ_resid, prestige_resid)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"steelblue\", linewidth = 1) +\n  geom_text(data = df_partial[abs(df_partial$prestige_resid) &gt; 15, ],\n            aes(label = job), hjust = -0.1, size = 2.5, alpha = 0.7) +\n  labs(x = \"Education residual (net of income, women)\",\n       y = \"Prestige residual (net of income, women)\",\n       title = \"Partial regression plot: education → prestige\",\n       subtitle = paste0(\"Slope = \", round(coef(mod_fwl)[2], 2),\n                         \" (same as the multivariate coefficient)\")) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nPartial regression plot: education → prestige (FWL residuals)\n\n\n\n\nThe slope of this line is the multivariate regression coefficient. Each point shows an occupation’s education and prestige after removing what income and gender composition predict. Ministers have high prestige residuals — more prestige than their income and gender composition would suggest."
  },
  {
    "objectID": "ch04-sensitivity.html#leverage-which-observations-pull-the-line",
    "href": "ch04-sensitivity.html#leverage-which-observations-pull-the-line",
    "title": "4. Sensitivity and Leverage",
    "section": "3 Leverage: which observations pull the line?",
    "text": "3 Leverage: which observations pull the line?\nThe diagonal elements of the hat matrix \\(P = X(X'X)^{-1}X'\\) measure leverage — how unusual each observation’s \\(X\\) values are relative to the rest of the data. The \\(i\\)-th leverage value is:\n\\[h_{ii} = X_i'(X'X)^{-1}X_i\\]\n\nDefinition 1 (Leverage) The leverage of observation \\(i\\) is \\(h_{ii} = X_i'(X'X)^{-1}X_i\\), the \\(i\\)-th diagonal element of the hat matrix \\(P\\). It measures how unusual the observation’s covariates are: \\(K/n \\leq h_{ii} \\leq 1\\), and \\(\\sum h_{ii} = K\\).\n\n\nX &lt;- cbind(1, Prestige$education, Prestige$income, Prestige$women)\nP &lt;- X %*% solve(crossprod(X)) %*% t(X)\nK &lt;- ncol(X)\n\n# Leverage = diagonal of P\nh &lt;- diag(P)\n\n# hatvalues() gives the same thing\nall.equal(h, as.numeric(hatvalues(mod_full)))\n\n[1] TRUE\n\n# Label with occupation names for later use\nnames(h) &lt;- rownames(Prestige)\n\n# Properties: leverage is between 0 and 1, sums to K\nc(min = min(h), max = max(h), sum = sum(h), K = K)\n\n   min    max    sum      K \n0.0104 0.3422 4.0000 4.0000 \n\n\nA regression is balanced when leverage values are roughly equal at \\(K/n\\). Observations far from the center of the \\(X\\) space have high leverage — they pull the regression line toward them:\n\ndf_lev &lt;- data.frame(leverage = h, job = rownames(Prestige))\n\nggplot(df_lev, aes(x = reorder(job, leverage), y = leverage)) +\n  geom_point(size = 0.8) +\n  geom_hline(yintercept = K / n, linetype = \"dashed\", color = \"tomato\") +\n  annotate(\"text\", x = 10, y = K / n + 0.005, label = \"K/n (balanced)\",\n           color = \"tomato\", size = 3) +\n  labs(x = \"\", y = \"Leverage (h_ii)\",\n       title = \"Leverage values for Prestige regression\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n# Which occupations have the highest leverage?\nhead(sort(h, decreasing = TRUE), 5)\n\n     general.managers            physicians             ministers \n               0.3422                0.2435                0.1021 \n              lawyers sewing.mach.operators \n               0.0988                0.0978"
  },
  {
    "objectID": "ch04-sensitivity.html#leave-one-out-regression",
    "href": "ch04-sensitivity.html#leave-one-out-regression",
    "title": "4. Sensitivity and Leverage",
    "section": "4 Leave-one-out regression",
    "text": "4 Leave-one-out regression\nIf we refit the model dropping observation \\(i\\), how much does \\(\\hat\\beta\\) change? The leave-one-out coefficient is:\n\\[\\hat\\beta_{(-i)} = \\hat\\beta - (X'X)^{-1}X_i \\tilde{e}_i\\]\nwhere \\(\\tilde{e}_i = \\hat{e}_i / (1 - h_{ii})\\) is the leave-one-out residual — the ordinary residual inflated by the leverage. High leverage shrinks the ordinary residual (the observation pulls the line toward itself), so dividing by \\((1 - h_{ii})\\) corrects for this.\n\nTheorem 2 (Leave-One-Out Formula) The leave-one-out coefficient change is \\(\\hat\\beta_{(-i)} = \\hat\\beta - (X'X)^{-1}X_i \\tilde{e}_i\\), where \\(\\tilde{e}_i = \\hat{e}_i / (1 - h_{ii})\\). High leverage shrinks ordinary residuals; dividing by \\((1 - h_{ii})\\) corrects for this self-influence.\n\n\n# Ordinary residuals\ne_hat &lt;- resid(mod_full)\n\n# Leave-one-out residuals\ne_tilde &lt;- e_hat / (1 - h)\n\n# Studentized residuals: leave-one-out residual / its standard error\n# rstudent() uses sigma_{(-i)}, the error variance without obs i\nrst &lt;- rstudent(mod_full)\n\n# Compare the first few\nhead(cbind(ordinary = e_hat, leave_one_out = e_tilde, studentized = rst))\n\n                    ordinary leave_one_out studentized\ngov.administrators      4.58          4.71       0.590\ngeneral.managers       -9.39        -14.28      -1.485\naccountants             4.69          4.78       0.601\npurchasing.officers     4.22          4.28       0.540\nchemists                8.15          8.55       1.065\nphysicists              4.47          4.73       0.584\n\n\nAn observation is influential if it has both high leverage and a large residual. The change in fitted values when observation \\(i\\) is dropped is:\n\\[\\hat{Y}_i - \\tilde{Y}_i = h_{ii} \\tilde{e}_i\\]\n\ndf_infl &lt;- data.frame(leverage = h, rstudent = rst, job = rownames(Prestige))\ndf_infl$flag &lt;- abs(rst) &gt; 2 | h &gt; 3 * K / n\n\nggplot(df_infl, aes(leverage, rstudent)) +\n  geom_point(aes(color = flag), size = 1.5) +\n  geom_text(data = df_infl[df_infl$flag, ],\n            aes(label = job), hjust = -0.1, size = 2.5) +\n  geom_hline(yintercept = c(-2, 2), linetype = \"dashed\", alpha = 0.4) +\n  geom_vline(xintercept = 3 * K / n, linetype = \"dashed\", alpha = 0.4) +\n  scale_color_manual(values = c(\"FALSE\" = \"gray50\", \"TRUE\" = \"tomato\"),\n                     guide = \"none\") +\n  labs(x = \"Leverage (h_ii)\", y = \"Studentized residual\",\n       title = \"Influential observations: high leverage AND large residual\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nObservations in the upper-right or lower-right are candidates for investigation: they have unusual \\(X\\) values and don’t fit the model well. This could indicate a data error, a different population, or a genuinely interesting case."
  },
  {
    "objectID": "ch04-sensitivity.html#regression-weights-which-observations-matter-most",
    "href": "ch04-sensitivity.html#regression-weights-which-observations-matter-most",
    "title": "4. Sensitivity and Leverage",
    "section": "5 Regression weights: which observations matter most?",
    "text": "5 Regression weights: which observations matter most?\nThe FWL result reveals that OLS assigns implicit weights to observations. For a single variable of interest \\(z\\) in a regression with controls \\(X\\), the coefficient is:\n\\[b = \\frac{\\sum z_i^* y_i}{\\sum z_i^{*2}}\\]\nwhere \\(z_i^* = (Mz)_i\\) is the residual from regressing \\(z\\) on the controls. Observations where \\(z_i^{*2}\\) is large — where the variable of interest has a lot of variation not explained by controls — receive the most weight.\n\n# Women's share in occupation: variable of interest\n# Controls: intercept, education, income\nz_star &lt;- as.vector(M1 %*% Prestige$women)  # M1 already built above\n\n# Regression weights\nomega &lt;- z_star^2\nnames(omega) &lt;- rownames(Prestige)\n\n# Most and least weighted occupations\ncat(\"Highest weight (most variation in women% net of controls):\\n\")\n\nHighest weight (most variation in women% net of controls):\n\nhead(sort(omega, decreasing = TRUE), 5)\n\n              physicians         general.managers osteopaths.chiropractors \n                4.53e-26                 2.33e-26                 1.67e-26 \n                 lawyers             farm.workers \n                1.57e-26                 9.39e-27 \n\ncat(\"\\nLowest weight (almost no unique variation):\\n\")\n\n\nLowest weight (almost no unique variation):\n\nhead(sort(omega), 5)\n\n construction.foremen          receptionsts              athletes \n             3.77e-32              1.97e-31              8.91e-31 \ncommercial.travellers     sales.supervisors \n             1.08e-30              1.77e-30 \n\n\n\n# Verify: weighted formula gives same coefficient as lm\nb_weighted &lt;- sum(z_star * y) / sum(z_star^2)\nc(weighted_formula = b_weighted, lm = coef(mod_full)[\"women\"])\n\nweighted_formula         lm.women \n       -4.26e+14        -8.91e-03 \n\n\nOccupations like general managers and ministers — where the share of women is very different from what education and income would predict — have thousands of times more influence on the coefficient than occupations where women’s share is well-predicted by the controls."
  },
  {
    "objectID": "ch04-sensitivity.html#partial-r2",
    "href": "ch04-sensitivity.html#partial-r2",
    "title": "4. Sensitivity and Leverage",
    "section": "6 Partial \\(R^2\\)",
    "text": "6 Partial \\(R^2\\)\nThe partial \\(R^2\\) measures how much of the remaining variance in \\(Y\\) (after accounting for \\(X_1\\)) is explained by \\(X_2\\):\n\\[R^2_{Y \\sim X_2 | X_1} = 1 - \\frac{\\text{RSS}(X_1, X_2)}{\\text{RSS}(X_1)} = \\frac{\\text{RSS}(X_1) - \\text{RSS}(X_1, X_2)}{\\text{RSS}(X_1)} \\tag{2}\\]\nFor a single variable, the partial \\(R^2\\) equals the squared partial correlation:\n\n# RSS from controls only\nmod_controls &lt;- lm(prestige ~ income + women, data = Prestige)\nRSS_controls &lt;- sum(resid(mod_controls)^2)\n\n# RSS from full model\nRSS_full &lt;- sum(resid(mod_full)^2)\n\n# Partial R^2 of education given (income, women)\npartial_r2_educ &lt;- 1 - RSS_full / RSS_controls\n\n# Equivalently: squared correlation of FWL residuals\ncor_fwl &lt;- cor(e_y, e_educ)^2\n\nc(partial_R2 = partial_r2_educ, squared_partial_cor = cor_fwl)\n\n         partial_R2 squared_partial_cor \n              0.542               0.542 \n\n\nWe can also compute partial \\(R^2\\) for each variable using the matrix formula. The FWL residuals give us everything we need:\n\n# Partial R^2 for each variable\npartial_r2 &lt;- function(mod, var_name) {\n  formula_reduced &lt;- update(formula(mod), paste(\"~ . -\", var_name))\n  mod_reduced &lt;- lm(formula_reduced, data = Prestige)\n  1 - sum(resid(mod)^2) / sum(resid(mod_reduced)^2)\n}\n\ndata.frame(\n  variable = c(\"education\", \"income\", \"women\"),\n  partial_R2 = sapply(c(\"education\", \"income\", \"women\"),\n                       function(v) partial_r2(mod_full, v))\n)\n\n           variable partial_R2\neducation education   0.542079\nincome       income   0.185784\nwomen         women   0.000874\n\n\nEducation has a high partial \\(R^2\\) — it explains a large share of prestige variation that income and women’s share cannot. Women’s share has a very low partial \\(R^2\\): once we know education and income, knowing the gender composition adds almost nothing."
  },
  {
    "objectID": "ch04-sensitivity.html#sensitivity-analysis-cinelli-hazlett-2020",
    "href": "ch04-sensitivity.html#sensitivity-analysis-cinelli-hazlett-2020",
    "title": "4. Sensitivity and Leverage",
    "section": "7 Sensitivity analysis: Cinelli-Hazlett (2020)",
    "text": "7 Sensitivity analysis: Cinelli-Hazlett (2020)\nEven after including controls, there may be unobserved confounders. The OVB formula from Chapter 2 says the bias from omitting a variable \\(Z\\) is:\n\\[\\text{bias} = \\underbrace{\\frac{\\text{Cov}(D, Z)}{\\text{Var}(D)}}_{\\text{imbalance}} \\times \\underbrace{\\frac{\\text{Cov}(Z^{\\perp D}, Y^{\\perp D})}{\\text{Var}(Z^{\\perp D})}}_{\\text{impact}}\\]\nCinelli and Hazlett (2020) reparameterize this in terms of partial \\(R^2\\) values, which are easier to reason about:\n\\[|\\text{bias}| \\propto \\sqrt{\\frac{R^2_{D \\sim Z} \\cdot R^2_{Y \\sim Z|D}}{1 - R^2_{D \\sim Z}}}\\]\nThe key insight: a confounder must predict both treatment and outcome to generate meaningful bias. If either partial \\(R^2\\) is small, the bias is small.\nLet’s simulate and run a sensitivity analysis:\n\nset.seed(123)\nN &lt;- 1000\nbeta_D &lt;- 2; beta_Z &lt;- 3\n\nD &lt;- rbinom(N, 1, 0.5)\nZ &lt;- rnorm(N)\nY &lt;- beta_D * D + beta_Z * Z + rnorm(N)\n\n# Naive model (omitting Z) -- biased\ncoef(lm(Y ~ D))\n\n(Intercept)           D \n     0.0569      1.9519 \n\n# Full model (including Z) -- unbiased\nmodel_full_sim &lt;- lm(Y ~ D + Z)\ncoef(model_full_sim)\n\n(Intercept)           D           Z \n     0.0254      1.9417      3.0600 \n\n\nThe naive model overestimates the effect of \\(D\\). The full model recovers \\(\\beta_D \\approx 2\\). But what if there were another confounder we couldn’t observe? We use the observed confounder \\(Z\\) as a benchmark:\n\n# How strong is Z as a confounder?\n# Partial R^2 of Z on D\npartial_r2_DZ &lt;- cor(D, Z)^2  # for a single variable, partial R^2 ≈ cor^2\n\n# Partial R^2 of Z on Y|D\nRSS_D_only &lt;- sum(resid(lm(Y ~ D))^2)\nRSS_full_sim &lt;- sum(resid(model_full_sim)^2)\npartial_r2_YZ_D &lt;- 1 - RSS_full_sim / RSS_D_only\n\nc(R2_D_Z = partial_r2_DZ, R2_Y_Z_given_D = partial_r2_YZ_D)\n\n        R2_D_Z R2_Y_Z_given_D \n      2.78e-06       9.07e-01 \n\n\nAn unobserved confounder would need partial \\(R^2\\) values at least this large with both \\(D\\) and \\(Y\\) to generate comparable bias. If the strongest observed predictor explains only a few percent of residual variation, an omitted variable would need to be far stronger to overturn the result.\n\n# The sensemakr package automates this analysis\nif (requireNamespace(\"sensemakr\", quietly = TRUE)) {\n  library(sensemakr)\n  sens &lt;- sensemakr(model_full_sim, treatment = \"D\",\n                    benchmark_covariates = \"Z\")\n  summary(sens)\n}\n\nSensitivity Analysis to Unobserved Confounding\n\nModel Formula: Y ~ D + Z\n\nNull hypothesis: q = 1 and reduce = TRUE \n-- This means we are considering biases that reduce the absolute value of the current estimate.\n-- The null hypothesis deemed problematic is H0:tau = 0 \n\nUnadjusted Estimates of 'D': \n  Coef. estimate: 1.94 \n  Standard Error: 0.062 \n  t-value (H0:tau = 0): 31.2 \n\nSensitivity Statistics:\n  Partial R2 of treatment with outcome: 0.494 \n  Robustness Value, q = 1: 0.614 \n  Robustness Value, q = 1, alpha = 0.05: 0.592 \n\nVerbal interpretation of sensitivity statistics:\n\n-- Partial R2 of the treatment with the outcome: an extreme confounder (orthogonal to the covariates) that explains 100% of the residual variance of the outcome, would need to explain at least 49.4% of the residual variance of the treatment to fully account for the observed estimated effect.\n\n-- Robustness Value, q = 1: unobserved confounders (orthogonal to the covariates) that explain more than 61.4% of the residual variance of both the treatment and the outcome are strong enough to bring the point estimate to 0 (a bias of 100% of the original estimate). Conversely, unobserved confounders that do not explain more than 61.4% of the residual variance of both the treatment and the outcome are not strong enough to bring the point estimate to 0.\n\n-- Robustness Value, q = 1, alpha = 0.05: unobserved confounders (orthogonal to the covariates) that explain more than 59.2% of the residual variance of both the treatment and the outcome are strong enough to bring the estimate to a range where it is no longer 'statistically different' from 0 (a bias of 100% of the original estimate), at the significance level of alpha = 0.05. Conversely, unobserved confounders that do not explain more than 59.2% of the residual variance of both the treatment and the outcome are not strong enough to bring the estimate to a range where it is no longer 'statistically different' from 0, at the significance level of alpha = 0.05.\n\nBounds on omitted variable bias:\n\n--The table below shows the maximum strength of unobserved confounders with association with the treatment and the outcome bounded by a multiple of the observed explanatory power of the chosen benchmark covariate(s).\n\n Bound Label R2dz.x R2yz.dx Treatment Adjusted Estimate Adjusted Se Adjusted T\n        1x Z      0       1         D              1.94           0        Inf\n Adjusted Lower CI Adjusted Upper CI\n              1.94              1.94\n\n\nThe robustness value tells us the minimum strength an unobserved confounder must have (in terms of partial \\(R^2\\) with both \\(D\\) and \\(Y\\)) to explain away the entire estimated effect.\n\n\n\n\n\n\nWarningSensitivity Does Not Prove Robustness\n\n\n\nA large robustness value means the result survives hypothetical confounders of a given strength — but it cannot rule out their existence. Sensitivity analysis quantifies what would be needed to overturn a finding; it does not establish that no such confounder exists."
  },
  {
    "objectID": "ch04-sensitivity.html#summary",
    "href": "ch04-sensitivity.html#summary",
    "title": "4. Sensitivity and Leverage",
    "section": "8 Summary",
    "text": "8 Summary\n\nFWL theorem: The coefficient on \\(X_2\\) in \\(y = X_1\\beta_1 + X_2\\beta_2 + e\\) equals the coefficient from regressing \\(M_1 y\\) on \\(M_1 X_2\\) — residualize both sides against the controls.\nPartial regression plots let us visualize multivariate relationships in 2D using FWL residuals.\nLeverage \\(h_{ii} = X_i'(X'X)^{-1}X_i\\) measures how unusual observation \\(i\\)’s covariates are. High leverage + large residual = influential observation.\nLeave-one-out residuals \\(\\tilde{e}_i = \\hat{e}_i/(1-h_{ii})\\) correct for the self-influence of observation \\(i\\).\nRegression weights: OLS implicitly weights observations by \\(z_i^{*2}\\), the squared residual from regressing the variable of interest on controls. Observations with more unique variation in the treatment get more weight.\nPartial \\(R^2\\) measures the share of residual variance explained by a variable after accounting for other regressors.\nSensitivity analysis (Cinelli-Hazlett): bias from an omitted confounder depends on its partial \\(R^2\\) with both treatment and outcome. Use observed covariates as benchmarks.\n\nNext: Efficiency and GLS — the Gauss-Markov theorem and generalized least squares."
  },
  {
    "objectID": "ch02-cef-blp.html",
    "href": "ch02-cef-blp.html",
    "title": "2. The CEF and Best Linear Predictor",
    "section": "",
    "text": "library(ggplot2)\noptions(digits = 3)\nThe conditional expectation function \\(m(X) = \\mathbb{E}[Y|X]\\) is the best predictor of \\(Y\\) given \\(X\\) — but we rarely know it. The best linear predictor (BLP) is the next best thing: a linear approximation that only requires knowing means, variances, and covariances. This chapter explores the gap between the two and what we gain (and lose) by going linear.\nQuestions this chapter answers:"
  },
  {
    "objectID": "ch02-cef-blp.html#cef-error-mean-independence-is-not-independence",
    "href": "ch02-cef-blp.html#cef-error-mean-independence-is-not-independence",
    "title": "2. The CEF and Best Linear Predictor",
    "section": "1 CEF error: mean independence is not independence",
    "text": "1 CEF error: mean independence is not independence\nDefine the CEF error as \\(e = Y - m(X)\\). By construction, \\(\\mathbb{E}[e|X] = 0\\) — this is called mean independence. But mean independence is weaker than full independence: the variance of \\(e\\) can still depend on \\(X\\).\nHere’s the lecture’s example: \\(X \\sim \\text{Uniform}(-1, 1)\\) and \\(Y = X^2 + X\\varepsilon\\), where \\(\\varepsilon \\sim N(0,1)\\) is independent of \\(X\\).\n\nDefinition 1 (Conditional Expectation Function) The CEF is \\(m(X) = \\mathbb{E}[Y|X]\\), the function that gives the mean of \\(Y\\) for each value of \\(X\\). The CEF error \\(e = Y - m(X)\\) satisfies mean independence: \\(\\mathbb{E}[e|X] = 0\\).\n\n\nset.seed(307)\nn &lt;- 5000\nx &lt;- runif(n, -1, 1)\neps &lt;- rnorm(n)\ny &lt;- x^2 + x * eps\n\n# CEF is m(X) = X^2\nm_x &lt;- x^2\ne &lt;- y - m_x\n\n# Mean independence: E[e|X] ≈ 0 in every bin\nbins &lt;- cut(x, breaks = 20)\nbin_means &lt;- tapply(e, bins, mean)\nround(bin_means, 3)\n\n         (-1,-0.9]        (-0.9,-0.8]        (-0.8,-0.7]        (-0.7,-0.6] \n            -0.062              0.036              0.036              0.070 \n       (-0.6,-0.5]        (-0.5,-0.4]        (-0.4,-0.3]        (-0.3,-0.2] \n            -0.006              0.021              0.012              0.018 \n       (-0.2,-0.1]   (-0.1,-0.000463] (-0.000463,0.0995]     (0.0995,0.199] \n            -0.009              0.000              0.002              0.006 \n     (0.199,0.299]      (0.299,0.399]      (0.399,0.499]      (0.499,0.599] \n            -0.007             -0.031             -0.056             -0.037 \n     (0.599,0.699]      (0.699,0.799]      (0.799,0.899]          (0.899,1] \n             0.033             -0.003              0.016              0.026 \n\n\nThe conditional means of \\(e\\) are all near zero. But the conditional variance grows with \\(|X|\\):\n\ndf &lt;- data.frame(x = x, e = e)\n\nggplot(df, aes(x, e)) +\n  geom_point(alpha = 0.15, size = 0.8) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"steelblue\", linewidth = 1) +\n  labs(x = \"X\", y = \"CEF error (e)\",\n       title = \"Mean independence holds, but errors are heteroskedastic\",\n       subtitle = \"Var(e|X) = X², so spread increases with |X|\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe smooth line hovers at zero (mean independence), but the spread fans out. This pattern — \\(\\text{Var}(e|X) = X^2\\) — is heteroskedasticity. The CEF decomposes \\(Y\\) into signal and noise, but doesn’t guarantee the noise is uniform."
  },
  {
    "objectID": "ch02-cef-blp.html#sec-cef-best",
    "href": "ch02-cef-blp.html#sec-cef-best",
    "title": "2. The CEF and Best Linear Predictor",
    "section": "2 The CEF is the best predictor",
    "text": "2 The CEF is the best predictor\nAmong all functions of \\(X\\), the CEF minimizes mean squared prediction error. Let’s demonstrate by comparing the CEF to some alternatives for the same DGP.\n\n# True CEF: m(X) = X^2\n# Competitor 1: the unconditional mean\n# Competitor 2: a linear predictor\n# Competitor 3: a cubic predictor\n\nmse_cef &lt;- mean((y - x^2)^2)\nmse_mean &lt;- mean((y - mean(y))^2)\nmse_linear &lt;- mean(residuals(lm(y ~ x))^2)\nmse_cubic &lt;- mean(residuals(lm(y ~ x + I(x^2) + I(x^3)))^2)\n\ndata.frame(\n  predictor = c(\"Unconditional mean\", \"Linear (BLP)\", \"Cubic\", \"CEF (X²)\"),\n  MSE = round(c(mse_mean, mse_linear, mse_cubic, mse_cef), 4)\n)\n\n           predictor   MSE\n1 Unconditional mean 0.401\n2       Linear (BLP) 0.401\n3              Cubic 0.311\n4           CEF (X²) 0.312\n\n\nThe CEF has the lowest MSE. The cubic comes close because \\(X^3\\) has mean zero for symmetric \\(X\\), so the cubic approximation is nearly \\(X^2\\). The linear predictor does worse — it can’t capture the curvature."
  },
  {
    "objectID": "ch02-cef-blp.html#cef-vs.-blp-when-the-line-misses-the-curve",
    "href": "ch02-cef-blp.html#cef-vs.-blp-when-the-line-misses-the-curve",
    "title": "2. The CEF and Best Linear Predictor",
    "section": "3 CEF vs. BLP: when the line misses the curve",
    "text": "3 CEF vs. BLP: when the line misses the curve\nThe BLP minimizes \\(\\mathbb{E}[(Y - a - bX)^2]\\) and yields \\(b = \\text{Cov}(X,Y)/\\text{Var}(X)\\). When the CEF is nonlinear, the BLP draws the best straight line through a curved relationship.\n\nDefinition 2 (Best Linear Predictor) The BLP of \\(Y\\) given \\(X\\) is \\(\\alpha + \\beta'X\\) where \\(\\beta = \\text{Var}(X)^{-1}\\text{Cov}(X, Y)\\). The BLP error satisfies \\(\\mathbb{E}[Xe] = 0\\) (uncorrelated with \\(X\\)) but not necessarily \\(\\mathbb{E}[e|X] = 0\\).\n\n\n# Same DGP: Y = X^2 + X*eps\ndf &lt;- data.frame(x = x, y = y)\n\nggplot(df, aes(x, y)) +\n  geom_point(alpha = 0.1, size = 0.5) +\n  stat_function(fun = function(x) x^2, aes(color = \"CEF: X²\"),\n                linewidth = 1.2) +\n  geom_smooth(method = \"lm\", se = FALSE, aes(color = \"BLP\"),\n              linewidth = 1.2) +\n  scale_color_manual(values = c(\"CEF: X²\" = \"tomato\", \"BLP\" = \"steelblue\"),\n                     name = \"\") +\n  labs(x = \"X\", y = \"Y\",\n       title = \"The BLP is the best line through a curved CEF\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe BLP is the best line through a curved CEF\n\n\n\n\nThe BLP residual \\(e = Y - X'\\beta\\) satisfies \\(\\mathbb{E}[Xe] = 0\\) (uncorrelated with \\(X\\)), but not \\(\\mathbb{E}[e|X] = 0\\). The residual still contains nonlinear structure: the BLP intercept absorbs \\(\\mathbb{E}[X^2] = 1/3\\), so the conditional mean of the residual traces out \\(X^2 - 1/3\\) rather than zero:\n\nmod &lt;- lm(y ~ x)\ndf$blp_resid &lt;- residuals(mod)\n\nggplot(df, aes(x, blp_resid)) +\n  geom_point(alpha = 0.1, size = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"tomato\", linewidth = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"X\", y = \"BLP residual\",\n       title = \"BLP residuals still contain nonlinear structure\",\n       subtitle = \"E[e|X] ≠ 0, but E[Xe] = 0\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "ch02-cef-blp.html#when-does-blp-cef",
    "href": "ch02-cef-blp.html#when-does-blp-cef",
    "title": "2. The CEF and Best Linear Predictor",
    "section": "4 When does BLP = CEF?",
    "text": "4 When does BLP = CEF?\nThe BLP equals the CEF when the CEF is actually linear. An important sufficient condition: if \\((X, Y)\\) are jointly normal, the CEF is linear. Let’s verify.\n\nlibrary(MASS)\nset.seed(42)\n\n# Jointly normal (X, Y)\nSigma &lt;- matrix(c(1, 0.7, 0.7, 1), 2, 2)\njoint &lt;- mvrnorm(5000, mu = c(0, 0), Sigma = Sigma)\nx_norm &lt;- joint[, 1]\ny_norm &lt;- joint[, 2]\n\ndf_norm &lt;- data.frame(x = x_norm, y = y_norm)\n\nggplot(df_norm, aes(x, y)) +\n  geom_point(alpha = 0.1, size = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, aes(color = \"BLP\"), linewidth = 1.2) +\n  geom_smooth(method = \"loess\", se = FALSE, aes(color = \"CEF (loess)\"), linewidth = 1.2) +\n  scale_color_manual(values = c(\"BLP\" = \"steelblue\", \"CEF (loess)\" = \"tomato\"),\n                     name = \"\") +\n  labs(title = \"Jointly normal: the BLP and CEF coincide\",\n       x = \"X\", y = \"Y\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe two lines overlap because the CEF is genuinely linear when the data are jointly normal.\n\n\n\n\n\n\nNoteJoint Normality Implies Linear CEF\n\n\n\nIf \\((X, Y)\\) are jointly normal, the conditional expectation \\(\\mathbb{E}[Y|X]\\) is exactly linear in \\(X\\), and the BLP equals the CEF. This is a sufficient but not necessary condition — other distributions can also produce linear CEFs."
  },
  {
    "objectID": "ch02-cef-blp.html#the-blp-in-formulas-and-code",
    "href": "ch02-cef-blp.html#the-blp-in-formulas-and-code",
    "title": "2. The CEF and Best Linear Predictor",
    "section": "5 The BLP in formulas and code",
    "text": "5 The BLP in formulas and code\nThe population BLP coefficient is \\(\\beta = \\text{Var}(X)^{-1}\\text{Cov}(X, Y)\\). In a sample, OLS estimates this:\n\n# Bivariate: beta = Cov(X,Y) / Var(X)\nbeta_formula &lt;- cov(x_norm, y_norm) / var(x_norm)\nalpha_formula &lt;- mean(y_norm) - beta_formula * mean(x_norm)\n\n# OLS gives the same thing\nbeta_ols &lt;- coef(lm(y_norm ~ x_norm))\n\ncbind(formula = c(alpha_formula, beta_formula),\n      OLS = beta_ols)\n\n             formula      OLS\n(Intercept) -0.00942 -0.00942\nx_norm       0.69753  0.69753"
  },
  {
    "objectID": "ch02-cef-blp.html#dummy-variables-the-cef-for-discrete-x",
    "href": "ch02-cef-blp.html#dummy-variables-the-cef-for-discrete-x",
    "title": "2. The CEF and Best Linear Predictor",
    "section": "6 Dummy variables: the CEF for discrete X",
    "text": "6 Dummy variables: the CEF for discrete X\nWhen \\(X\\) is a categorical variable, the CEF is just group means. Regression with dummies recovers these exactly.\n\nset.seed(2026)\nn &lt;- 300\nparty &lt;- sample(c(\"Dem\", \"Rep\", \"Ind\"), n, replace = TRUE,\n                prob = c(0.4, 0.35, 0.25))\napproval &lt;- 50 + 10 * (party == \"Dem\") - 5 * (party == \"Rep\") + rnorm(n, sd = 8)\n\ndf_party &lt;- data.frame(party = factor(party), approval = approval)\n\n# Group means = the CEF\ntapply(df_party$approval, df_party$party, mean)\n\n Dem  Ind  Rep \n59.6 50.7 45.9 \n\n# Regression with dummies (R does this automatically)\ncoef(lm(approval ~ party, data = df_party))\n\n(Intercept)    partyInd    partyRep \n       59.6        -8.9       -13.7 \n\n\nThe intercept is the mean for the baseline group (alphabetically first: Dem), and the coefficients are differences from that baseline."
  },
  {
    "objectID": "ch02-cef-blp.html#interactions-when-the-slope-depends-on-a-group",
    "href": "ch02-cef-blp.html#interactions-when-the-slope-depends-on-a-group",
    "title": "2. The CEF and Best Linear Predictor",
    "section": "7 Interactions: when the slope depends on a group",
    "text": "7 Interactions: when the slope depends on a group\nAn interaction term \\(D_i \\times x_i\\) allows the effect of \\(x\\) to differ by group. This is a richer model than parallel slopes.\n\nset.seed(2026)\nn_int &lt;- 400\ngroup &lt;- sample(0:1, n_int, replace = TRUE)\nx_int &lt;- rnorm(n_int)\n# Group 0: slope = 2, Group 1: slope = -1\ny_int &lt;- 3 + 1.5 * group + 2 * x_int - 3 * group * x_int + rnorm(n_int)\n\ndf_int &lt;- data.frame(x = x_int, y = y_int, group = factor(group))\n\n# Parallel slopes (no interaction)\nmod_parallel &lt;- lm(y ~ group + x, data = df_int)\n\n# Interaction model\nmod_interact &lt;- lm(y ~ group * x, data = df_int)\n\nggplot(df_int, aes(x, y, color = group)) +\n  geom_point(alpha = 0.3, size = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1) +\n  scale_color_manual(values = c(\"steelblue\", \"tomato\"), name = \"Group\") +\n  labs(title = \"Interaction: different slopes for each group\",\n       x = \"X\", y = \"Y\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\ncoef(mod_interact)\n\n(Intercept)      group1           x    group1:x \n       2.91        1.58        1.98       -2.85 \n\n\nThe group1:x coefficient (\\(\\approx -3\\)) is the difference in slopes between group 1 and group 0. The marginal effect of \\(x\\) for group 1 is x + group1:x."
  },
  {
    "objectID": "ch02-cef-blp.html#sec-ovb",
    "href": "ch02-cef-blp.html#sec-ovb",
    "title": "2. The CEF and Best Linear Predictor",
    "section": "8 Omitted variable bias",
    "text": "8 Omitted variable bias\nIf the true model is \\(Y = X_1'\\beta_1 + X_2'\\beta_2 + e\\) but we only regress on \\(X_1\\), we get \\(\\gamma_1 = \\beta_1 + \\Gamma_{12}\\beta_2\\), where \\(\\Gamma_{12}\\) is the regression of \\(X_2\\) on \\(X_1\\). The bias has a clear direction when we know the signs.\n\nset.seed(2026)\nn_ovb &lt;- 1000\n\n# X2 (institutions) is correlated with X1 (democracy)\nx1 &lt;- rnorm(n_ovb)                        # democracy\nx2 &lt;- 0.6 * x1 + rnorm(n_ovb, sd = 0.8)  # institutions (correlated)\ny_ovb &lt;- 1 + 0.5 * x1 + 2 * x2 + rnorm(n_ovb)  # growth\n\n# True model\ncoef(lm(y_ovb ~ x1 + x2))\n\n(Intercept)          x1          x2 \n      0.967       0.511       1.962 \n\n# Short regression (omit x2)\ncoef(lm(y_ovb ~ x1))\n\n(Intercept)          x1 \n      0.942       1.661 \n\n\nThe true effect of democracy (\\(\\beta_1\\)) is 0.5, but the short regression gives \\(\\gamma_1 \\approx 1.7\\). The bias is \\(\\Gamma_{12} \\times \\beta_2 \\approx 0.6 \\times 2 = 1.2\\), inflating the estimate because institutions are positively correlated with democracy and positively affect growth.\n\n# Verify the OVB formula\ngamma_12 &lt;- coef(lm(x2 ~ x1))[2]  # regression of X2 on X1\nbeta_2 &lt;- coef(lm(y_ovb ~ x1 + x2))[3]\n\nc(short_coef = coef(lm(y_ovb ~ x1))[2],\n  long_coef_plus_bias = coef(lm(y_ovb ~ x1 + x2))[2] + gamma_12 * beta_2)\n\n         short_coef.x1 long_coef_plus_bias.x1 \n                  1.66                   1.66 \n\n\nThe OVB formula \\(\\gamma_1 = \\beta_1 + \\Gamma_{12}\\beta_2\\) matches exactly.\n\nTheorem 1 (Omitted Variable Bias) If the true model is \\(Y = X_1'\\beta_1 + X_2'\\beta_2 + e\\) and we regress \\(Y\\) on \\(X_1\\) alone, the short regression coefficient is \\(\\gamma_1 = \\beta_1 + \\Gamma_{12}\\beta_2\\), where \\(\\Gamma_{12}\\) is the coefficient from regressing \\(X_2\\) on \\(X_1\\). The bias \\(\\Gamma_{12}\\beta_2\\) depends on both the correlation between \\(X_1\\) and \\(X_2\\) and the effect of \\(X_2\\) on \\(Y\\).\n\n\n\n\n\n\n\nWarningOVB Is About Population, Not Sample\n\n\n\nThe omitted variable bias formula \\(\\gamma_1 = \\beta_1 + \\Gamma_{12}\\beta_2\\) is an exact population relationship between projection coefficients. It does not require large samples, normality, or any distributional assumption — it follows purely from the algebra of linear projection.\n\n\n\n8.1 Adding variables doesn’t always help\nA subtlety from the lecture: if the true model has three variables and you can’t observe \\(X_3\\), adding \\(X_2\\) can make the bias on \\(\\beta_1\\) worse, depending on the correlation structure.\n\nset.seed(2026)\nn_ow &lt;- 1000\nz1 &lt;- rnorm(n_ow)\nz2 &lt;- 0.3 * z1 + rnorm(n_ow)\nz3 &lt;- -0.8 * z1 + 0.5 * z2 + rnorm(n_ow)\ny_ow &lt;- 1 + 1 * z1 + 0.5 * z2 + 3 * z3 + rnorm(n_ow)\n\n# True beta_1 = 1\nc(true = coef(lm(y_ow ~ z1 + z2 + z3))[2],\n  short = coef(lm(y_ow ~ z1))[2],\n  medium = coef(lm(y_ow ~ z1 + z2))[2])\n\n  true.z1  short.z1 medium.z1 \n    1.015    -0.886    -1.408 \n\n\nWhy does adding \\(X_2\\) make things worse? Apply the OVB formula to the medium regression: omitting \\(X_3\\) biases \\(\\hat\\beta_1\\) by \\(\\Gamma_{13\\cdot 2}\\beta_3\\), where \\(\\Gamma_{13\\cdot 2}\\) is the coefficient on \\(X_1\\) in the regression of \\(X_3\\) on \\((X_1, X_2)\\). Conditioning on \\(X_2\\) can change the partial correlation between \\(X_1\\) and \\(X_3\\), amplifying the bias.\n\n# OVB decomposition for the medium model\ngamma_13_2 &lt;- coef(lm(z3 ~ z1 + z2))[2]  # partial effect of z1 on z3, controlling for z2\nbeta_3 &lt;- coef(lm(y_ow ~ z1 + z2 + z3))[4]\n\nc(medium_bias = coef(lm(y_ow ~ z1 + z2))[2] - 1,\n  ovb_formula = gamma_13_2 * beta_3)\n\nmedium_bias.z1 ovb_formula.z1 \n         -2.41          -2.42 \n\n\nDepending on the correlation structure, the “medium” model (with \\(X_2\\) but not \\(X_3\\)) may be further from the truth than the “short” model. More controls are not automatically better."
  },
  {
    "objectID": "ch02-cef-blp.html#causal-interpretation-regression-as-a-weighted-average",
    "href": "ch02-cef-blp.html#causal-interpretation-regression-as-a-weighted-average",
    "title": "2. The CEF and Best Linear Predictor",
    "section": "9 Causal interpretation: regression as a weighted average",
    "text": "9 Causal interpretation: regression as a weighted average\nThe lecture ends with a deep result: when you regress \\(Y\\) on a treatment dummy \\(D\\) and controls \\(X\\), the regression coefficient is a weighted average of the group-specific treatment effects \\(\\delta_X = \\mathbb{E}[Y|X, D=1] - \\mathbb{E}[Y|X, D=0]\\), with weights proportional to the conditional variance of \\(D\\) given \\(X\\).\n\\[\\delta_R = \\frac{\\mathbb{E}[\\sigma^2_{D|X} \\cdot \\delta_X]}{\\mathbb{E}[\\sigma^2_{D|X}]} \\tag{1}\\]\nGroups where treatment is most variable (closest to a 50/50 split) get the most weight.\n\nset.seed(2026)\nn_cw &lt;- 5000\n\n# X determines both treatment probability and treatment effect\nx_cw &lt;- runif(n_cw, 0, 1)\nprob_treat &lt;- x_cw^2  # treatment rare for low X, common for high X\nD &lt;- rbinom(n_cw, 1, prob_treat)\n\n# Heterogeneous treatment effect: large for low X, negative for high X\ndelta_x &lt;- 10 - 16 * x_cw   # ranges from +10 at X=0 to -6 at X=1\ny_cw &lt;- 2 + 3 * x_cw + delta_x * D + rnorm(n_cw)\n\n# Regression coefficient\ndelta_R &lt;- coef(lm(y_cw ~ D + x_cw))[2]\n\n# Weighted average with weights = Var(D|X) = p(1-p)\nwt &lt;- prob_treat * (1 - prob_treat)\ndelta_weighted &lt;- sum(wt * delta_x) / sum(wt)\n\nc(regression = delta_R, weighted_avg = delta_weighted,\n  unweighted_avg = mean(delta_x))\n\n  regression.D   weighted_avg unweighted_avg \n       -0.4937         0.0374         2.0417 \n\n\nThe unweighted average treatment effect is about 2, but the regression coefficient is near 0. That’s because treatment assignment \\(p(X) = X^2\\) is most variable around \\(X \\approx 0.7\\) (where \\(p(1-p)\\) peaks), and at that point the treatment effect \\(\\delta(X) = 10 - 16(0.7) \\approx -1\\) is near zero or negative. The regression upweights these observations and downweights the large positive effects at low \\(X\\), where treatment is rare and \\(\\text{Var}(D|X)\\) is small.\n\ndf_cw &lt;- data.frame(x = x_cw, delta = delta_x, weight = wt)\n\nggplot(df_cw, aes(x)) +\n  geom_line(aes(y = delta, color = \"Treatment effect δ(X)\"), linewidth = 1.2) +\n  geom_line(aes(y = weight * 40, color = \"Weight: Var(D|X) (rescaled)\"),\n            linewidth = 1.2) +\n  scale_y_continuous(\n    name = \"Treatment effect δ(X)\",\n    sec.axis = sec_axis(~ . / 40, name = \"Var(D|X)\")\n  ) +\n  scale_color_manual(values = c(\"Treatment effect δ(X)\" = \"tomato\",\n                                \"Weight: Var(D|X) (rescaled)\" = \"steelblue\"),\n                     name = \"\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  labs(x = \"X\", title = \"Regression weights favor where treatment is most variable\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis is why a randomized experiment with \\(p = 0.5\\) for everyone gives equal weight to all subgroups: when \\(\\text{Var}(D|X)\\) is constant, the variance-weighted average reduces to the simple average."
  },
  {
    "objectID": "ch02-cef-blp.html#summary",
    "href": "ch02-cef-blp.html#summary",
    "title": "2. The CEF and Best Linear Predictor",
    "section": "10 Summary",
    "text": "10 Summary\n\nThe CEF \\(m(X) = \\mathbb{E}[Y|X]\\) is the best predictor, but the CEF error can be heteroskedastic (mean independence \\(\\neq\\) independence).\nThe BLP \\(X'\\beta\\) is the best linear approximation. It equals the CEF when the CEF is linear (e.g., jointly normal data).\nBLP residuals satisfy \\(\\mathbb{E}[Xe] = 0\\) (weaker than \\(\\mathbb{E}[e|X] = 0\\)), so nonlinear patterns can remain.\nDummy variables make the CEF discrete: the regression recovers group means.\nInteractions allow slopes to vary by group or by the value of a moderator.\nOmitted variable bias has a precise formula: \\(\\gamma_1 = \\beta_1 + \\Gamma_{12}\\beta_2\\). Adding controls doesn’t always reduce bias.\nCausal interpretation: regression weights treatment effects by the conditional variance of treatment assignment.\n\nNext: Multivariate OLS — moving from population quantities to sample estimation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estimation I: Computational Companion",
    "section": "",
    "text": "This site is a computational companion to the lecture slides for Estimation I (PLSC 30700), taught by Robert Gulotty at the University of Chicago.\nEach chapter mirrors a lecture topic and demonstrates the key ideas in R. The goal is not to replace the slides or the textbook, but to give you a parallel way to build intuition: by computing, simulating, and visualizing the concepts."
  },
  {
    "objectID": "index.html#how-to-use-this-site",
    "href": "index.html#how-to-use-this-site",
    "title": "Estimation I: Computational Companion",
    "section": "1 How to use this site",
    "text": "1 How to use this site\n\nRead alongside the lectures. Each chapter covers the same material as the corresponding lecture, but tells the story through code.\nRun the code yourself. Copy-paste into R or RStudio. Modify the examples. Break things.\nFocus on understanding, not memorization. The code is a tool for building intuition about what the math is doing."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Estimation I: Computational Companion",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\nYou will need R installed. The following packages are used throughout:\ninstall.packages(c(\"ggplot2\", \"MASS\", \"car\", \"carData\", \"sensemakr\",\n                   \"sandwich\", \"lmtest\", \"estimatr\", \"haven\", \"plm\", \"boot\",\n                   \"AER\", \"gmm\", \"panelView\", \"fixest\", \"did\", \"fect\"))"
  },
  {
    "objectID": "index.html#chapters",
    "href": "index.html#chapters",
    "title": "Estimation I: Computational Companion",
    "section": "3 Chapters",
    "text": "3 Chapters\n\n\n\nWeek\nChapter\nTopic\n\n\n\n\n1\n1. Probability and Linear Algebra\nExpectation, variance, the CEF, matrix algebra, and projection\n\n\n1\n2. The CEF and Best Linear Predictor\nWhy regression approximates the CEF\n\n\n2\n3. Multivariate OLS\nDeriving and computing the OLS estimator\n\n\n2\n4. Sensitivity and Leverage\nFrisch-Waugh-Lovell, partial R², influential observations\n\n\n3\n5. Efficiency and GLS\nWLS, feasible GLS, method of moments\n\n\n4\n6. Small Sample Inference\nLikelihood, the normal linear model, t- and F-tests\n\n\n5\n7. Probit and MLE\nBinary outcomes, maximum likelihood estimation\n\n\n5–6\n8. Asymptotics\nLarge-sample theory, the delta method, and the bootstrap\n\n\n6\n9. Hypothesis Testing\nF-test, test trinity, multiple testing, power\n\n\n7\n10. Instrumental Variables and 2SLS\nEndogeneity, IV estimation, 2SLS, weak instruments, LATE\n\n\n8\n11. GMM\nMoment conditions, efficient weighting, J-test, missing data\n\n\n9\n12. Panel Data\npanelView, pooled/between/within, DiD, clustering, Arellano–Bond GMM\n\n\n9\n13. Fixed Effects and Modern DiD\nRE, Hausman test, CRE, Callaway–Sant’Anna, fect, matrix completion"
  },
  {
    "objectID": "ch11-gmm.html",
    "href": "ch11-gmm.html",
    "title": "11. GMM",
    "section": "",
    "text": "This chapter demonstrates the Generalized Method of Moments (GMM) framework computationally. We show that OLS, IV, and 2SLS are all special cases, then build up to efficient GMM estimation, the J-test, and a missing data application. The key reference is Hansen, Chapter 13.\nQuestions this chapter answers:\nlibrary(ggplot2)\nlibrary(gmm)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(estimatr)\nlibrary(MASS)"
  },
  {
    "objectID": "ch11-gmm.html#everything-is-a-moment-condition",
    "href": "ch11-gmm.html#everything-is-a-moment-condition",
    "title": "11. GMM",
    "section": "1 Everything is a moment condition",
    "text": "1 Everything is a moment condition\nEvery estimator we have studied solves a set of moment conditions \\(\\mathbb{E}[g_i(\\beta)] = 0\\). The sample analog sets \\(\\bar{g}_n(\\hat\\beta) = \\frac{1}{n}\\sum g_i(\\hat\\beta) = 0\\).\n\n\n\nEstimator\nMoment condition\nSolution\n\n\n\n\nOLS\n\\(\\mathbb{E}[X_i(Y_i - X_i'\\beta)] = 0\\)\nthe OLS estimator\n\n\nIV\n\\(\\mathbb{E}[Z_i(Y_i - X_i'\\beta)] = 0\\)\n\\((Z'X)^{-1}Z'Y\\)\n\n\n2SLS\nSame as IV, with \\(W = (Z'Z)^{-1}\\)\n\\((X'P_Z X)^{-1}X'P_Z Y\\)\n\n\n\n\nDefinition 1 (GMM Estimator (Hansen 13.1)) The GMM estimator minimizes \\(J(\\beta) = n\\,\\bar{g}_n(\\beta)' W\\, \\bar{g}_n(\\beta)\\), where \\(\\bar{g}_n(\\beta) = \\frac{1}{n}\\sum g_i(\\beta)\\) and \\(W\\) is a positive definite weight matrix. Different choices of \\(W\\) yield different estimators; the optimal \\(W = \\Omega^{-1}\\) minimizes asymptotic variance.\n\n\n\n\n\n\n\nNoteGMM Unifies Everything\n\n\n\nEvery estimator in this course — OLS, GLS, IV, 2SLS, probit MLE — is a GMM estimator with specific moment conditions and weight matrices. The GMM framework provides a single set of tools for estimation, inference, and specification testing.\n\n\nLet’s verify this computationally. We simulate an IV model with one endogenous regressor and one instrument:\n\nset.seed(42)\nn &lt;- 500\n\n# DGP: Y = 1 + 0.5*X + e, where X is endogenous\nu &lt;- rnorm(n)\nZ &lt;- rnorm(n)\nX &lt;- 0.8 * Z + u          # X correlated with u\ne &lt;- 0.6 * u + rnorm(n)   # e correlated with u through shared u\nY &lt;- 1 + 0.5 * X + e\n\nOLS is biased because \\(\\text{Cov}(X, e) \\neq 0\\):\n\n# OLS: E[X(Y - Xb)] = 0\nbeta_ols &lt;- solve(t(cbind(1, X)) %*% cbind(1, X)) %*% t(cbind(1, X)) %*% Y\ncat(\"OLS slope:\", round(beta_ols[2], 4), \" (biased, true = 0.5)\\n\")\n\nOLS slope: 0.8725  (biased, true = 0.5)\n\n\nIV uses the instrument \\(Z\\) to form moment conditions \\(\\mathbb{E}[Z(Y - X\\beta)] = 0\\):\n\n# IV: E[Z(Y - Xb)] = 0, just-identified (ell = k)\nXmat &lt;- cbind(1, X)\nZmat &lt;- cbind(1, Z)\nbeta_iv &lt;- solve(t(Zmat) %*% Xmat) %*% t(Zmat) %*% Y\ncat(\"IV slope:\", round(beta_iv[2], 4), \" (consistent)\\n\")\n\nIV slope: 0.4949  (consistent)\n\n\nWhen the number of moment conditions \\(\\ell\\) equals the number of parameters \\(k\\), we can solve exactly. But what happens when \\(\\ell &gt; k\\)?"
  },
  {
    "objectID": "ch11-gmm.html#the-overidentification-problem",
    "href": "ch11-gmm.html#the-overidentification-problem",
    "title": "11. GMM",
    "section": "2 The overidentification problem",
    "text": "2 The overidentification problem\nWith more instruments than parameters, no \\(\\beta\\) can simultaneously zero out all moment conditions. We need a principled way to get as close as possible.\n\n# Now add a second instrument\nZ2 &lt;- 0.5 * X + rnorm(n, sd = 2)  # second instrument (weaker)\nZmat2 &lt;- cbind(1, Z, Z2)           # 3 instruments, 2 parameters\n\n# Can't solve Z'(Y - Xb) = 0 exactly (3 equations, 2 unknowns)\n# Check: the IV formula with the full Z matrix is not square\ncat(\"Z'X dimensions:\", nrow(t(Zmat2) %*% Xmat), \"x\", ncol(t(Zmat2) %*% Xmat), \"\\n\")\n\nZ'X dimensions: 3 x 2 \n\ncat(\"(not square -- can't invert directly)\\n\")\n\n(not square -- can't invert directly)\n\n\n\n2.1 The GMM criterion function\nFor a positive definite weight matrix \\(W\\), the GMM criterion is:\n\\[J(\\beta) = n\\,\\bar{g}_n(\\beta)' W\\, \\bar{g}_n(\\beta) \\tag{1}\\]\nThe GMM estimator minimizes this weighted quadratic form.\n\n# GMM criterion function for the linear IV model\ngmm_criterion &lt;- function(beta, Y, X, Z, W) {\n  g_bar &lt;- colMeans(Z * as.numeric(Y - X %*% beta))  # sample moments\n  n &lt;- nrow(X)\n  n * t(g_bar) %*% W %*% g_bar\n}\n\n# Closed-form GMM estimator: (X'Z W Z'X)^{-1} (X'Z W Z'Y)\ngmm_linear &lt;- function(Y, X, Z, W) {\n  XZ &lt;- t(X) %*% Z\n  solve(XZ %*% W %*% t(XZ)) %*% XZ %*% W %*% t(Z) %*% Y\n}\n\nDifferent weight matrices yield different estimators:\n\n# Identity matrix\nW_ident &lt;- diag(3)\nbeta_gmm_ident &lt;- gmm_linear(Y, Xmat, Zmat2, W_ident)\n\n# (Z'Z)^{-1} -- this gives 2SLS\nW_2sls &lt;- solve(t(Zmat2) %*% Zmat2)\nbeta_gmm_2sls &lt;- gmm_linear(Y, Xmat, Zmat2, W_2sls)\n\n# Verify 2SLS matches\nPZ &lt;- Zmat2 %*% solve(t(Zmat2) %*% Zmat2) %*% t(Zmat2)\nbeta_2sls &lt;- solve(t(Xmat) %*% PZ %*% Xmat) %*% t(Xmat) %*% PZ %*% Y\n\ncat(\"GMM (W = I):     slope =\", round(beta_gmm_ident[2], 4), \"\\n\")\n\nGMM (W = I):     slope = 0.6327 \n\ncat(\"GMM (W = 2SLS):  slope =\", round(beta_gmm_2sls[2], 4), \"\\n\")\n\nGMM (W = 2SLS):  slope = 0.5307 \n\ncat(\"2SLS directly:   slope =\", round(beta_2sls[2], 4), \"\\n\")\n\n2SLS directly:   slope = 0.5307 \n\n\n\nTheorem 1 (2SLS as GMM (Hansen 13.2)) When \\(W = (Z'Z/n)^{-1}\\), the GMM estimator for the linear IV model reduces to 2SLS: \\(\\hat\\beta_{2SLS} = (X'P_Z X)^{-1}X'P_Z Y\\). Under homoskedasticity, 2SLS is efficient among all GMM estimators.\n\nThe choice of \\(W\\) matters when the model is overidentified—but the optimal choice is \\(W = \\Omega^{-1}\\), where \\(\\Omega = \\mathbb{E}[Z_i Z_i' e_i^2]\\) is the variance of the moment conditions."
  },
  {
    "objectID": "ch11-gmm.html#sec-efficient-gmm",
    "href": "ch11-gmm.html#sec-efficient-gmm",
    "title": "11. GMM",
    "section": "3 Efficient GMM by hand",
    "text": "3 Efficient GMM by hand\nThe optimal weight matrix \\(W = \\Omega^{-1}\\) minimizes the asymptotic variance of the GMM estimator. The two-step procedure is:\n\nTheorem 2 (Efficient GMM (Hansen 13.4-13.5)) The efficient GMM estimator uses \\(W = \\hat\\Omega^{-1}\\), where \\(\\hat\\Omega = \\frac{1}{n}\\sum g_i(\\tilde\\beta)g_i(\\tilde\\beta)'\\) and \\(\\tilde\\beta\\) is a preliminary consistent estimator. This achieves the semiparametric efficiency bound among all estimators using the same moment conditions.\n\n\nEstimate \\(\\beta\\) by 2SLS (using \\(W = (Z'Z)^{-1}\\)). Compute residuals \\(\\tilde{e}_i\\).\nEstimate \\(\\hat\\Omega = \\frac{1}{n}\\sum Z_i Z_i' \\tilde{e}_i^2\\), set \\(\\hat{W} = \\hat\\Omega^{-1}\\), and re-estimate.\n\n\n# Step 1: 2SLS\nbeta_step1 &lt;- beta_2sls\ne_step1 &lt;- Y - Xmat %*% beta_step1\n\n# Step 2: Estimate optimal weight matrix\nOmega_hat &lt;- (t(Zmat2) %*% diag(as.numeric(e_step1^2)) %*% Zmat2) / n\nW_opt &lt;- solve(Omega_hat)\n\n# Re-estimate with optimal W\nbeta_step2 &lt;- gmm_linear(Y, Xmat, Zmat2, W_opt)\n\ncat(\"Step 1 (2SLS) slope:     \", round(beta_step1[2], 4), \"\\n\")\n\nStep 1 (2SLS) slope:      0.5307 \n\ncat(\"Step 2 (efficient) slope:\", round(beta_step2[2], 4), \"\\n\")\n\nStep 2 (efficient) slope: 0.529 \n\n\nWe can iterate—updating \\(\\hat\\Omega\\) and re-estimating—until convergence:\n\n# Iterated GMM\nbeta_iter &lt;- beta_2sls\nfor (s in 1:20) {\n  e_s &lt;- Y - Xmat %*% beta_iter\n  Omega_s &lt;- (t(Zmat2) %*% diag(as.numeric(e_s^2)) %*% Zmat2) / n\n  W_s &lt;- solve(Omega_s)\n  beta_new &lt;- gmm_linear(Y, Xmat, Zmat2, W_s)\n  if (max(abs(beta_new - beta_iter)) &lt; 1e-8) {\n    cat(\"Converged in\", s, \"iterations\\n\")\n    break\n  }\n  beta_iter &lt;- beta_new\n}\n\nConverged in 5 iterations\n\ncat(\"Iterated GMM slope:\", round(beta_iter[2], 4), \"\\n\")\n\nIterated GMM slope: 0.529"
  },
  {
    "objectID": "ch11-gmm.html#using-the-gmm-package",
    "href": "ch11-gmm.html#using-the-gmm-package",
    "title": "11. GMM",
    "section": "4 Using the gmm package",
    "text": "4 Using the gmm package\nThe gmm package handles moment function specification, weighting, and inference. For a linear IV model, we can pass the formula directly:\n\n# Linear IV via gmm()\n# Formula: Y ~ X | Z  (endogenous ~ instruments)\ndat &lt;- data.frame(Y = Y, X = X, Z1 = Z, Z2 = Z2)\n\n# Two-step GMM\nfit_2step &lt;- gmm(Y ~ X, ~ Z1 + Z2, data = dat, type = \"twoStep\")\nsummary(fit_2step)\n\n\nCall:\ngmm(g = Y ~ X, x = ~Z1 + Z2, type = \"twoStep\", data = dat)\n\n\nMethod:  twoStep \n\nKernel:  Quadratic Spectral(with bw =  0.57799 )\n\nCoefficients:\n             Estimate    Std. Error  t value     Pr(&gt;|t|)  \n(Intercept)  9.4738e-01  4.6839e-02  2.0226e+01  5.7701e-91\nX            5.3688e-01  5.7361e-02  9.3595e+00  8.0104e-21\n\nJ-Test: degrees of freedom is 1 \n                J-test    P-value \nTest E(g)=0:    4.620103  0.031599\n\nInitial values of the coefficients\n(Intercept)           X \n  0.9419577   0.5306796 \n\n\nCompare to the iterative version:\n\nfit_iter &lt;- gmm(Y ~ X, ~ Z1 + Z2, data = dat, type = \"iterative\")\ncat(\"Two-step slope:\", round(coef(fit_2step)[\"X\"], 4), \"\\n\")\n\nTwo-step slope: 0.5369 \n\ncat(\"Iterated slope:\", round(coef(fit_iter)[\"X\"], 4), \"\\n\")\n\nIterated slope: 0.5369 \n\n\nAnd verify against iv_robust (which computes 2SLS):\n\nfit_iv &lt;- iv_robust(Y ~ X | Z1 + Z2, data = dat)\ncat(\"2SLS (iv_robust):\", round(coef(fit_iv)[\"X\"], 4), \"\\n\")\n\n2SLS (iv_robust): 0.5307 \n\ncat(\"Two-step GMM:    \", round(coef(fit_2step)[\"X\"], 4), \"\\n\")\n\nTwo-step GMM:     0.5369 \n\n\nUnder homoskedasticity, 2SLS and efficient GMM give the same estimates (Hansen Thm 13.6). The differences here arise because we simulated heteroskedastic-style errors."
  },
  {
    "objectID": "ch11-gmm.html#when-does-efficient-gmm-help-a-simulation",
    "href": "ch11-gmm.html#when-does-efficient-gmm-help-a-simulation",
    "title": "11. GMM",
    "section": "5 When does efficient GMM help? A simulation",
    "text": "5 When does efficient GMM help? A simulation\n2SLS is efficient only under homoskedasticity. Under heteroskedasticity, efficient GMM can reduce variance. Let’s demonstrate with a Monte Carlo:\n\nset.seed(99)\nB &lt;- 500\nn_sim &lt;- 300\nbeta_true &lt;- 0.5\nresults &lt;- data.frame(method = character(), estimate = numeric())\n\nfor (b in 1:B) {\n  u &lt;- rnorm(n_sim)\n  z1 &lt;- rnorm(n_sim)\n  z2 &lt;- rnorm(n_sim)\n  x &lt;- 0.6 * z1 + 0.3 * z2 + u\n\n  # Heteroskedastic errors: variance depends on z1\n  sigma_i &lt;- 0.5 + abs(z1)\n  e &lt;- sigma_i * rnorm(n_sim) + 0.5 * u\n  y &lt;- 1 + beta_true * x + e\n\n  d &lt;- data.frame(y = y, x = x, z1 = z1, z2 = z2)\n\n  # 2SLS\n  iv_fit &lt;- tryCatch(\n    iv_robust(y ~ x | z1 + z2, data = d, se_type = \"classical\"),\n    error = function(e) NULL\n  )\n\n  # Two-step GMM\n  gmm_fit &lt;- tryCatch(\n    gmm(y ~ x, ~ z1 + z2, data = d, type = \"twoStep\"),\n    error = function(e) NULL\n  )\n\n  if (!is.null(iv_fit) && !is.null(gmm_fit)) {\n    results &lt;- rbind(results,\n      data.frame(method = \"2SLS\", estimate = coef(iv_fit)[\"x\"]),\n      data.frame(method = \"Efficient GMM\", estimate = coef(gmm_fit)[\"x\"])\n    )\n  }\n}\n\n# Compare variances\nvar_table &lt;- aggregate(estimate ~ method, data = results,\n                       FUN = function(x) c(mean = mean(x), var = var(x)))\nvar_table &lt;- cbind(var_table[1], as.data.frame(var_table[[2]]))\nnames(var_table) &lt;- c(\"method\", \"mean\", \"variance\")\nvar_table$efficiency_ratio &lt;- var_table$variance / min(var_table$variance)\nprint(var_table, digits = 4)\n\n         method   mean variance efficiency_ratio\n1          2SLS 0.5025  0.03440            1.067\n2 Efficient GMM 0.5000  0.03224            1.000\n\n\n\nggplot(results, aes(x = estimate, fill = method)) +\n  geom_density(alpha = 0.4) +\n  geom_vline(xintercept = beta_true, linetype = \"dashed\") +\n  labs(title = \"2SLS vs efficient GMM under heteroskedasticity\",\n       subtitle = paste0(\"n = \", n_sim, \", \", B, \" replications\"),\n       x = expression(hat(beta)), y = \"Density\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"steelblue\", \"coral\"))\n\n\n\n\n2SLS vs. efficient GMM under heteroskedasticity\n\n\n\n\nUnder heteroskedasticity, efficient GMM has lower variance because it weights moment conditions by the inverse of their variance—upweighting informative, low-noise moments and downweighting noisy ones. Just as the Gauss-Markov theorem bounds OLS among linear unbiased estimators, the efficient GMM achieves the semiparametric bound among all estimators using the same moment conditions."
  },
  {
    "objectID": "ch11-gmm.html#sec-j-test",
    "href": "ch11-gmm.html#sec-j-test",
    "title": "11. GMM",
    "section": "6 The J-test for overidentification",
    "text": "6 The J-test for overidentification\nWhen \\(\\ell &gt; k\\) (more moment conditions than parameters), the model imposes testable restrictions. If the moment conditions are correctly specified, the minimized criterion value should be small.\n\\[J = n\\,\\bar{g}_n(\\hat\\beta)'\\hat\\Omega^{-1}\\bar{g}_n(\\hat\\beta) \\xrightarrow{d} \\chi^2_{\\ell - k} \\tag{2}\\]\n\nTheorem 3 (J-Test for Overidentification (Hansen 13.14)) Under correct specification, the minimized GMM criterion \\(J = n\\,\\bar{g}_n(\\hat\\beta)'\\hat\\Omega^{-1}\\bar{g}_n(\\hat\\beta) \\xrightarrow{d} \\chi^2_{\\ell - k}\\), where \\(\\ell\\) is the number of moment conditions and \\(k\\) is the number of parameters. Rejection indicates that the moment conditions are mutually inconsistent.\n\n\n# J-test from our earlier fit (valid instruments)\nspecTest(fit_2step)\n\n\n ##  J-Test: degrees of freedom is 1  ## \n\n                J-test    P-value \nTest E(g)=0:    4.620103  0.031599\n\n\nHere \\(\\ell - k = 3 - 2 = 1\\) degree of freedom. A large p-value means we fail to reject: the overidentifying restrictions are consistent with the data.\n\n6.1 When the J-test rejects\nIf an instrument is invalid (correlated with the error), the J-test should detect this:\n\nset.seed(123)\nn &lt;- 500\nu &lt;- rnorm(n)\nz1 &lt;- rnorm(n)\nz2_bad &lt;- rnorm(n)\nx &lt;- 0.6 * z1 + 0.3 * z2_bad + u\ne &lt;- 0.5 * u + 0.4 * z2_bad + rnorm(n)  # z2_bad enters the error!\ny &lt;- 1 + 0.5 * x + e\n\nd_bad &lt;- data.frame(y = y, x = x, z1 = z1, z2 = z2_bad)\nfit_bad &lt;- gmm(y ~ x, ~ z1 + z2, data = d_bad, type = \"twoStep\")\nspecTest(fit_bad)\n\n\n ##  J-Test: degrees of freedom is 1  ## \n\n                J-test      P-value   \nTest E(g)=0:    1.4055e+01  1.7758e-04\n\n\nThe J-test has a limitation: if all instruments are invalid in the same direction, the test may not detect the problem. It tests whether the instruments disagree with each other, not whether any individual instrument is valid.\n\n\n\n\n\n\nWarningJ-Test Limitations\n\n\n\nThe J-test only detects when instruments disagree with each other. If all instruments are invalid in the same direction, the test has no power. It tests internal consistency of the overidentifying restrictions, not the validity of any individual instrument.\n\n\n\n\n6.2 J-test rejection rates by simulation\nLet’s verify the J-test has correct size (under valid instruments) and power (under an invalid instrument):\n\nset.seed(42)\nB &lt;- 500\nn_sim &lt;- 300\nrejections &lt;- data.frame(scenario = character(), rejected = logical())\n\nfor (scenario in c(\"valid\", \"invalid\")) {\n  for (b in 1:B) {\n    u &lt;- rnorm(n_sim)\n    z1 &lt;- rnorm(n_sim)\n    z2 &lt;- rnorm(n_sim)\n    x &lt;- 0.6 * z1 + 0.3 * z2 + u\n\n    if (scenario == \"valid\") {\n      e &lt;- 0.5 * u + rnorm(n_sim)\n    } else {\n      e &lt;- 0.5 * u + 0.3 * z2 + rnorm(n_sim)  # z2 in error\n    }\n    y &lt;- 1 + 0.5 * x + e\n\n    d &lt;- data.frame(y = y, x = x, z1 = z1, z2 = z2)\n    fit &lt;- tryCatch(gmm(y ~ x, ~ z1 + z2, data = d, type = \"twoStep\"),\n                    error = function(e) NULL)\n    if (!is.null(fit)) {\n      jtest &lt;- tryCatch(specTest(fit), error = function(e) NULL)\n      if (!is.null(jtest)) {\n        pval &lt;- jtest$test[1, 2]  # p-value\n        rejections &lt;- rbind(rejections,\n          data.frame(scenario = scenario, rejected = pval &lt; 0.05))\n      }\n    }\n  }\n}\n\nrej_rates &lt;- aggregate(rejected ~ scenario, data = rejections, mean)\nnames(rej_rates)[2] &lt;- \"rejection_rate\"\nrej_rates\n\n  scenario rejection_rate\n1  invalid          0.998\n2    valid          0.068\n\n\nUnder valid instruments, the rejection rate should be near 0.05 (the nominal size). Under the invalid instrument, the rejection rate should be substantially higher."
  },
  {
    "objectID": "ch11-gmm.html#nonlinear-gmm-custom-moment-functions",
    "href": "ch11-gmm.html#nonlinear-gmm-custom-moment-functions",
    "title": "11. GMM",
    "section": "7 Nonlinear GMM: custom moment functions",
    "text": "7 Nonlinear GMM: custom moment functions\nThe gmm package also accepts user-defined moment functions for nonlinear models. The moment function takes parameters \\(\\theta\\) and data \\(x\\), and returns an \\(n \\times \\ell\\) matrix of moment contributions.\nHere’s a simple example: estimating the mean and variance of a distribution simultaneously using moment conditions \\(\\mathbb{E}[Y - \\mu] = 0\\) and \\(\\mathbb{E}[(Y - \\mu)^2 - \\sigma^2] = 0\\):\n\n# Moment function: E[Y - mu] = 0 and E[(Y - mu)^2 - sigma2] = 0\ng_meanvar &lt;- function(theta, x) {\n  mu &lt;- theta[1]\n  sigma2 &lt;- theta[2]\n  m1 &lt;- x - mu\n  m2 &lt;- (x - mu)^2 - sigma2\n  cbind(m1, m2)  # n x 2 matrix\n}\n\n# Generate data\nset.seed(1)\ny_data &lt;- rnorm(200, mean = 3, sd = 2)\n\n# GMM estimation (just-identified: 2 moments, 2 parameters)\nfit_mv &lt;- gmm(g_meanvar, x = y_data, t0 = c(0, 1))\ncat(\"GMM estimates:  mu =\", round(coef(fit_mv)[1], 3),\n    \" sigma2 =\", round(coef(fit_mv)[2], 3), \"\\n\")\n\nGMM estimates:  mu = 3.071  sigma2 = 3.436 \n\ncat(\"Sample moments: mean =\", round(mean(y_data), 3),\n    \" var =\", round(var(y_data) * (200-1)/200, 3), \"\\n\")\n\nSample moments: mean = 3.071  var = 3.436 \n\n\nThe GMM estimates match the sample moments exactly because this is a just-identified model."
  },
  {
    "objectID": "ch11-gmm.html#application-missing-data-and-gmm",
    "href": "ch11-gmm.html#application-missing-data-and-gmm",
    "title": "11. GMM",
    "section": "8 Application: missing data and GMM",
    "text": "8 Application: missing data and GMM\nThis section implements the Abrevaya & Donald (2017) approach to missing data using GMM. The idea is to exploit moment conditions from both complete and incomplete observations.\n\n8.1 The setup\nSuppose we want to estimate: \\[y_i = \\beta_0 + \\alpha \\cdot x_i + \\beta_1 \\cdot z_i + \\varepsilon_i\\]\nwhere \\(x_i\\) is sometimes missing but \\(z_i\\) is always observed. If \\(x_i\\) has a linear projection on \\(z_i\\): \\[x_i = \\gamma_0 + \\gamma_1 \\cdot z_i + \\xi_i\\]\nthen for observations where \\(x_i\\) is missing, we can substitute: \\[y_i = (\\beta_0 + \\alpha\\gamma_0) + (\\beta_1 + \\alpha\\gamma_1) z_i + (\\varepsilon_i + \\alpha\\xi_i)\\]\nThis gives us three blocks of moment conditions—and more conditions than parameters (overidentification).\n\n\n8.2 Simulating the WLS-like data\nWe simulate data similar to the Wisconsin Longitudinal Study, where education (\\(y\\)) depends on a BMI-related rating (\\(x\\), sometimes missing) and IQ (\\(z\\), always observed):\n\nset.seed(314)\nn_wls &lt;- 5000\n\n# Parameters\nbeta_0_true &lt;- 12\nalpha_true &lt;- -0.04\nbeta_iq_true &lt;- 0.06\ngamma_0_true &lt;- 3.5\ngamma_iq_true &lt;- 0.005\n\n# Generate data\niq &lt;- rnorm(n_wls, mean = 100, sd = 15)\nxi &lt;- rnorm(n_wls, sd = 1)\nbmi &lt;- gamma_0_true + gamma_iq_true * iq + xi\neps &lt;- rnorm(n_wls, sd = 1.5)\neduc &lt;- beta_0_true + alpha_true * bmi + beta_iq_true * iq + eps\n\n# Missingness: depends on iq (MAR) but not on eps or xi\nprob_missing &lt;- plogis(-2 + 0.01 * iq)  # ~20% missing overall\nmissing &lt;- rbinom(n_wls, 1, prob_missing)\ncat(\"Fraction missing:\", round(mean(missing), 3), \"\\n\")\n\nFraction missing: 0.273 \n\n# Set bmi to 0 for missing observations (placeholder)\nbmi_obs &lt;- ifelse(missing == 0, bmi, 0)\n\nwls &lt;- data.frame(educ = educ, bmi = bmi_obs, iq = iq, bmimissing = missing)\n\n\n\n8.3 Complete case and dummy variable approaches\n\n# Complete case: drop missing observations\ncc_fit &lt;- lm(educ ~ bmi + iq, data = subset(wls, bmimissing == 0))\n\n# Dummy variable: fill in 0, add missing indicator\ndv_fit &lt;- lm_robust(educ ~ bmi + iq + bmimissing, data = wls)\n\ncat(\"Complete case:   alpha =\", round(coef(cc_fit)[\"bmi\"], 4),\n    \" SE =\", round(summary(cc_fit)$coefficients[\"bmi\", \"Std. Error\"], 4), \"\\n\")\n\nComplete case:   alpha = -0.0573  SE = 0.0248 \n\ncat(\"Dummy variable:  alpha =\", round(coef(dv_fit)[\"bmi\"], 4),\n    \" SE =\", round(dv_fit$std.error[\"bmi\"], 4), \"\\n\")\n\nDummy variable:  alpha = -0.0573  SE = 0.0249 \n\ncat(\"True alpha:     \", alpha_true, \"\\n\")\n\nTrue alpha:      -0.04 \n\n\n\n\n8.4 GMM with three blocks of moment conditions\nNow we implement the Abrevaya & Donald GMM estimator. The five parameters are \\(\\theta = (\\beta_0, \\alpha, \\beta_{iq}, \\gamma_0, \\gamma_{iq})\\) and we have seven moment conditions:\n\n# Moment function: 7 conditions for 5 parameters\ng_missing &lt;- function(theta, x) {\n  beta_0  &lt;- theta[1]\n  alpha   &lt;- theta[2]\n  beta_iq &lt;- theta[3]\n  gamma_0 &lt;- theta[4]\n  gamma_iq &lt;- theta[5]\n\n  educ &lt;- x[, 1]\n  bmi  &lt;- x[, 2]\n  iq   &lt;- x[, 3]\n  m    &lt;- x[, 4]  # missing indicator\n\n  # Block 1: structural equation, observed cases only\n  resid1 &lt;- (1 - m) * (educ - beta_0 - alpha * bmi - beta_iq * iq)\n\n  # Block 2: projection equation, observed cases only\n  resid2 &lt;- (1 - m) * (bmi - gamma_0 - gamma_iq * iq)\n\n  # Block 3: substituted equation, missing cases only\n  resid3 &lt;- m * (educ - (gamma_0 * alpha + beta_0) - (gamma_iq * alpha + beta_iq) * iq)\n\n  cbind(\n    resid1,              # m1: E[(1-m)(y - b0 - a*x - b1*z)] = 0\n    resid1 * bmi,        # m2: E[(1-m)(y - ...)*x] = 0\n    resid1 * iq,         # m3: E[(1-m)(y - ...)*z] = 0\n    resid2,              # m4: E[(1-m)(x - g0 - g1*z)] = 0\n    resid2 * iq,         # m5: E[(1-m)(x - ...)*z] = 0\n    resid3,              # m6: E[m(y - delta'z)] = 0\n    resid3 * iq          # m7: E[m(y - delta'z)*z] = 0\n  )\n}\n\nThe instruments for each block multiply the residuals: Block 1 uses \\((1, x, z)\\), Block 2 uses \\((1, z)\\), Block 3 uses \\((1, z)\\). Total: \\(3 + 2 + 2 = 7\\) moment conditions for 5 parameters, giving us 2 overidentifying restrictions.\n\n\n8.5 Starting values and estimation\nGood starting values help the optimizer. We use complete-case regressions:\n\n# Starting values from complete cases\ncc_reg &lt;- lm(educ ~ bmi + iq, data = subset(wls, bmimissing == 0))\nproj_reg &lt;- lm(bmi ~ iq, data = subset(wls, bmimissing == 0))\n\nstart_vals &lt;- c(\n  beta_0   = unname(coef(cc_reg)[\"(Intercept)\"]),\n  alpha    = unname(coef(cc_reg)[\"bmi\"]),\n  beta_iq  = unname(coef(cc_reg)[\"iq\"]),\n  gamma_0  = unname(coef(proj_reg)[\"(Intercept)\"]),\n  gamma_iq = unname(coef(proj_reg)[\"iq\"])\n)\n\n\n# Data matrix\nx_mat &lt;- as.matrix(wls[, c(\"educ\", \"bmi\", \"iq\", \"bmimissing\")])\n\n# Two-step GMM\ngmm_fit &lt;- gmm(\n  g_missing,\n  x = x_mat,\n  t0 = start_vals,\n  type = \"twoStep\",\n  wmatrix = \"ident\",\n  vcov = \"HAC\"\n)\n\nsummary(gmm_fit)\n\n\nCall:\ngmm(g = g_missing, x = x_mat, t0 = start_vals, type = \"twoStep\", \n    wmatrix = \"ident\", vcov = \"HAC\")\n\n\nMethod:  One step GMM with W = identity \n\nKernel:  Quadratic Spectral\n\nCoefficients:\n          Estimate      Std. Error    t value       Pr(&gt;|t|)    \nbeta_0      1.1602e+01    2.1935e+00    5.2894e+00    1.2271e-07\nalpha      -1.7500e-02    1.2947e-01   -1.3516e-01    8.9248e-01\nbeta_iq     6.3019e-02    1.6528e-02    3.8129e+00    1.3734e-04\ngamma_0     3.4071e+00    1.1759e-01    2.8975e+01   1.3572e-184\ngamma_iq    5.8239e-03    1.1712e-03    4.9726e+00    6.6058e-07\n\nJ-Test: degrees of freedom is 2 \n                J-test      P-value   \nTest E(g)=0:    1.9454e+01  5.9644e-05\n\n#############\nInformation related to the numerical optimization\nConvergence code =  1 \nFunction eval. =  502 \nGradian eval. =  NA \n\n\n\n\n8.6 Comparing all methods\n\ncomparison &lt;- data.frame(\n  Method = c(\"True\", \"Complete Case\", \"Dummy Variable\", \"GMM\"),\n  alpha = c(\n    alpha_true,\n    coef(cc_fit)[\"bmi\"],\n    coef(dv_fit)[\"bmi\"],\n    coef(gmm_fit)[\"alpha\"]\n  ),\n  SE_alpha = c(\n    NA,\n    summary(cc_fit)$coefficients[\"bmi\", \"Std. Error\"],\n    dv_fit$std.error[\"bmi\"],\n    sqrt(vcov(gmm_fit)[\"alpha\", \"alpha\"])\n  ),\n  beta_iq = c(\n    beta_iq_true,\n    coef(cc_fit)[\"iq\"],\n    coef(dv_fit)[\"iq\"],\n    coef(gmm_fit)[\"beta_iq\"]\n  )\n)\ncomparison$alpha &lt;- round(comparison$alpha, 4)\ncomparison$SE_alpha &lt;- round(comparison$SE_alpha, 4)\ncomparison$beta_iq &lt;- round(comparison$beta_iq, 4)\ncomparison\n\n          Method   alpha SE_alpha beta_iq\n1           True -0.0400       NA  0.0600\n2  Complete Case -0.0573   0.0248  0.0578\n3 Dummy Variable -0.0573   0.0249  0.0577\n4            GMM -0.0175   0.1295  0.0630\n\n\nGMM uses all observations (both complete and incomplete cases), which can improve efficiency. The complete case estimator is consistent but throws away data. The dummy variable approach can be inconsistent depending on the missingness mechanism.\n\n\n8.7 J-test for the missing data model\nThe J-test checks whether the overidentifying restrictions are satisfied—that is, whether the linear projection assumption holds equally for observed and missing subgroups:\n\nspecTest(gmm_fit)\n\n\n ##  J-Test: degrees of freedom is 2  ## \n\n                J-test      P-value   \nTest E(g)=0:    1.9454e+01  5.9644e-05\n\n\nWith 7 moments and 5 parameters, the J-test has \\(7 - 5 = 2\\) degrees of freedom. A large p-value means we fail to reject the model specification.\n\n\n8.8 Monte Carlo: comparing estimator properties\n\nset.seed(77)\nB_mc &lt;- 300\nn_mc &lt;- 1000\nmc_results &lt;- data.frame(method = character(), alpha_hat = numeric())\n\nfor (b in 1:B_mc) {\n  iq_b &lt;- rnorm(n_mc, 100, 15)\n  xi_b &lt;- rnorm(n_mc)\n  bmi_b &lt;- gamma_0_true + gamma_iq_true * iq_b + xi_b\n  eps_b &lt;- rnorm(n_mc, sd = 1.5)\n  educ_b &lt;- beta_0_true + alpha_true * bmi_b + beta_iq_true * iq_b + eps_b\n\n  prob_m &lt;- plogis(-2 + 0.01 * iq_b)\n  m_b &lt;- rbinom(n_mc, 1, prob_m)\n  bmi_obs_b &lt;- ifelse(m_b == 0, bmi_b, 0)\n\n  # Complete case\n  cc_b &lt;- lm(educ_b[m_b == 0] ~ bmi_b[m_b == 0] + iq_b[m_b == 0])\n\n  # Dummy variable\n  dv_b &lt;- lm(educ_b ~ bmi_obs_b + iq_b + m_b)\n\n  # GMM\n  x_b &lt;- cbind(educ_b, bmi_obs_b, iq_b, m_b)\n  proj_b &lt;- lm(bmi_b[m_b == 0] ~ iq_b[m_b == 0])\n  start_b &lt;- c(beta_0 = unname(coef(cc_b)[1]),\n               alpha = unname(coef(cc_b)[2]),\n               beta_iq = unname(coef(cc_b)[3]),\n               gamma_0 = unname(coef(proj_b)[1]),\n               gamma_iq = unname(coef(proj_b)[2]))\n\n  gmm_b &lt;- tryCatch(\n    gmm(g_missing, x = x_b, t0 = start_b, type = \"twoStep\",\n        wmatrix = \"ident\", vcov = \"HAC\"),\n    error = function(e) NULL\n  )\n\n  mc_results &lt;- rbind(mc_results,\n    data.frame(method = \"Complete Case\", alpha_hat = unname(coef(cc_b)[2])),\n    data.frame(method = \"Dummy Variable\", alpha_hat = unname(coef(dv_b)[2]))\n  )\n  if (!is.null(gmm_b)) {\n    mc_results &lt;- rbind(mc_results,\n      data.frame(method = \"GMM\", alpha_hat = unname(coef(gmm_b)[\"alpha\"])))\n  }\n}\n\n\nmc_summary &lt;- aggregate(alpha_hat ~ method, data = mc_results,\n                        FUN = function(x) c(\n                          bias = mean(x) - alpha_true,\n                          variance = var(x),\n                          mse = mean((x - alpha_true)^2)\n                        ))\nmc_summary &lt;- cbind(mc_summary[1], round(as.data.frame(mc_summary[[2]]), 6))\nnames(mc_summary) &lt;- c(\"Method\", \"Bias\", \"Variance\", \"MSE\")\nmc_summary\n\n          Method     Bias Variance      MSE\n1  Complete Case 0.004402 0.003086 0.003095\n2 Dummy Variable 0.004618 0.003053 0.003064\n3            GMM 0.107638 0.340920 0.351369\n\n\n\nggplot(mc_results, aes(x = alpha_hat, fill = method)) +\n  geom_density(alpha = 0.4) +\n  geom_vline(xintercept = alpha_true, linetype = \"dashed\") +\n  labs(title = \"Estimator comparison with missing data\",\n       subtitle = paste0(\"True alpha = \", alpha_true, \", n = \", n_mc,\n                        \", \", B_mc, \" replications\"),\n       x = expression(hat(alpha)), y = \"Density\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"steelblue\", \"coral\", \"forestgreen\"))"
  },
  {
    "objectID": "ch11-gmm.html#gmm-as-a-unifying-framework",
    "href": "ch11-gmm.html#gmm-as-a-unifying-framework",
    "title": "11. GMM",
    "section": "9 GMM as a unifying framework",
    "text": "9 GMM as a unifying framework\nThe progression through the course is a series of generalizations, each nesting the previous:\n\n\n\n\n\n\n\n\n\nEstimator\nMoment condition\nWeight matrix\nWhen efficient\n\n\n\n\nOLS\n\\(\\mathbb{E}[X_i e_i] = 0\\)\n\\((X'X)^{-1}\\) (implicit)\nHomoskedastic, exogenous\n\n\nGLS\n\\(\\mathbb{E}[\\bar{X}_i \\Sigma^{-1} e_i] = 0\\)\nKnown \\(\\Sigma\\)\nKnown error structure\n\n\nIV\n\\(\\mathbb{E}[Z_i e_i] = 0\\)\n\\(\\ell = k\\) (unique)\nJust-identified\n\n\n2SLS\n\\(\\mathbb{E}[Z_i e_i] = 0\\)\n\\((Z'Z)^{-1}\\)\nHomoskedastic\n\n\nGMM\n\\(\\mathbb{E}[g_i(\\beta)] = 0\\)\n\\(\\hat\\Omega^{-1}\\)\nAlways (semiparametric bound)\n\n\n\nEvery test we have seen—\\(t\\)-tests, \\(F\\)-tests, Hausman tests, Sargan tests—is a special case of a GMM test, often valid under weaker assumptions.\n\n9.1 Recovering OLS as GMM\n\n# OLS via gmm package (instruments = regressors)\nols_gmm &lt;- gmm(Y ~ X, ~ X, data = dat)\nols_lm &lt;- lm(Y ~ X, data = dat)\ncat(\"GMM (as OLS):\", round(coef(ols_gmm)[\"X\"], 4), \"\\n\")\n\nGMM (as OLS): 0.8725 \n\ncat(\"lm():        \", round(coef(ols_lm)[\"X\"], 4), \"\\n\")\n\nlm():         0.8725 \n\n\nWhen the instruments are the regressors themselves (\\(Z = X\\)), the model is just-identified and GMM reduces to OLS regardless of the weight matrix."
  },
  {
    "objectID": "ch11-gmm.html#connection-to-mte",
    "href": "ch11-gmm.html#connection-to-mte",
    "title": "11. GMM",
    "section": "10 Connection to MTE",
    "text": "10 Connection to MTE\nThe Marginal Treatment Effect (MTE) framework (Heckman & Vytlacil, 2005) uses GMM to estimate treatment effects that vary across the population:\n\\[\\Delta^{MTE}(u_D) = \\mathbb{E}[Y_1 - Y_0 \\mid U_D = u_D]\\]\nwhere \\(U_D\\) indexes resistance to treatment. The LATE estimated by IV is a weighted average of the MTE curve over the complier population. GMM estimation of MTE involves:\n\nNonlinear moment conditions: \\(\\mathbb{E}[Y \\mid X, Z]\\) depends on \\(\\int_0^{P(Z)} \\Delta^{MTE}(u)\\,du\\), which is nonlinear in the MTE parameters.\nOveridentification from multiple instruments: more instruments than MTE parameters.\nJ-test: tests whether the MTE specification (e.g., polynomial degree) fits the data.\n\nThe ivmte package (Shea & Torgovitsky) implements this under the hood. Conceptually, the call looks like:\n# Not run -- requires ivmte package and appropriate data\nlibrary(ivmte)\nresult &lt;- ivmte(\n  data = df,\n  outcome = \"y\",\n  treatment = \"d\",\n  instrument = \"z\",\n  target = \"ate\",             # or \"att\", \"late\", \"prte\"\n  m0 = ~ u + I(u^2),         # MTE polynomial for control\n  m1 = ~ u + I(u^2),         # MTE polynomial for treated\n  propensity = d ~ z\n)\nThe key insight from GMM: by specifying moment conditions rather than a full parametric model, we can estimate flexible treatment effect heterogeneity while maintaining testable restrictions via the J-test."
  },
  {
    "objectID": "ch11-gmm.html#summary",
    "href": "ch11-gmm.html#summary",
    "title": "11. GMM",
    "section": "11 Summary",
    "text": "11 Summary\n\nGMM is a unifying framework: OLS, GLS, IV, and 2SLS are all special cases of GMM with specific weight matrices.\nEfficient GMM sets \\(W = \\hat\\Omega^{-1}\\) (the inverse of the moment covariance), minimizing asymptotic variance. Two-step and iterated procedures achieve this.\n2SLS is efficient only under homoskedasticity. Under heteroskedasticity, efficient GMM provides tighter estimates.\nThe J-test is a natural diagnostic for overidentified models. It tests whether the moment conditions are mutually consistent.\nNonlinear GMM (custom moment functions) handles problems like missing data (Abrevaya & Donald) and MTE estimation.\nThe semiparametric efficiency bound (Chamberlain, 1987): efficient GMM extracts the maximum information from moment conditions without distributional assumptions."
  },
  {
    "objectID": "ch09-testing.html",
    "href": "ch09-testing.html",
    "title": "9. Hypothesis Testing",
    "section": "",
    "text": "Chapter 8 gave us sandwich standard errors and bootstrap confidence intervals. This chapter puts them to work: we test hypotheses, build confidence regions, guard against false discoveries, and ask whether our sample is large enough to detect effects we care about.\nThe emphasis is on doing — running tests in R, interpreting the output correctly, and avoiding common pitfalls. We organize around applied questions:\nWe use the Salaries dataset from carData throughout: salaries of 397 U.S. professors, with rank, discipline, years since PhD, years of service, and sex.\nlibrary(ggplot2)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(estimatr)\nlibrary(car)\nlibrary(carData)\noptions(digits = 4)\n\ndata(Salaries)"
  },
  {
    "objectID": "ch09-testing.html#testing-a-single-coefficient",
    "href": "ch09-testing.html#testing-a-single-coefficient",
    "title": "9. Hypothesis Testing",
    "section": "1 Testing a single coefficient",
    "text": "1 Testing a single coefficient\nWe start with a wage equation and ask: controlling for rank, discipline, and experience, does sex predict salary?\n\nmod &lt;- lm(salary ~ rank + discipline + yrs.since.phd + yrs.service + sex,\n          data = Salaries)\nsummary(mod)\n\n\nCall:\nlm(formula = salary ~ rank + discipline + yrs.since.phd + yrs.service + \n    sex, data = Salaries)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-65248 -13211  -1775  10384  99592 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      65955       4589   14.37  &lt; 2e-16 ***\nrankAssocProf    12908       4145    3.11    0.002 ** \nrankProf         45066       4238   10.63  &lt; 2e-16 ***\ndisciplineB      14418       2343    6.15  1.9e-09 ***\nyrs.since.phd      535        241    2.22    0.027 *  \nyrs.service       -490        212   -2.31    0.021 *  \nsexMale           4784       3859    1.24    0.216    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22500 on 390 degrees of freedom\nMultiple R-squared:  0.455, Adjusted R-squared:  0.446 \nF-statistic: 54.2 on 6 and 390 DF,  p-value: &lt;2e-16\n\n\nThe summary() output gives t-statistics and p-values, but these assume homoskedasticity. The robust version uses sandwich standard errors:\n\ncoeftest(mod, vcov. = vcovHC(mod, type = \"HC1\"))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      65955       2896   22.78  &lt; 2e-16 ***\nrankAssocProf    12908       2206    5.85  1.0e-08 ***\nrankProf         45066       3285   13.72  &lt; 2e-16 ***\ndisciplineB      14418       2316    6.22  1.2e-09 ***\nyrs.since.phd      535        312    1.71    0.088 .  \nyrs.service       -490        307   -1.60    0.111    \nsexMale           4784       2396    2.00    0.047 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThese are the same tests, built from the same Wald statistic \\(W = (\\hat{\\beta}_j / \\widehat{\\text{se}}_j)^2\\). The only difference is the denominator. When the two disagree, trust the robust version — it doesn’t assume \\(\\text{Var}(e_i \\mid X_i)\\) is constant.\nYou can also use linearHypothesis() for the same test, which makes the Wald structure explicit:\n\nlinearHypothesis(mod, \"sexMale = 0\", vcov. = vcovHC(mod, type = \"HC1\"))\n\n\nLinear hypothesis test:\nsexMale = 0\n\nModel 1: restricted model\nModel 2: salary ~ rank + discipline + yrs.since.phd + yrs.service + sex\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df    F Pr(&gt;F)  \n1    391                 \n2    390  1 3.99  0.047 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis tests \\(H_0: \\beta_{\\text{Male}} = 0\\) against \\(H_1: \\beta_{\\text{Male}} \\neq 0\\) using the robust covariance matrix. The Chisq column is the Wald statistic; divide by 1 (one restriction) and you get the squared t-statistic.\n\n1.1 Testing against a non-zero value\nSuppose theory predicts a gender gap of $5,000. The null is \\(H_0: \\beta_{\\text{Male}} = 5000\\):\n\nlinearHypothesis(mod, \"sexMale = 5000\", vcov. = vcovHC(mod, type = \"HC1\"))\n\n\nLinear hypothesis test:\nsexMale = 5000\n\nModel 1: restricted model\nModel 2: salary ~ rank + discipline + yrs.since.phd + yrs.service + sex\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df    F Pr(&gt;F)\n1    391               \n2    390  1 0.01   0.93\n\n\nThe test is not significant if the confidence interval from Chapter 8 already contains $5,000. Test and CI carry exactly the same information — they are duals."
  },
  {
    "objectID": "ch09-testing.html#testing-multiple-restrictions-the-f-test",
    "href": "ch09-testing.html#testing-multiple-restrictions-the-f-test",
    "title": "9. Hypothesis Testing",
    "section": "2 Testing multiple restrictions: the F-test",
    "text": "2 Testing multiple restrictions: the F-test\nIndividual t-tests tell you about one coefficient at a time. But sometimes you need a joint test. Does rank matter at all? That means testing two coefficients simultaneously (since rank is a three-level factor with two dummies):\n\\[H_0: \\beta_{\\text{AssocProf}} = \\beta_{\\text{Prof}} = 0\\]\n\n2.1 By hand: comparing two regressions\nThe F-test compares the fit of an unrestricted model to a restricted model that imposes \\(H_0\\):\n\\[F = \\frac{(\\text{SSE}_R - \\text{SSE}_U) / q}{\\text{SSE}_U / (n - k)}\\]\nwhere \\(q\\) is the number of restrictions.\n\n## Unrestricted model (full)\nmod_U &lt;- mod\n\n## Restricted model: drop rank\nmod_R &lt;- lm(salary ~ discipline + yrs.since.phd + yrs.service + sex,\n            data = Salaries)\n\n## Manual F-stat\nSSE_U &lt;- sum(resid(mod_U)^2)\nSSE_R &lt;- sum(resid(mod_R)^2)\nq &lt;- 2                              # two restrictions\nn &lt;- nobs(mod_U)\nk &lt;- length(coef(mod_U))\n\nF_stat &lt;- ((SSE_R - SSE_U) / q) / (SSE_U / (n - k))\np_val  &lt;- 1 - pf(F_stat, q, n - k)\n\ncat(\"F =\", round(F_stat, 2), \"  df1 =\", q, \"  df2 =\", n - k,\n    \"  p =\", format.pval(p_val), \"\\n\")\n\nF = 68.41   df1 = 2   df2 = 390   p = &lt;2e-16 \n\n\n\n\n2.2 The easy way: anova() and linearHypothesis()\n\n## Nested model comparison\nanova(mod_R, mod_U)\n\nAnalysis of Variance Table\n\nModel 1: salary ~ discipline + yrs.since.phd + yrs.service + sex\nModel 2: salary ~ rank + discipline + yrs.since.phd + yrs.service + sex\n  Res.Df      RSS Df Sum of Sq    F Pr(&gt;F)    \n1    392 2.68e+11                             \n2    390 1.98e+11  2  6.95e+10 68.4 &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n## Same test via linearHypothesis (with robust SEs)\nlinearHypothesis(mod, c(\"rankAssocProf = 0\", \"rankProf = 0\"),\n                 vcov. = vcovHC(mod, type = \"HC1\"))\n\n\nLinear hypothesis test:\nrankAssocProf = 0\nrankProf = 0\n\nModel 1: restricted model\nModel 2: salary ~ rank + discipline + yrs.since.phd + yrs.service + sex\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df   F Pr(&gt;F)    \n1    392                  \n2    390  2 109 &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote that anova() uses the classical (homoskedastic) F-test, while linearHypothesis() with a robust vcov. gives the heteroskedasticity-robust Wald test. In this example they agree; with strongly heteroskedastic data they might not.\n\n\n2.3 Equality constraints\nDoes the return to years since PhD equal the return to years of service? This is \\(H_0: \\beta_{\\text{yrs.phd}} = \\beta_{\\text{yrs.service}}\\):\n\nlinearHypothesis(mod, \"yrs.since.phd = yrs.service\",\n                 vcov. = vcovHC(mod, type = \"HC1\"))\n\n\nLinear hypothesis test:\nyrs.since.phd - yrs.service = 0\n\nModel 1: restricted model\nModel 2: salary ~ rank + discipline + yrs.since.phd + yrs.service + sex\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df    F Pr(&gt;F)  \n1    391                 \n2    390  1 2.92  0.088 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis works because linearHypothesis() can test any linear restriction \\(\\mathbf{R}\\boldsymbol{\\beta} = \\mathbf{r}\\). Under the hood it constructs the restriction matrix \\(\\mathbf{R}\\) and computes: \\[W = (\\mathbf{R}\\hat{\\boldsymbol{\\beta}} - \\mathbf{r})' [\\mathbf{R}\\hat{\\mathbf{V}}\\mathbf{R}']^{-1} (\\mathbf{R}\\hat{\\boldsymbol{\\beta}} - \\mathbf{r}) \\;\\xrightarrow{d}\\; \\chi^2_q \\tag{1}\\]\n\nTheorem 1 (Wald Statistic) For testing \\(H_0: R\\beta = r\\), the Wald statistic is \\(W = (R\\hat\\beta - r)'[R\\hat{V}R']^{-1}(R\\hat\\beta - r) \\xrightarrow{d} \\chi^2_q\\) under \\(H_0\\), where \\(q\\) is the number of restrictions. With robust \\(\\hat{V}\\), the test is valid under heteroskedasticity."
  },
  {
    "objectID": "ch09-testing.html#the-test-trinity-wald-score-and-f",
    "href": "ch09-testing.html#the-test-trinity-wald-score-and-f",
    "title": "9. Hypothesis Testing",
    "section": "3 The test trinity: Wald, Score, and F",
    "text": "3 The test trinity: Wald, Score, and F\nThree classical approaches test the same null \\(H_0: \\mathbf{R}\\boldsymbol{\\beta} = \\mathbf{r}\\):\n\n\n\n\n\n\n\n\nTest\nEstimates from\nMeasures\n\n\n\n\nWald\nUnrestricted model\nHow far \\(\\hat{\\boldsymbol{\\beta}}\\) is from \\(H_0\\)\n\n\nScore (LM)\nRestricted model\nWhether the objective function wants to move away from \\(H_0\\)\n\n\nF / LR\nBoth models\nHow much the fit worsens when we impose \\(H_0\\)\n\n\n\nFor linear restrictions in a normal linear model, they are monotone transformations of each other and always give the same accept/reject decision. Let’s verify:\n\n## Test: rank doesn't matter (q = 2 restrictions)\n## Wald (homoskedastic, for comparability)\nW &lt;- linearHypothesis(mod_U, c(\"rankAssocProf = 0\", \"rankProf = 0\"))\nW_stat &lt;- W$F[2] * q  # linearHypothesis reports F; multiply by q for chi-sq scale\n\n## F from anova\nFtest &lt;- anova(mod_R, mod_U)\n\n## Score test (LM): use restricted residuals\n## S = n * (1 - SSE_U / SSE_R) under homoskedasticity\nS &lt;- n * (1 - SSE_U / SSE_R)\n\ncat(\"Wald statistic (chi-sq scale):\", round(W_stat, 3), \"\\n\")\n\nWald statistic (chi-sq scale): 136.8 \n\ncat(\"F statistic (x q):           \", round(Ftest$F[2] * q, 3), \"\\n\")\n\nF statistic (x q):            136.8 \n\ncat(\"Score statistic:             \", round(S, 3), \"\\n\")\n\nScore statistic:              103.1 \n\n\nThe three numbers are close but not identical — they use different variance estimates (unrestricted \\(s^2\\), restricted \\(\\tilde{\\sigma}^2\\), or pooled). Asymptotically they converge. Under normality, we have the exact distributions that make the F-test slightly more conservative in finite samples, which is why anova() uses \\(F_{q, n-k}\\) critical values instead of \\(\\chi^2_q / q\\).\n\n3.1 When do they diverge?\nFor non-normal models, the trinity can disagree because the three test statistics are no longer monotone transformations of each other. Here is an example using a joint test in the probit model from Chapter 7:\n\ndata(Mroz)\nprobit &lt;- glm(lfp ~ k5 + k618 + age + wc + lwg + inc,\n              family = binomial(link = \"probit\"), data = Mroz)\n\n## Test H0: k5 = 0, k618 = 0 (joint, q = 2)\nprobit_r &lt;- glm(lfp ~ age + wc + lwg + inc,\n                family = binomial(link = \"probit\"), data = Mroz)\n\n## Wald\nW_probit &lt;- linearHypothesis(probit, c(\"k5 = 0\", \"k618 = 0\"))\n\n## LR\nLR &lt;- -2 * as.numeric(logLik(probit_r) - logLik(probit))\n\n## Score (Rao)\nS_probit &lt;- anova(probit_r, probit, test = \"Rao\")\n\ncat(\"Wald chi-sq: \", round(W_probit$Chisq[2], 2), \"\\n\")\n\nWald chi-sq:  58.45 \n\ncat(\"LR chi-sq:   \", round(LR, 2), \"\\n\")\n\nLR chi-sq:    65.77 \n\ncat(\"Score chi-sq:\", round(S_probit$Rao[2], 2), \"\\n\")\n\nScore chi-sq: 65.03 \n\n\nThe three statistics differ noticeably: the Wald statistic (58.5) is substantially smaller than the LR (65.8) and Score (65.0). All three reject decisively here, but in borderline cases they could lead to different conclusions. The LR test tends to be the most reliable in nonlinear models because it directly measures the change in the log-likelihood."
  },
  {
    "objectID": "ch09-testing.html#confidence-regions-and-test-inversion",
    "href": "ch09-testing.html#confidence-regions-and-test-inversion",
    "title": "9. Hypothesis Testing",
    "section": "4 Confidence regions and test inversion",
    "text": "4 Confidence regions and test inversion\nA 95% confidence interval is the set of values \\(\\theta_0\\) that would not be rejected at the 5% level:\n\\[\\hat{C} = \\{\\theta_0 : |T(\\theta_0)| \\leq 1.96\\}\\]\nThis is test inversion. For a single linear coefficient, test inversion gives the familiar \\(\\hat{\\beta} \\pm 1.96 \\cdot \\widehat{\\text{se}}\\). But for nonlinear functions of parameters, test inversion is more reliable than the delta method.\n\n\n\n\n\n\nNoteTest Inversion Gives Confidence Sets\n\n\n\nA 95% confidence set is exactly the set of parameter values not rejected at the 5% level. For linear parameters, this gives the familiar \\(\\hat\\beta \\pm 1.96 \\cdot \\text{SE}\\). For nonlinear functions, Fieller’s method (test inversion over a grid) is more reliable than the delta method.\n\n\n\n4.1 Confidence ellipses for two coefficients\nFor two parameters tested jointly, the confidence region is an ellipse — the set of \\((\\beta_1, \\beta_2)\\) values consistent with the Wald test:\n\n## Joint confidence region for the two experience variables\nconfidenceEllipse(mod, which.coef = c(\"yrs.since.phd\", \"yrs.service\"),\n                  vcov. = vcovHC(mod, type = \"HC1\"),\n                  main = \"95% joint confidence region\",\n                  xlab = \"Coefficient on yrs.since.phd\",\n                  ylab = \"Coefficient on yrs.service\",\n                  col = \"steelblue\", lwd = 2)\nabline(h = 0, lty = 2, col = \"gray50\")\nabline(v = 0, lty = 2, col = \"gray50\")\n## Add the 45-degree line for equality\nabline(a = 0, b = 1, lty = 3, col = \"coral\", lwd = 1.5)\nlegend(\"topright\", legend = c(\"95% joint region\", \"equality line\"),\n       col = c(\"steelblue\", \"coral\"), lty = c(1, 3), lwd = c(2, 1.5))\n\n\n\n\n\n\n\n\nThe dashed lines at zero show the individual null values. The coral line shows where the two coefficients are equal. If the ellipse crosses a line, the corresponding null cannot be rejected.\n\n\n4.2 Test inversion for a nonlinear parameter: Fieller’s method\nSuppose we want a confidence interval for the ratio \\(\\theta = \\beta_{\\text{Prof}} / \\beta_{\\text{AssocProf}}\\) — how many times larger is the full professor salary premium than the associate professor premium? The delta method (Chapter 8) gives a quick CI, but it can be unreliable when the denominator is imprecisely estimated.\nFieller’s trick: rewrite \\(\\theta = \\beta_1 / \\beta_2\\) as the linear restriction \\(\\beta_1 - \\theta \\beta_2 = 0\\), then invert the Wald test over a grid of \\(\\theta\\) values:\n\nb &lt;- coef(mod)\nV &lt;- vcovHC(mod, type = \"HC1\")\n\n## Delta method CI for comparison\ndm &lt;- deltaMethod(mod, \"rankProf / rankAssocProf\",\n                  vcov. = vcovHC(mod, type = \"HC1\"))\ndm_ci &lt;- c(dm$Estimate - 1.96 * dm$SE, dm$Estimate + 1.96 * dm$SE)\n\n## Grid search: invert the Wald test\ntheta_grid &lt;- seq(0, 8, by = 0.01)\nidx_prof &lt;- which(names(b) == \"rankProf\")\nidx_assoc &lt;- which(names(b) == \"rankAssocProf\")\n\nin_CI &lt;- sapply(theta_grid, function(th) {\n  r_vec &lt;- numeric(length(b))\n  r_vec[idx_prof] &lt;- 1\n  r_vec[idx_assoc] &lt;- -th\n  num &lt;- (sum(r_vec * b))^2\n  den &lt;- t(r_vec) %*% V %*% r_vec\n  num / den &lt;= qchisq(0.95, 1)\n})\nfieller_ci &lt;- range(theta_grid[in_CI])\n\ncat(\"Point estimate:  \", round(dm$Estimate, 3), \"\\n\")\n\nPoint estimate:   3.491 \n\ncat(\"Delta method CI: \", round(dm_ci, 3), \"\\n\")\n\nDelta method CI:  2.602 4.381 \n\ncat(\"Fieller CI:      \", round(fieller_ci, 3), \"\\n\")\n\nFieller CI:       2.81 4.79 \n\n\nHere the two CIs are similar because the denominator (\\(\\beta_{\\text{AssocProf}}\\)) is precisely estimated. The ratio tells us that the full professor premium is about 3.5 times the associate professor premium.\n\n## Plot the Wald statistic over theta\nwald_vals &lt;- sapply(theta_grid, function(th) {\n  r_vec &lt;- numeric(length(b))\n  r_vec[idx_prof] &lt;- 1\n  r_vec[idx_assoc] &lt;- -th\n  num &lt;- (sum(r_vec * b))^2\n  den &lt;- c(t(r_vec) %*% V %*% r_vec)\n  num / den\n})\n\ndf_fieller &lt;- data.frame(theta = theta_grid, W = wald_vals)\nggplot(df_fieller, aes(theta, W)) +\n  geom_line(color = \"steelblue\", linewidth = 0.8) +\n  geom_hline(yintercept = qchisq(0.95, 1), linetype = \"dashed\", color = \"coral\") +\n  annotate(\"text\", x = 6.5, y = qchisq(0.95, 1) + 0.5, label = \"95% critical value\",\n           color = \"coral\") +\n  coord_cartesian(ylim = c(0, 15)) +\n  labs(title = \"Test inversion: Wald statistic as a function of θ₀\",\n       subtitle = \"The Fieller CI is the set of θ₀ values below the dashed line\",\n       x = expression(theta[0] == beta[Prof] / beta[AssocProf]),\n       y = \"Wald statistic\")\n\n\n\n\n\n\n\n\n\n\n4.3 When Fieller and the delta method disagree\nThe real value of Fieller’s method appears when the denominator is imprecise. Consider \\(\\theta = \\beta_{\\text{yrs.phd}} / \\beta_{\\text{yrs.service}}\\) — the ratio of the two experience coefficients. Both are individually insignificant:\n\n## Experience coefficients are imprecise\ncat(\"yrs.since.phd: estimate =\", round(b[\"yrs.since.phd\"]),\n    \"  SE =\", round(sqrt(V[\"yrs.since.phd\", \"yrs.since.phd\"])), \"\\n\")\n\nyrs.since.phd: estimate = 535   SE = 312 \n\ncat(\"yrs.service:   estimate =\", round(b[\"yrs.service\"]),\n    \"  SE =\", round(sqrt(V[\"yrs.service\", \"yrs.service\"])), \"\\n\")\n\nyrs.service:   estimate = -490   SE = 307 \n\n## Delta method gives a finite CI\ndm2 &lt;- deltaMethod(mod, \"yrs.since.phd / yrs.service\",\n                   vcov. = vcovHC(mod, type = \"HC1\"))\ncat(\"\\nDelta method CI:\", round(c(dm2$Estimate - 1.96 * dm2$SE,\n                                  dm2$Estimate + 1.96 * dm2$SE), 2), \"\\n\")\n\n\nDelta method CI: -1.75 -0.43 \n\n## Fieller: check a wide grid\ntheta_wide &lt;- seq(-50, 50, by = 0.1)\nidx_phd &lt;- which(names(b) == \"yrs.since.phd\")\nidx_svc &lt;- which(names(b) == \"yrs.service\")\nin_CI2 &lt;- sapply(theta_wide, function(th) {\n  r_vec &lt;- numeric(length(b))\n  r_vec[idx_phd] &lt;- 1\n  r_vec[idx_svc] &lt;- -th\n  num &lt;- (sum(r_vec * b))^2\n  den &lt;- c(t(r_vec) %*% V %*% r_vec)\n  num / den &lt;= qchisq(0.95, 1)\n})\n\n## Check if CI covers entire grid (unbounded)\nif (all(in_CI2)) {\n  cat(\"Fieller CI: (-∞, +∞)  [unbounded!]\\n\")\n} else if (any(in_CI2)) {\n  cat(\"Fieller CI:\", range(theta_wide[in_CI2]), \"\\n\")\n}\n\nFieller CI: (-∞, +∞)  [unbounded!]\n\n\nThe delta method reports a finite CI of width ~1.3, but the Fieller CI is unbounded — the entire real line. This is the correct answer: since \\(\\beta_{\\text{yrs.service}}\\) is not significantly different from zero, the ratio could be anything. The delta method’s finite interval is overconfident."
  },
  {
    "objectID": "ch09-testing.html#sec-multiple-testing",
    "href": "ch09-testing.html#sec-multiple-testing",
    "title": "9. Hypothesis Testing",
    "section": "5 Multiple testing",
    "text": "5 Multiple testing\nWhen you look at many coefficients, some will be “significant” by chance.\n\n5.1 Simulation: false discoveries under the null\nTo make this concrete: generate 20 pure-noise regressors, test each at 5%, and count how many reject:\n\nset.seed(303)\nn &lt;- 200\nk_noise &lt;- 20\nY &lt;- rnorm(n)\nX_noise &lt;- matrix(rnorm(n * k_noise), nrow = n)\ncolnames(X_noise) &lt;- paste0(\"X\", 1:k_noise)\ndat_noise &lt;- data.frame(Y, X_noise)\n\nmod_noise &lt;- lm(Y ~ ., data = dat_noise)\npvals &lt;- summary(mod_noise)$coefficients[-1, 4]  # drop intercept\n\ncat(\"Number of regressors with p &lt; 0.05:\", sum(pvals &lt; 0.05), \"out of\", k_noise, \"\\n\")\n\nNumber of regressors with p &lt; 0.05: 1 out of 20 \n\ncat(\"Significant variables:\", paste(names(pvals[pvals &lt; 0.05]), collapse = \", \"), \"\\n\")\n\nSignificant variables: X12 \n\n\nUnder the global null, we expect \\(20 \\times 0.05 = 1\\) false rejection on average. Repeat this many times to see the problem clearly:\n\nset.seed(42)\nB &lt;- 5000\nn &lt;- 200\nk_noise &lt;- 20\n\nfalse_rejections &lt;- replicate(B, {\n  Y &lt;- rnorm(n)\n  X &lt;- matrix(rnorm(n * k_noise), nrow = n)\n  pv &lt;- summary(lm(Y ~ X))$coefficients[-1, 4]\n  sum(pv &lt; 0.05)\n})\n\ncat(\"P(at least one false rejection):\", mean(false_rejections &gt;= 1), \"\\n\")\n\nP(at least one false rejection): 0.6204 \n\ncat(\"Expected:\", round(1 - (1 - 0.05)^k_noise, 3), \"\\n\")\n\nExpected: 0.642 \n\nggplot(data.frame(rejections = false_rejections), aes(rejections)) +\n  geom_bar(fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = 1, linetype = \"dashed\", color = \"coral\") +\n  labs(title = \"False rejections when all 20 nulls are true\",\n       subtitle = paste0(\"P(≥1 false rejection) = \",\n                         round(mean(false_rejections &gt;= 1), 3)),\n       x = \"Number of rejections at α = 0.05\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\n\nWith 20 tests at 5%, you have about a 64% chance of at least one false rejection. This is the familywise error rate (FWER) problem.\n\n\n\n\n\n\nWarningMultiple Testing Inflates the Familywise Error Rate\n\n\n\nWith \\(k\\) independent tests at level \\(\\alpha\\), the probability of at least one false rejection is \\(1 - (1 - \\alpha)^k\\). With 20 tests at 5%, this exceeds 60%. Use Holm’s correction (uniformly more powerful than Bonferroni) to control the FWER.\n\n\n\n\n5.2 Corrections: Bonferroni and Holm\nThe Bonferroni correction rejects the \\(j\\)th hypothesis only if \\(p_j &lt; \\alpha / k\\). It controls the FWER for any dependence structure:\n\n## Back to our salary model\npvals_salary &lt;- summary(mod)$coefficients[-1, 4]\n\ncorrections &lt;- data.frame(\n  raw = round(pvals_salary, 4),\n  bonferroni = round(p.adjust(pvals_salary, method = \"bonferroni\"), 4),\n  holm = round(p.adjust(pvals_salary, method = \"holm\"), 4)\n)\ncorrections\n\n                 raw bonferroni   holm\nrankAssocProf 0.0020     0.0119 0.0079\nrankProf      0.0000     0.0000 0.0000\ndisciplineB   0.0000     0.0000 0.0000\nyrs.since.phd 0.0270     0.1619 0.0643\nyrs.service   0.0214     0.1286 0.0643\nsexMale       0.2158     1.0000 0.2158\n\n\nHolm’s method is uniformly more powerful than Bonferroni while still controlling the FWER. It works by ordering the p-values from smallest to largest and comparing the \\(j\\)th smallest to \\(\\alpha / (k - j + 1)\\). Use Holm whenever you would use Bonferroni.\n\n\n5.3 When to worry about multiple testing\nMultiple testing corrections matter when you:\n\nExamine many coefficients and report the “most significant”\nTry many specifications and present the one that “works”\nTest across subgroups (by age, gender, region, …)\nUse stepwise selection procedures\n\nThey matter less when you have a small number of pre-specified hypotheses motivated by theory."
  },
  {
    "objectID": "ch09-testing.html#power-can-you-detect-the-effect",
    "href": "ch09-testing.html#power-can-you-detect-the-effect",
    "title": "9. Hypothesis Testing",
    "section": "6 Power: can you detect the effect?",
    "text": "6 Power: can you detect the effect?\nA test’s power is the probability it rejects when \\(H_0\\) is false:\n\\[\\text{Power}(\\theta) = P[\\text{Reject } H_0 \\mid \\theta \\neq \\theta_0]\\]\nPower depends on four things: the true effect size, the standard error, the significance level, and the sample size. In OLS, for a two-sided test of \\(H_0: \\beta = 0\\):\n\\[\\delta = \\frac{|\\beta|}{\\text{se}(\\hat{\\beta})} \\approx \\frac{|\\beta| \\cdot \\text{sd}(X) \\cdot \\sqrt{n}}{\\sigma_e}\\]\nPower \\(\\approx \\Phi(\\delta - z_{\\alpha/2}) + \\Phi(-\\delta - z_{\\alpha/2})\\), where \\(\\delta\\) is the signal-to-noise ratio.\n\nTheorem 2 (Power of a Two-Sided Test) For testing \\(H_0: \\beta = 0\\) at level \\(\\alpha\\), the power against alternative \\(\\beta \\neq 0\\) is approximately \\(\\Phi(\\delta - z_{\\alpha/2}) + \\Phi(-\\delta - z_{\\alpha/2})\\), where \\(\\delta = |\\beta|/\\text{SE}(\\hat\\beta)\\) is the signal-to-noise ratio.\n\n\n## Power for a two-sided t-test in OLS\npower_ols &lt;- function(effect, sigma_e, sd_x, n, alpha = 0.05) {\n  se &lt;- sigma_e / (sd_x * sqrt(n))\n  delta &lt;- abs(effect) / se\n  z &lt;- qnorm(1 - alpha / 2)\n  pnorm(delta - z) + pnorm(-delta - z)\n}\n\n\n6.1 Power curves\nHow does power vary with sample size and effect size?\n\nn_seq &lt;- seq(20, 500, by = 5)\neffects &lt;- c(0.05, 0.10, 0.20, 0.40)\n\ndf_power &lt;- do.call(rbind, lapply(effects, function(eff) {\n  data.frame(\n    n = n_seq,\n    power = sapply(n_seq, function(nn) power_ols(eff, sigma_e = 1, sd_x = 1, nn)),\n    effect = paste0(\"β = \", eff)\n  )\n}))\n\nggplot(df_power, aes(n, power, color = effect)) +\n  geom_line(linewidth = 1) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"gray50\") +\n  annotate(\"text\", x = 480, y = 0.82, label = \"80% power\", color = \"gray50\") +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\", color = \"gray70\") +\n  annotate(\"text\", x = 480, y = 0.07, label = \"α = 0.05\", color = \"gray70\") +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Power curves for a two-sided OLS t-test\",\n       subtitle = \"σ_e = 1, sd(X) = 1\",\n       x = \"Sample size (n)\", y = \"Power\", color = \"True effect\") +\n  coord_cartesian(ylim = c(0, 1))\n\n\n\n\nPower curves for a two-sided OLS t-test\n\n\n\n\nSmall effects require enormous samples. With \\(\\beta = 0.10\\) and \\(\\sigma_e / \\text{sd}(X) = 1\\), you need roughly \\(n = 800\\) for 80% power.\n\n\n\n\n\n\nTipThe 80% Power Rule of Thumb\n\n\n\nA study should have at least 80% power to detect the smallest effect of scientific interest. This requires \\(n \\approx 16\\sigma_e^2 / (\\beta^2 \\cdot \\text{Var}(X))\\) for a two-sided 5% test. Adding relevant controls reduces \\(\\sigma_e\\) and boosts power for free.\n\n\n\n\n6.2 Simulation: observing power\nLet’s verify the formula with a Monte Carlo simulation:\n\nset.seed(12)\nB &lt;- 2000\ntrue_beta &lt;- 0.3\nsigma_e &lt;- 1\nn_vals &lt;- c(20, 50, 100, 200)\n\nsim_power &lt;- function(n, B) {\n  rejections &lt;- replicate(B, {\n    x &lt;- rnorm(n)\n    y &lt;- true_beta * x + rnorm(n, sd = sigma_e)\n    pval &lt;- summary(lm(y ~ x))$coefficients[\"x\", \"Pr(&gt;|t|)\"]\n    pval &lt; 0.05\n  })\n  mean(rejections)\n}\n\nresults &lt;- data.frame(\n  n = n_vals,\n  simulated = sapply(n_vals, sim_power, B = B),\n  formula = sapply(n_vals, function(nn) power_ols(true_beta, sigma_e, 1, nn))\n)\nresults\n\n    n simulated formula\n1  20    0.2290  0.2687\n2  50    0.5575  0.5641\n3 100    0.8250  0.8508\n4 200    0.9880  0.9888\n\n\nThe simulated power matches the formula closely.\n\n\n6.3 Controls boost power\nAdding a relevant control variable reduces \\(\\sigma_e\\) without reducing \\(\\text{sd}(X)\\), giving a free power boost:\n\nset.seed(99)\nB &lt;- 2000\nn &lt;- 100\nbeta_x &lt;- 0.2\nbeta_z &lt;- 1.0  # strong control\n\n## Without control\npower_no_ctrl &lt;- mean(replicate(B, {\n  x &lt;- rnorm(n); z &lt;- rnorm(n)\n  y &lt;- beta_x * x + beta_z * z + rnorm(n)\n  summary(lm(y ~ x))$coefficients[\"x\", \"Pr(&gt;|t|)\"] &lt; 0.05\n}))\n\n## With control\npower_with_ctrl &lt;- mean(replicate(B, {\n  x &lt;- rnorm(n); z &lt;- rnorm(n)\n  y &lt;- beta_x * x + beta_z * z + rnorm(n)\n  summary(lm(y ~ x + z))$coefficients[\"x\", \"Pr(&gt;|t|)\"] &lt; 0.05\n}))\n\ncat(\"Power without control:\", round(power_no_ctrl, 3), \"\\n\")\n\nPower without control: 0.294 \n\ncat(\"Power with control:   \", round(power_with_ctrl, 3), \"\\n\")\n\nPower with control:    0.5 \n\ncat(\"Residual SD without z:\", round(sqrt(beta_z^2 + 1), 2), \"\\n\")\n\nResidual SD without z: 1.41 \n\ncat(\"Residual SD with z:   \", round(1, 2), \"\\n\")\n\nResidual SD with z:    1 \n\n\nThe control cuts the residual standard deviation roughly in half, dramatically increasing power. This is why pre-treatment covariates help in experiments even though they are unnecessary for unbiasedness.\n\n\n6.4 Joint tests have less power\nTesting more restrictions simultaneously dilutes power. Fix the total non-centrality parameter and compare:\n\nlambda_seq &lt;- seq(0, 15, by = 0.1)\n\ndf_joint &lt;- do.call(rbind, lapply(c(1, 2, 5, 10), function(q) {\n  data.frame(\n    lambda = lambda_seq,\n    power = 1 - pchisq(qchisq(0.95, q), q, ncp = lambda_seq),\n    q = paste0(\"q = \", q)\n  )\n}))\n\nggplot(df_joint, aes(lambda, power, color = q)) +\n  geom_line(linewidth = 1) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"gray50\") +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Power vs. non-centrality for joint tests\",\n       subtitle = \"More restrictions require a larger signal to achieve the same power\",\n       x = expression(paste(\"Non-centrality parameter \", lambda)),\n       y = \"Power\", color = \"Restrictions\")\n\n\n\n\n\n\n\n\nA single-coefficient t-test is more powerful than a joint F-test that lumps in other restrictions. Test exactly what you care about."
  },
  {
    "objectID": "ch09-testing.html#statistical-vs.-economic-significance",
    "href": "ch09-testing.html#statistical-vs.-economic-significance",
    "title": "9. Hypothesis Testing",
    "section": "7 Statistical vs. economic significance",
    "text": "7 Statistical vs. economic significance\nWith a large enough sample, any nonzero coefficient becomes statistically significant. This is test consistency: \\(P[\\text{Reject}] \\to 1\\) whenever \\(\\beta \\neq 0\\).\n\nset.seed(77)\ntrue_beta &lt;- 0.01   # tiny effect\nn_vals &lt;- c(50, 200, 1000, 5000, 20000)\nB &lt;- 1000\n\nrejection_rate &lt;- sapply(n_vals, function(nn) {\n  mean(replicate(B, {\n    x &lt;- rnorm(nn)\n    y &lt;- true_beta * x + rnorm(nn)\n    summary(lm(y ~ x))$coefficients[\"x\", \"Pr(&gt;|t|)\"] &lt; 0.05\n  }))\n})\n\ndf_consist &lt;- data.frame(n = n_vals, rejection_rate = rejection_rate)\nggplot(df_consist, aes(n, rejection_rate)) +\n  geom_line(linewidth = 1, color = \"steelblue\") +\n  geom_point(size = 3, color = \"steelblue\") +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\", color = \"coral\") +\n  scale_x_log10() +\n  labs(title = \"Test consistency: even β = 0.01 gets rejected eventually\",\n       subtitle = \"With enough data, statistical significance is guaranteed for any nonzero effect\",\n       x = \"Sample size (log scale)\", y = \"Rejection rate at α = 0.05\")\n\n\n\n\n\n\n\n\nA \\(p\\)-value tells you whether the data are inconsistent with \\(H_0\\). It tells you nothing about whether the effect is large enough to matter. Always report the magnitude alongside the significance."
  },
  {
    "objectID": "ch09-testing.html#applied-workflow-the-salary-example",
    "href": "ch09-testing.html#applied-workflow-the-salary-example",
    "title": "9. Hypothesis Testing",
    "section": "8 Applied workflow: the salary example",
    "text": "8 Applied workflow: the salary example\nLet’s bring everything together with a thorough analysis of the professor salary data.\n\n8.1 Step 1: Fit and inspect\n\nmod_full &lt;- lm(salary ~ rank + discipline + yrs.since.phd + yrs.service + sex,\n               data = Salaries)\ncoeftest(mod_full, vcov. = vcovHC(mod_full, type = \"HC1\"))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      65955       2896   22.78  &lt; 2e-16 ***\nrankAssocProf    12908       2206    5.85  1.0e-08 ***\nrankProf         45066       3285   13.72  &lt; 2e-16 ***\ndisciplineB      14418       2316    6.22  1.2e-09 ***\nyrs.since.phd      535        312    1.71    0.088 .  \nyrs.service       -490        307   -1.60    0.111    \nsexMale           4784       2396    2.00    0.047 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n8.2 Step 2: Key hypotheses\nDoes rank matter? (joint test, \\(q = 2\\))\n\nlinearHypothesis(mod_full, c(\"rankAssocProf = 0\", \"rankProf = 0\"),\n                 vcov. = vcovHC(mod_full, type = \"HC1\"))\n\n\nLinear hypothesis test:\nrankAssocProf = 0\nrankProf = 0\n\nModel 1: restricted model\nModel 2: salary ~ rank + discipline + yrs.since.phd + yrs.service + sex\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df   F Pr(&gt;F)    \n1    392                  \n2    390  2 109 &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDoes discipline matter? (single restriction)\n\nlinearHypothesis(mod_full, \"disciplineB = 0\",\n                 vcov. = vcovHC(mod_full, type = \"HC1\"))\n\n\nLinear hypothesis test:\ndisciplineB = 0\n\nModel 1: restricted model\nModel 2: salary ~ rank + discipline + yrs.since.phd + yrs.service + sex\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df    F  Pr(&gt;F)    \n1    391                    \n2    390  1 38.7 1.2e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAre the two experience measures interchangeable? (\\(\\beta_{\\text{phd}} = \\beta_{\\text{svc}}\\))\n\nlinearHypothesis(mod_full, \"yrs.since.phd = yrs.service\",\n                 vcov. = vcovHC(mod_full, type = \"HC1\"))\n\n\nLinear hypothesis test:\nyrs.since.phd - yrs.service = 0\n\nModel 1: restricted model\nModel 2: salary ~ rank + discipline + yrs.since.phd + yrs.service + sex\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df    F Pr(&gt;F)  \n1    391                 \n2    390  1 2.92  0.088 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n8.3 Step 3: Confidence intervals\n\n## Robust CIs\nci_robust &lt;- coefci(mod_full, vcov. = vcovHC(mod_full, type = \"HC1\"))\nci_robust\n\n                 2.5 %  97.5 %\n(Intercept)   60261.83 71648.6\nrankAssocProf  8571.09 17244.1\nrankProf      38607.31 51524.7\ndisciplineB    9863.61 18971.6\nyrs.since.phd   -79.26  1149.4\nyrs.service   -1092.74   113.7\nsexMale          72.96  9494.0\n\n\n\n\n8.4 Step 4: Multiple testing adjustment\nIf we’re testing all six regressors simultaneously:\n\npvals_all &lt;- coeftest(mod_full, vcov. = vcovHC(mod_full, type = \"HC1\"))[-1, 4]\ndata.frame(\n  raw_p = round(pvals_all, 4),\n  holm_p = round(p.adjust(pvals_all, method = \"holm\"), 4),\n  significant_raw = ifelse(pvals_all &lt; 0.05, \"*\", \"\"),\n  significant_holm = ifelse(p.adjust(pvals_all, method = \"holm\") &lt; 0.05, \"*\", \"\")\n)\n\n               raw_p holm_p significant_raw significant_holm\nrankAssocProf 0.0000 0.0000               *                *\nrankProf      0.0000 0.0000               *                *\ndisciplineB   0.0000 0.0000               *                *\nyrs.since.phd 0.0876 0.1752                                 \nyrs.service   0.1114 0.1752                                 \nsexMale       0.0466 0.1397               *                 \n\n\n\n\n8.5 Step 5: Power assessment for the gender gap\nSuppose the “true” gender gap is $5,000 (about 5% of mean salary). Could we detect it?\n\nsigma_e &lt;- sigma(mod_full)\nsd_sex &lt;- sd(as.numeric(Salaries$sex == \"Male\"))\nn &lt;- nrow(Salaries)\n\npower_sex &lt;- power_ols(effect = 5000, sigma_e = sigma_e,\n                       sd_x = sd_sex, n = n, alpha = 0.05)\ncat(\"Estimated power to detect a $5,000 gender gap:\", round(power_sex, 3), \"\\n\")\n\nEstimated power to detect a $5,000 gender gap: 0.261 \n\n## What sample size for 80% power?\nn_needed &lt;- seq(100, 5000, by = 10)\npow_curve &lt;- sapply(n_needed, function(nn)\n  power_ols(5000, sigma_e, sd_sex, nn, 0.05))\nn80 &lt;- n_needed[which(pow_curve &gt;= 0.80)[1]]\ncat(\"Sample size for 80% power:\", n80, \"\\n\")\n\nSample size for 80% power: 1800"
  },
  {
    "objectID": "ch09-testing.html#practical-advice-after-hansen",
    "href": "ch09-testing.html#practical-advice-after-hansen",
    "title": "9. Hypothesis Testing",
    "section": "9 Practical advice (after Hansen)",
    "text": "9 Practical advice (after Hansen)\nA checklist for reporting regression results:\n\nReport standard errors, not just t-ratios. Standard errors convey precision; t-statistics reduce everything to a binary yes/no.\nReport p-values, not asterisks. The difference between \\(p = 0.049\\) and \\(p = 0.051\\) is not meaningful. Stars obscure this.\nFocus on substantive hypotheses. Don’t mechanically test every coefficient against zero. Test things that answer a scientific question.\n“Fail to reject” \\(\\neq\\) “accept.” Low power can explain non-rejection. Always ask: could we have detected a meaningful effect?\nStatistical significance \\(\\neq\\) economic significance. A tiny but significant coefficient may be policy-irrelevant. A large but insignificant coefficient may reflect low power.\nUse robust inference by default. There is rarely a good reason to report classical SEs when \\(n\\) is not tiny. Use HC1/HC2 or the bootstrap.\nAccount for multiple testing. If you report the “most significant” result from many tests, apply Holm or Bonferroni corrections."
  },
  {
    "objectID": "ch09-testing.html#connection-to-gmm",
    "href": "ch09-testing.html#connection-to-gmm",
    "title": "9. Hypothesis Testing",
    "section": "10 Connection to GMM",
    "text": "10 Connection to GMM\nThe Wald test has a natural GMM interpretation. In GMM, testing \\(H_0: \\mathbf{R}\\boldsymbol{\\beta} = \\mathbf{r}\\) is equivalent to testing whether the restricted moment conditions \\(E[\\mathbf{Z}'(Y - \\mathbf{X}\\boldsymbol{\\beta})] = \\mathbf{0}\\) with \\(\\mathbf{R}\\boldsymbol{\\beta} = \\mathbf{r}\\) are compatible with the data. The J-test of overidentifying restrictions, which we’ll meet in Chapter 11, generalizes the F-test to the setting where there are more moment conditions than parameters.\n\n\n\n\n\n\n\nOLS test\nGMM counterpart\n\n\n\n\nF-test (restricted vs. unrestricted)\nDistance test (restricted vs. unrestricted GMM)\n\n\nWald test\nWald test (same formula, different \\(\\hat{\\mathbf{V}}\\))\n\n\nScore / LM test\nScore test from restricted GMM\n\n\nOverall F-statistic\nJ-test of overidentifying restrictions"
  },
  {
    "objectID": "ch07-probit.html",
    "href": "ch07-probit.html",
    "title": "7. Probit and MLE",
    "section": "",
    "text": "Chapter 6 developed the likelihood for the normal linear model, where MLE turned out to be OLS. This chapter applies the same MLE toolkit to a genuinely nonlinear problem: modeling binary outcomes. We start with the linear probability model (LPM), motivate the probit link, write the probit likelihood and score by hand, implement Newton–Raphson, and compare everything to R’s built-in glm(). The running theme is that the score equation \\(\\sum s_i(\\beta) = 0\\) is just another moment condition — the same logic that gave us OLS, but now for a nonlinear model.\nQuestions this chapter answers:\nlibrary(ggplot2)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(estimatr)\nlibrary(carData)\noptions(digits = 4)\nWe use the Mroz (1987) dataset: 753 married women, with labor force participation (lfp) as the binary outcome.\ndata(Mroz)\nMroz$lfp_bin &lt;- as.integer(Mroz$lfp == \"yes\")\nmean(Mroz$lfp_bin)\n\n[1] 0.5684\nAbout 57% of women in the sample participate in the labor force."
  },
  {
    "objectID": "ch07-probit.html#the-linear-probability-model",
    "href": "ch07-probit.html#the-linear-probability-model",
    "title": "7. Probit and MLE",
    "section": "1 The linear probability model",
    "text": "1 The linear probability model\nWhen \\(Y \\in \\{0,1\\}\\), the conditional expectation is a probability:\n\\[E[Y \\mid X] = P(Y = 1 \\mid X)\\]\nThe best linear predictor (Chapter 2) still exists — OLS estimates it by solving the usual normal equations. This is the linear probability model (LPM):\n\nlpm &lt;- lm_robust(lfp_bin ~ k5 + k618 + age + wc + inc, data = Mroz, se_type = \"HC2\")\nsummary(lpm)\n\n\nCall:\nlm_robust(formula = lfp_bin ~ k5 + k618 + age + wc + inc, data = Mroz, \n    se_type = \"HC2\")\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper  DF\n(Intercept)  1.28867    0.11527   11.18 6.12e-27  1.06239  1.51495 747\nk5          -0.30255    0.03297   -9.18 4.28e-19 -0.36728 -0.23783 747\nk618        -0.01670    0.01462   -1.14 2.54e-01 -0.04540  0.01200 747\nage         -0.01320    0.00243   -5.43 7.77e-08 -0.01798 -0.00843 747\nwcyes        0.22065    0.03707    5.95 4.05e-09  0.14788  0.29341 747\ninc         -0.00627    0.00153   -4.10 4.50e-05 -0.00927 -0.00327 747\n\nMultiple R-squared:  0.131 ,    Adjusted R-squared:  0.125 \nF-statistic: 28.8 on 5 and 747 DF,  p-value: &lt;2e-16\n\n\nBinary outcomes have built-in heteroskedasticity: \\(\\text{Var}(Y \\mid X) = p(X)(1 - p(X))\\) depends on \\(X\\) by construction, so we use HC2 standard errors throughout.\n\n1.1 Predictions outside \\([0,1]\\)\nThe LPM is a linear function — nothing stops it from predicting probabilities below 0 or above 1:\n\nlpm_ols &lt;- lm(lfp_bin ~ k5 + k618 + age + wc + inc, data = Mroz)\np_hat_lpm &lt;- fitted(lpm_ols)\n\nc(min = min(p_hat_lpm), max = max(p_hat_lpm),\n  outside_01 = sum(p_hat_lpm &lt; 0 | p_hat_lpm &gt; 1))\n\n       min        max outside_01 \n    -0.286      1.024     11.000 \n\n\n\ndf_lpm &lt;- data.frame(fitted = p_hat_lpm, age = Mroz$age, y = Mroz$lfp_bin)\nggplot(df_lpm, aes(age, fitted)) +\n  geom_point(alpha = 0.3) +\n  geom_hline(yintercept = c(0, 1), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"LPM fitted values vs. age\",\n       subtitle = \"Dashed lines mark 0 and 1 --- LPM can exceed these bounds\",\n       y = \"P̂(lfp = 1)\")\n\n\n\n\nLPM fitted values vs. age — dashed lines mark the [0, 1] bounds\n\n\n\n\nThis motivates models that constrain predictions to \\([0, 1]\\).\n\n\n\n\n\n\nWarningLPM Predictions Outside [0, 1]\n\n\n\nThe linear probability model can predict probabilities below 0 or above 1 because it fits a line through binary data. While this doesn’t affect the interpretation of coefficients as average marginal effects, it limits the model’s use for prediction."
  },
  {
    "objectID": "ch07-probit.html#the-probit-model",
    "href": "ch07-probit.html#the-probit-model",
    "title": "7. Probit and MLE",
    "section": "2 The probit model",
    "text": "2 The probit model\n\n2.1 Latent variable representation\nSuppose there is an unobserved continuous variable:\n\\[Y_i^* = X_i'\\beta + \\varepsilon_i, \\qquad \\varepsilon_i \\sim N(0, 1)\\]\nWe observe \\(Y_i = \\mathbf{1}\\{Y_i^* &gt; 0\\}\\). Then:\n\\[P(Y_i = 1 \\mid X_i) = P(\\varepsilon_i &gt; -X_i'\\beta) = \\Phi(X_i'\\beta)\\]\nwhere \\(\\Phi\\) is the standard normal CDF. This is the probit model — a single-index model with link function \\(G = \\Phi\\).\n\nDefinition 1 (Probit Model) The probit model specifies \\(P(Y_i = 1 | X_i) = \\Phi(X_i'\\beta)\\), where \\(\\Phi\\) is the standard normal CDF. It arises from a latent variable \\(Y_i^* = X_i'\\beta + \\varepsilon_i\\) with \\(\\varepsilon_i \\sim N(0,1)\\) and \\(Y_i = \\mathbf{1}\\{Y_i^* &gt; 0\\}\\).\n\n\n\n2.2 Fitting probit with glm()\n\nprobit &lt;- glm(lfp_bin ~ k5 + k618 + age + wc + inc,\n              data = Mroz, family = binomial(link = \"probit\"))\nsummary(probit)\n\n\nCall:\nglm(formula = lfp_bin ~ k5 + k618 + age + wc + inc, family = binomial(link = \"probit\"), \n    data = Mroz)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.28263    0.36748    6.21  5.2e-10 ***\nk5          -0.87865    0.11331   -7.75  8.9e-15 ***\nk618        -0.05185    0.04055   -1.28      0.2    \nage         -0.03814    0.00749   -5.09  3.5e-07 ***\nwcyes        0.63741    0.11742    5.43  5.7e-08 ***\ninc         -0.01850    0.00457   -4.05  5.1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1029.75  on 752  degrees of freedom\nResidual deviance:  923.04  on 747  degrees of freedom\nAIC: 935\n\nNumber of Fisher Scoring iterations: 4\n\n\nglm() uses Fisher scoring (iteratively reweighted least squares), which is a variant of Newton–Raphson. Below, we implement this from scratch.\n\n\n2.3 Identification and scale\nAn important difference from OLS: probit coefficients are identified only up to scale. If we allowed \\(\\text{Var}(\\varepsilon) = \\sigma^2\\) instead of normalizing to 1, then \\(P(Y = 1 \\mid X) = \\Phi(X'\\beta/\\sigma)\\) — only \\(\\beta/\\sigma\\) is identified. The normalization \\(\\sigma = 1\\) pins down the scale, but it means \\(\\beta_j\\) does not have the same interpretation as in OLS. We return to this in the marginal effects section."
  },
  {
    "objectID": "ch07-probit.html#writing-the-probit-likelihood-by-hand",
    "href": "ch07-probit.html#writing-the-probit-likelihood-by-hand",
    "title": "7. Probit and MLE",
    "section": "3 Writing the probit likelihood by hand",
    "text": "3 Writing the probit likelihood by hand\nThe probit log-likelihood is:\n\\[\\ell_n(\\beta) = \\sum_{i=1}^n \\left[ Y_i \\log \\Phi(X_i'\\beta) + (1 - Y_i) \\log(1 - \\Phi(X_i'\\beta)) \\right] \\tag{1}\\]\n\ny &lt;- Mroz$lfp_bin\nX &lt;- model.matrix(~ k5 + k618 + age + wc + inc, data = Mroz)\nn &lt;- nrow(X)\nK &lt;- ncol(X)\n\n# Log-likelihood\nprobit_loglik &lt;- function(beta) {\n  Xb &lt;- as.numeric(X %*% beta)\n  Phi &lt;- pnorm(Xb)\n  # Clamp to avoid log(0)\n  Phi &lt;- pmax(pmin(Phi, 1 - 1e-10), 1e-10)\n  sum(y * log(Phi) + (1 - y) * log(1 - Phi))\n}\n\n# Evaluate at glm's estimate\nprobit_loglik(coef(probit))\n\n[1] -461.5\n\nlogLik(probit)\n\n'log Lik.' -461.5 (df=6)\n\n\n\n3.1 Profile likelihood\nWe can visualize the likelihood as a function of a single coefficient, holding the others at their MLE values:\n\nbeta_hat &lt;- coef(probit)\nj &lt;- 2  # k5 coefficient\n\nb_grid &lt;- seq(beta_hat[j] - 1, beta_hat[j] + 1, length.out = 200)\nll_profile &lt;- numeric(length(b_grid))\nfor (i in seq_along(b_grid)) {\n  beta_try &lt;- beta_hat\n  beta_try[j] &lt;- b_grid[i]\n  ll_profile[i] &lt;- probit_loglik(beta_try)\n}\n\ndf_profile &lt;- data.frame(beta_k5 = b_grid, loglik = ll_profile)\nggplot(df_profile, aes(beta_k5, loglik)) +\n  geom_line(linewidth = 1) +\n  geom_vline(xintercept = beta_hat[j], linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Profile log-likelihood for β_k5\",\n       subtitle = paste(\"MLE =\", round(beta_hat[j], 3)),\n       x = expression(beta[k5]), y = \"Log-likelihood\")"
  },
  {
    "objectID": "ch07-probit.html#the-score-function",
    "href": "ch07-probit.html#the-score-function",
    "title": "7. Probit and MLE",
    "section": "4 The score function",
    "text": "4 The score function\nRecall the score and Fisher information from Chapter 6. The individual score contribution for the probit model is:\n\\[s_i(\\beta) = \\frac{\\phi(X_i'\\beta)}{\\Phi(X_i'\\beta)(1 - \\Phi(X_i'\\beta))} (Y_i - \\Phi(X_i'\\beta)) X_i \\tag{2}\\]\nwhere \\(\\phi\\) is the standard normal pdf. The weight \\(w_i = \\phi/({\\Phi(1 - \\Phi)})\\) upweights observations near \\(X'\\beta = 0\\) (where the likelihood is most informative) and downweights observations deep in the tails.\n\n# Score function (returns a K-vector)\nprobit_score &lt;- function(beta) {\n  Xb &lt;- as.numeric(X %*% beta)\n  Phi &lt;- pnorm(Xb)\n  phi &lt;- dnorm(Xb)\n  Phi &lt;- pmax(pmin(Phi, 1 - 1e-10), 1e-10)\n  w &lt;- phi / (Phi * (1 - Phi))\n  resid_w &lt;- w * (y - Phi)\n  as.numeric(t(X) %*% resid_w)\n}\n\n# At the MLE, the score should be zero\nprobit_score(coef(probit))\n\n[1] 1.798e-04 7.338e-05 6.640e-04 6.994e-03 1.101e-04 7.178e-03\n\n\nAll elements are numerically zero — confirming that glm() found the score root.\n\n4.1 Comparing estimating equations\nSetting the score to zero defines the MLE. Compare this to the OLS normal equation:\n\n\n\n\n\n\n\n\nModel\nEstimating equation\nWeights\n\n\n\n\nOLS\n\\(\\sum_i X_i(Y_i - X_i'\\beta) = 0\\)\nEqual\n\n\nProbit MLE\n\\(\\sum_i w_i(Y_i - \\Phi(X_i'\\beta)) X_i = 0\\)\n\\(w_i = \\phi/(\\Phi(1-\\Phi))\\)\n\n\nGeneral\n\\(\\sum_i g_i(\\theta) = 0\\)\n\\(\\to\\) GMM\n\n\n\nBoth OLS and probit MLE are method-of-moments estimators — they set sample moment conditions to zero. GMM (Chapter 13) is the general framework.\n\n\n\n\n\n\nTipThe Probit Score Is a Moment Condition\n\n\n\nSetting the probit score to zero, \\(\\sum w_i(Y_i - \\Phi(X_i'\\beta))X_i = 0\\), is a method-of-moments estimating equation — just like OLS normal equations but with observation-specific weights \\(w_i\\). This connection to GMM (Chapter 11) unifies all the estimators in this course."
  },
  {
    "objectID": "ch07-probit.html#newtonraphson-from-scratch",
    "href": "ch07-probit.html#newtonraphson-from-scratch",
    "title": "7. Probit and MLE",
    "section": "5 Newton–Raphson from scratch",
    "text": "5 Newton–Raphson from scratch\nThe Hessian (second derivative of the log-likelihood) is:\n\n# Hessian function (returns a K x K matrix)\nprobit_hessian &lt;- function(beta) {\n  Xb &lt;- as.numeric(X %*% beta)\n  Phi &lt;- pnorm(Xb)\n  phi &lt;- dnorm(Xb)\n  Phi &lt;- pmax(pmin(Phi, 1 - 1e-10), 1e-10)\n\n  # Weight for the Hessian\n  lambda &lt;- phi / (Phi * (1 - Phi))\n  d &lt;- lambda * (lambda + Xb)  # diagonal weights\n  # Negative Hessian contribution from each obs\n  w_hess &lt;- -(phi^2) / (Phi * (1 - Phi))^2 *\n            ((y - Phi) * (-Xb * Phi * (1 - Phi) - phi * (1 - 2 * Phi)) -\n             phi^2) / (1)\n\n  # Simpler: use the expected Hessian (Fisher scoring)\n  w_fisher &lt;- phi^2 / (Phi * (1 - Phi))\n  -t(X) %*% diag(w_fisher) %*% X\n}\n\nNewton–Raphson iterates: \\(\\beta^{(t+1)} = \\beta^{(t)} - [H_n(\\beta^{(t)})]^{-1} S_n(\\beta^{(t)})\\).\n\n# Newton-Raphson (using Fisher scoring: expected Hessian)\nnewton_raphson_probit &lt;- function(X, y, tol = 1e-8, max_iter = 50) {\n  n &lt;- nrow(X)\n  K &lt;- ncol(X)\n\n  # Initialize with OLS\n  beta &lt;- solve(crossprod(X)) %*% crossprod(X, y)\n  beta &lt;- as.numeric(beta)\n\n  for (iter in 1:max_iter) {\n    Xb &lt;- as.numeric(X %*% beta)\n    Phi &lt;- pnorm(Xb)\n    phi &lt;- dnorm(Xb)\n    Phi &lt;- pmax(pmin(Phi, 1 - 1e-10), 1e-10)\n\n    # Score\n    w &lt;- phi / (Phi * (1 - Phi))\n    S &lt;- as.numeric(t(X) %*% (w * (y - Phi)))\n\n    # Expected Hessian (Fisher scoring)\n    W &lt;- phi^2 / (Phi * (1 - Phi))\n    H &lt;- -t(X) %*% diag(W) %*% X\n\n    # Update\n    delta &lt;- -solve(H) %*% S\n    beta &lt;- beta + as.numeric(delta)\n\n    if (max(abs(delta)) &lt; tol) {\n      return(list(coefficients = beta, iterations = iter,\n                  converged = TRUE, hessian = H))\n    }\n  }\n  list(coefficients = beta, iterations = max_iter, converged = FALSE, hessian = H)\n}\n\nfit_nr &lt;- newton_raphson_probit(X, y)\nfit_nr$iterations\n\n[1] 7\n\n\n\n# Compare our Newton-Raphson to glm()\ndata.frame(\n  Variable = colnames(X),\n  NR = round(fit_nr$coefficients, 6),\n  glm = round(coef(probit), 6),\n  diff = round(fit_nr$coefficients - coef(probit), 10)\n)\n\n               Variable       NR      glm       diff\n(Intercept) (Intercept)  2.28263  2.28263 -2.450e-06\nk5                   k5 -0.87865 -0.87865  3.073e-07\nk618               k618 -0.05185 -0.05185  5.738e-07\nage                 age -0.03814 -0.03814  1.780e-08\nwcyes             wcyes  0.63741  0.63741  3.525e-07\ninc                 inc -0.01850 -0.01850  5.800e-08\n\n\nOur implementation matches glm() to many decimal places, converging in just a handful of iterations.\n\n5.1 Visualizing convergence\n\n# Track convergence\nbeta_path &lt;- matrix(NA, 20, K)\nbeta &lt;- as.numeric(solve(crossprod(X)) %*% crossprod(X, y))\nbeta_path[1, ] &lt;- beta\n\nfor (iter in 2:20) {\n  Xb &lt;- as.numeric(X %*% beta)\n  Phi &lt;- pnorm(Xb)\n  phi &lt;- dnorm(Xb)\n  Phi &lt;- pmax(pmin(Phi, 1 - 1e-10), 1e-10)\n\n  w &lt;- phi / (Phi * (1 - Phi))\n  S &lt;- as.numeric(t(X) %*% (w * (y - Phi)))\n  W &lt;- phi^2 / (Phi * (1 - Phi))\n  H &lt;- -t(X) %*% diag(W) %*% X\n\n  delta &lt;- -solve(H) %*% S\n  beta &lt;- beta + as.numeric(delta)\n  beta_path[iter, ] &lt;- beta\n  if (max(abs(delta)) &lt; 1e-10) break\n}\n\n# Distance from MLE at each iteration\ndist_to_mle &lt;- apply(beta_path, 1, function(b) {\n  if (any(is.na(b))) return(NA)\n  sqrt(sum((b - coef(probit))^2))\n})\ndist_to_mle &lt;- dist_to_mle[!is.na(dist_to_mle)]\n\ndf_conv &lt;- data.frame(iteration = seq_along(dist_to_mle), distance = dist_to_mle)\nggplot(df_conv, aes(iteration, distance)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  scale_y_log10() +\n  labs(title = \"Newton-Raphson convergence\",\n       subtitle = \"Distance to MLE on log scale --- quadratic convergence\",\n       y = \"||β(t) - β̂|| (log scale)\")\n\n\n\n\n\n\n\n\nNewton–Raphson converges quadratically: the number of correct digits roughly doubles each iteration.\n\n\n\n\n\n\nNoteNewton-Raphson Exploits Concavity\n\n\n\nThe probit log-likelihood is globally concave, guaranteeing that Newton-Raphson (or Fisher scoring) converges to the unique maximum from any starting point. Quadratic convergence means the number of correct digits roughly doubles each iteration."
  },
  {
    "objectID": "ch07-probit.html#sec-marginal-effects",
    "href": "ch07-probit.html#sec-marginal-effects",
    "title": "7. Probit and MLE",
    "section": "6 Marginal effects",
    "text": "6 Marginal effects\nIn OLS, \\(\\partial E[Y \\mid X] / \\partial x_j = \\beta_j\\) — the coefficient is the marginal effect. In probit:\n\\[\\frac{\\partial P(Y = 1 \\mid X)}{\\partial x_j} = \\phi(X'\\beta) \\cdot \\beta_j\\]\nThe marginal effect depends on \\(X\\) through \\(\\phi(X'\\beta)\\). Two standard summaries:\n\nDefinition 2 (Marginal Effects) In the probit model, \\(\\partial P(Y=1|X)/\\partial x_j = \\phi(X'\\beta) \\cdot \\beta_j\\). The marginal effect at the mean (MEM) evaluates at \\(\\bar{X}\\); the average marginal effect (AME) averages over all observations. AME is comparable to the LPM coefficient.\n\n\nMarginal effect at the mean (MEM): evaluate at \\(\\bar{X}\\)\nAverage marginal effect (AME): average the marginal effect over all observations\n\n\nXb_hat &lt;- as.numeric(X %*% coef(probit))\n\n# Marginal effect at the mean\nphi_at_mean &lt;- dnorm(mean(Xb_hat))\nmem &lt;- phi_at_mean * coef(probit)\n\n# Average marginal effect\nphi_each &lt;- dnorm(Xb_hat)\name &lt;- colMeans(outer(phi_each, coef(probit)))\n\n# Compare to LPM coefficients\ndata.frame(\n  Variable = names(coef(probit)),\n  LPM = round(coef(lpm_ols), 4),\n  Probit = round(coef(probit), 4),\n  MEM = round(mem, 4),\n  AME = round(ame, 4)\n)\n\n               Variable     LPM  Probit     MEM     AME\n(Intercept) (Intercept)  1.2887  2.2826  0.8946  0.7986\nk5                   k5 -0.3026 -0.8787 -0.3444 -0.3074\nk618               k618 -0.0167 -0.0519 -0.0203 -0.0181\nage                 age -0.0132 -0.0381 -0.0149 -0.0133\nwcyes             wcyes  0.2206  0.6374  0.2498  0.2230\ninc                 inc -0.0063 -0.0185 -0.0073 -0.0065\n\n\nThe AME is directly comparable to the LPM coefficient: both estimate the average change in \\(P(Y=1)\\) per unit change in \\(X_j\\). They are generally similar, with differences reflecting the curvature of \\(\\Phi\\).\n\n6.1 Where marginal effects differ most\n\n# Marginal effect of age as a function of the index\nxb_grid &lt;- seq(-3, 3, length.out = 200)\nme_age &lt;- dnorm(xb_grid) * coef(probit)[\"age\"]\n\ndf_me &lt;- data.frame(index = xb_grid, me = me_age)\nggplot(df_me, aes(index, me)) +\n  geom_line(linewidth = 1) +\n  geom_hline(yintercept = coef(lpm_ols)[\"age\"], linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Marginal effect of age on P(lfp = 1)\",\n       subtitle = \"Probit ME (black) vs. LPM constant effect (red dashed)\",\n       x = \"X'β (probit index)\", y = \"∂P/∂age\")\n\n\n\n\n\n\n\n\nThe probit marginal effect is largest near \\(X'\\beta = 0\\) (where the CDF is steepest) and vanishes in the tails. The LPM assumes a constant effect everywhere."
  },
  {
    "objectID": "ch07-probit.html#fisher-information-and-standard-errors",
    "href": "ch07-probit.html#fisher-information-and-standard-errors",
    "title": "7. Probit and MLE",
    "section": "7 Fisher information and standard errors",
    "text": "7 Fisher information and standard errors\n\n7.1 Model-based standard errors\nUnder correct specification, the information matrix equality holds:\n\\[\\mathcal{I}(\\beta) = E[s_i s_i'] = -E\\left[\\frac{\\partial^2 \\ell_i}{\\partial \\beta \\partial \\beta'}\\right]\\]\nThe asymptotic variance of \\(\\hat{\\beta}\\) is \\(\\mathcal{I}(\\beta)^{-1}/n\\), estimated by inverting the negative Hessian:\n\n# Model-based variance: inverse of negative Hessian\nV_model &lt;- solve(-fit_nr$hessian)\nse_model &lt;- sqrt(diag(V_model))\n\n# Compare to glm's reported SEs\ndata.frame(\n  Variable = colnames(X),\n  SE_manual = round(se_model, 5),\n  SE_glm = round(summary(probit)$coefficients[, \"Std. Error\"], 5)\n)\n\n               Variable SE_manual  SE_glm\n(Intercept) (Intercept)   0.36748 0.36748\nk5                   k5   0.11331 0.11331\nk618               k618   0.04055 0.04055\nage                 age   0.00749 0.00749\nwcyes             wcyes   0.11742 0.11742\ninc                 inc   0.00457 0.00457\n\n\n\n\n7.2 Sandwich standard errors\nIf the probit model is misspecified (wrong link function), the information matrix equality fails and we need the sandwich:\n\\[V_{\\text{sandwich}} = H^{-1} \\left(\\sum_i s_i s_i'\\right) H^{-1}\\]\n\n# Robust SEs\ncoeftest(probit)                                       # model-based\n\n\nz test of coefficients:\n\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.28263    0.36748    6.21  5.2e-10 ***\nk5          -0.87865    0.11331   -7.75  8.9e-15 ***\nk618        -0.05185    0.04055   -1.28      0.2    \nage         -0.03814    0.00749   -5.09  3.5e-07 ***\nwcyes        0.63741    0.11742    5.43  5.7e-08 ***\ninc         -0.01850    0.00457   -4.05  5.1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncoeftest(probit, vcov = vcovHC(probit, type = \"HC1\"))  # sandwich\n\n\nz test of coefficients:\n\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.28263    0.37312    6.12  9.5e-10 ***\nk5          -0.87865    0.11725   -7.49  6.7e-14 ***\nk618        -0.05185    0.04359   -1.19  0.23425    \nage         -0.03814    0.00755   -5.05  4.4e-07 ***\nwcyes        0.63741    0.11764    5.42  6.0e-08 ***\ninc         -0.01850    0.00493   -3.75  0.00018 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nse_classical &lt;- summary(probit)$coefficients[, \"Std. Error\"]\nse_robust &lt;- sqrt(diag(vcovHC(probit, type = \"HC1\")))\n\ndata.frame(\n  Variable = names(coef(probit)),\n  Model_based = round(se_classical, 4),\n  Sandwich = round(se_robust, 4),\n  Ratio = round(se_robust / se_classical, 3)\n)\n\n               Variable Model_based Sandwich Ratio\n(Intercept) (Intercept)      0.3675   0.3731 1.015\nk5                   k5      0.1133   0.1172 1.035\nk618               k618      0.0406   0.0436 1.075\nage                 age      0.0075   0.0076 1.009\nwcyes             wcyes      0.1174   0.1176 1.002\ninc                 inc      0.0046   0.0049 1.079\n\n\nIf model-based and sandwich SEs differ substantially, this signals possible misspecification of the link function. Here they are similar, which is reassuring."
  },
  {
    "objectID": "ch07-probit.html#lpm-vs.-probit-applied-comparison",
    "href": "ch07-probit.html#lpm-vs.-probit-applied-comparison",
    "title": "7. Probit and MLE",
    "section": "8 LPM vs. probit: applied comparison",
    "text": "8 LPM vs. probit: applied comparison\n\n8.1 Fitted probabilities\n\np_probit &lt;- predict(probit, type = \"response\")\n\ndf_compare &lt;- data.frame(\n  lpm = p_hat_lpm,\n  probit = p_probit,\n  y = Mroz$lfp_bin\n)\n\nggplot(df_compare, aes(lpm, probit)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"LPM vs. Probit fitted probabilities\",\n       subtitle = \"Close agreement in the interior, diverge near 0 and 1\",\n       x = \"P̂ (LPM)\", y = \"P̂ (Probit)\")\n\n\n\n\n\n\n\n\n\n# Summary comparison\ndata.frame(\n  Model = c(\"LPM\", \"Probit\"),\n  LogLik = c(sum(dbinom(y, 1, pmax(pmin(p_hat_lpm, 1-1e-10), 1e-10), log = TRUE)),\n             as.numeric(logLik(probit))),\n  AIC = c(NA, AIC(probit)),\n  Predictions_outside_01 = c(sum(p_hat_lpm &lt; 0 | p_hat_lpm &gt; 1), 0)\n)\n\n   Model LogLik AIC Predictions_outside_01\n1    LPM -462.6  NA                     11\n2 Probit -461.5 935                      0\n\n\n\n\n8.2 When does the choice matter?\nThe LPM and probit give nearly identical results when predicted probabilities are far from 0 and 1. They diverge most when:\n\nThe outcome is rare (or nearly universal)\nPredictions reach the boundaries\nYou need to interpret the functional form (e.g., for counterfactual predictions)\n\nFor estimating average marginal effects, the LPM is a robust baseline that doesn’t require correct specification of the link function."
  },
  {
    "objectID": "ch07-probit.html#logit-the-main-alternative",
    "href": "ch07-probit.html#logit-the-main-alternative",
    "title": "7. Probit and MLE",
    "section": "9 Logit: the main alternative",
    "text": "9 Logit: the main alternative\nThe logit model uses the logistic CDF \\(\\Lambda(z) = e^z/(1 + e^z)\\) instead of \\(\\Phi\\):\n\nlogit &lt;- glm(lfp_bin ~ k5 + k618 + age + wc + inc,\n             data = Mroz, family = binomial(link = \"logit\"))\n\n# Probit and logit fitted values are nearly identical\nmax(abs(predict(probit, type = \"response\") - predict(logit, type = \"response\")))\n\n[1] 0.01162\n\n\n\n# Coefficients differ by approximately a factor of 1.6\ndata.frame(\n  Variable = names(coef(probit)),\n  Probit = round(coef(probit), 4),\n  Logit = round(coef(logit), 4),\n  Ratio = round(coef(logit) / coef(probit), 3)\n)\n\n               Variable  Probit   Logit Ratio\n(Intercept) (Intercept)  2.2826  3.7999 1.665\nk5                   k5 -0.8787 -1.4631 1.665\nk618               k618 -0.0519 -0.0876 1.689\nage                 age -0.0381 -0.0637 1.669\nwcyes             wcyes  0.6374  1.0623 1.667\ninc                 inc -0.0185 -0.0308 1.663\n\n\nThe ratio of logit to probit coefficients is approximately \\(\\pi/\\sqrt{3} \\approx 1.81\\), because the logistic distribution has variance \\(\\pi^2/3\\) while the standard normal has variance 1. The fitted probabilities are nearly indistinguishable."
  },
  {
    "objectID": "ch07-probit.html#simulation-verifying-asymptotic-normality",
    "href": "ch07-probit.html#simulation-verifying-asymptotic-normality",
    "title": "7. Probit and MLE",
    "section": "10 Simulation: verifying asymptotic normality",
    "text": "10 Simulation: verifying asymptotic normality\nThe MLE theory from Chapter 6 says \\(\\sqrt{n}(\\hat{\\beta} - \\beta_0) \\to N(0, \\mathcal{I}^{-1})\\). Let’s verify with a controlled simulation:\n\nset.seed(42)\nB &lt;- 2000\nn_sim &lt;- 500\nbeta_true &lt;- c(0.3, -0.5, 0.8)  # intercept, x1, x2\nK_sim &lt;- length(beta_true)\n\nb_sim &lt;- matrix(NA, B, K_sim)\nfor (b in 1:B) {\n  x1 &lt;- rnorm(n_sim)\n  x2 &lt;- rnorm(n_sim)\n  X_sim &lt;- cbind(1, x1, x2)\n  p_sim &lt;- pnorm(X_sim %*% beta_true)\n  y_sim &lt;- rbinom(n_sim, 1, p_sim)\n  fit &lt;- glm(y_sim ~ x1 + x2, family = binomial(link = \"probit\"))\n  b_sim[b, ] &lt;- coef(fit)\n}\n\n# Focus on x1 coefficient\nj_sim &lt;- 2\nc(true = beta_true[j_sim],\n  mean_bhat = mean(b_sim[, j_sim]),\n  sd_bhat = sd(b_sim[, j_sim]))\n\n     true mean_bhat   sd_bhat \n -0.50000  -0.50775   0.07339 \n\n\n\ndf_sim &lt;- data.frame(b_hat = b_sim[, j_sim])\nggplot(df_sim, aes(b_hat)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.5) +\n  stat_function(fun = dnorm,\n                args = list(mean = mean(b_sim[, j_sim]),\n                            sd = sd(b_sim[, j_sim])),\n                color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = beta_true[j_sim], linetype = \"dashed\") +\n  labs(title = \"Sampling distribution of probit MLE\",\n       subtitle = paste(\"n =\", n_sim, \"— centered on true β, approximately normal\"),\n       x = expression(hat(beta)[x1]))\n\n\n\n\n\n\n\n\nThe MLE is centered on the true value (unbiased in large samples) and approximately normal, as the theory predicts."
  },
  {
    "objectID": "ch07-probit.html#prediction-and-classification",
    "href": "ch07-probit.html#prediction-and-classification",
    "title": "7. Probit and MLE",
    "section": "11 Prediction and classification",
    "text": "11 Prediction and classification\n\n# Predicted probabilities\np_pred &lt;- predict(probit, type = \"response\")\n\n# Classify at threshold 0.5\ny_pred &lt;- as.integer(p_pred &gt; 0.5)\n\n# Confusion matrix\ntable(Predicted = y_pred, Actual = Mroz$lfp_bin)\n\n         Actual\nPredicted   0   1\n        0 166  83\n        1 159 345\n\n\n\n# Accuracy\nmean(y_pred == Mroz$lfp_bin)\n\n[1] 0.6786\n\n\nA simple 50% threshold gives reasonable classification. But the primary goal of probit in econometrics is estimating the effect of covariates on \\(P(Y=1)\\), not prediction."
  },
  {
    "objectID": "ch07-probit.html#connecting-to-gmm",
    "href": "ch07-probit.html#connecting-to-gmm",
    "title": "7. Probit and MLE",
    "section": "12 Connecting to GMM",
    "text": "12 Connecting to GMM\nThe probit score equation, the OLS normal equation, and the GLS estimating equation are all special cases of:\n\\[\\frac{1}{n} \\sum_{i=1}^n g(W_i, \\theta) = 0\\]\n\n\n\nEstimator\nMoment condition \\(g(W_i, \\theta)\\)\n\n\n\n\nOLS\n\\(X_i(Y_i - X_i'\\beta)\\)\n\n\nWLS\n\\(W_i^{1/2} X_i(Y_i - X_i'\\beta)\\)\n\n\nProbit MLE\n\\(w_i(Y_i - \\Phi(X_i'\\beta)) X_i\\)\n\n\n\n\n# All three are sample moment conditions that equal zero at the estimate\n\n# OLS\nmax(abs(t(X) %*% (y - X %*% coef(lpm_ols))))\n\n[1] 2.976e-10\n\n# Probit score\nmax(abs(probit_score(coef(probit))))\n\n[1] 0.007178\n\n\nWhen there are more moment conditions than parameters, the generalized method of moments (Chapter 13) provides the efficient way to combine them."
  },
  {
    "objectID": "ch07-probit.html#summary",
    "href": "ch07-probit.html#summary",
    "title": "7. Probit and MLE",
    "section": "13 Summary",
    "text": "13 Summary\n\n\n\n\n\n\n\n\nConcept\nFormula\nR code\n\n\n\n\nLinear probability model\n\\(P(Y=1\\mid X) = X'\\beta\\)\nlm_robust(y ~ x, se_type = \"HC2\")\n\n\nProbit model\n\\(P(Y=1\\mid X) = \\Phi(X'\\beta)\\)\nglm(y ~ x, family = binomial(link = \"probit\"))\n\n\nLog-likelihood\n\\(\\sum[Y\\log\\Phi + (1-Y)\\log(1-\\Phi)]\\)\nlogLik(probit)\n\n\nScore\n\\(\\sum w_i(Y_i - \\Phi(X_i'\\beta))X_i = 0\\)\nmanual (see above)\n\n\nNewton–Raphson\n\\(\\beta^{(t+1)} = \\beta^{(t)} - H^{-1}S\\)\nglm() does this internally\n\n\nAverage marginal effect\n\\(\\frac{1}{n}\\sum \\phi(X_i'\\hat\\beta)\\hat\\beta_j\\)\nmean(dnorm(Xb)) * coef(probit)\n\n\nModel-based SEs\n\\(\\sqrt{[-H_n(\\hat\\beta)]^{-1}_{jj}}\\)\nsummary(probit)\n\n\nSandwich SEs\n\\(\\sqrt{[H^{-1}(\\sum s_is_i')H^{-1}]_{jj}}\\)\nvcovHC(probit, type = \"HC1\")\n\n\nLogit\n\\(P(Y=1\\mid X) = \\Lambda(X'\\beta)\\)\nfamily = binomial(link = \"logit\")\n\n\n\nKey takeaway. The probit model is the natural extension of MLE to binary outcomes. The coefficients are not marginal effects (compute AME instead), the score equation is a moment condition just like OLS normal equations, and sandwich SEs protect against misspecification of the link function. The LPM remains a robust baseline for estimating average effects."
  },
  {
    "objectID": "ch03-ols.html",
    "href": "ch03-ols.html",
    "title": "3. Multivariate OLS",
    "section": "",
    "text": "library(ggplot2)\nlibrary(carData)\noptions(digits = 3)\ntr &lt;- function(M) sum(diag(M))  # R has no built-in trace\nThe OLS estimator \\(\\hat\\beta = (X'X)^{-1}X'y\\) is a matrix formula. Every piece of it — the transpose, the product, the inverse — has a direct R function. This chapter builds OLS from those building blocks, first geometrically in two dimensions, then in full matrix form with real data.\nQuestions this chapter answers:"
  },
  {
    "objectID": "ch03-ols.html#rs-matrix-toolkit",
    "href": "ch03-ols.html#rs-matrix-toolkit",
    "title": "3. Multivariate OLS",
    "section": "1 R’s matrix toolkit",
    "text": "1 R’s matrix toolkit\nBefore deriving anything, here are the operations we’ll use throughout.\n\nA &lt;- matrix(c(2, 1, 1, 3), nrow = 2)\nA\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    1    3\n\n# 1. Transpose: t()\nt(A)\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    1    3\n\n# 2. Matrix multiply: %*%  (not * which is element-wise)\nB &lt;- matrix(c(1, 0, -1, 2), nrow = 2)\nA %*% B\n\n     [,1] [,2]\n[1,]    2    0\n[2,]    1    5\n\n# 3. Inverse: solve()\nsolve(A)\n\n     [,1] [,2]\n[1,]  0.6 -0.2\n[2,] -0.2  0.4\n\nA %*% solve(A)  # identity\n\n          [,1] [,2]\n[1,]  1.00e+00    0\n[2,] -1.11e-16    1\n\n# 4. Trace: sum(diag())\ntr(A)   # sum of diagonal elements\n\n[1] 5\n\n# 5. Eigendecomposition: eigen()\neigen(A)\n\neigen() decomposition\n$values\n[1] 3.62 1.38\n\n$vectors\n      [,1]   [,2]\n[1,] 0.526 -0.851\n[2,] 0.851  0.526\n\n# 6. Determinant: det()\ndet(A)\n\n[1] 5\n\n# 7. Cross product shortcuts\nx_vec &lt;- c(1, 2, 3)\ny_vec &lt;- c(4, 5, 6)\nc(manual = sum(x_vec * y_vec),\n  crossprod = as.numeric(crossprod(x_vec, y_vec)))\n\n   manual crossprod \n       32        32 \n\n\ncrossprod(X) computes \\(X'X\\) faster than t(X) %*% X, and crossprod(X, y) computes \\(X'y\\). We’ll use these constantly."
  },
  {
    "objectID": "ch03-ols.html#geometry-projection-in-two-dimensions",
    "href": "ch03-ols.html#geometry-projection-in-two-dimensions",
    "title": "3. Multivariate OLS",
    "section": "2 Geometry: projection in two dimensions",
    "text": "2 Geometry: projection in two dimensions\nYou’re used to plotting data with variables on the axes — one axis for \\(X\\), one for \\(Y\\), and each point is an observation. The geometric view of regression flips this: each axis is an observation, and each variable is a vector. A variable with \\(n\\) observations is a vector in \\(\\mathbb{R}^n\\).\nWhy think this way? Because regression asks: among all scalar multiples of \\(\\mathbf{x}\\) (all predictions of the form \\(b\\mathbf{x}\\)), which one is closest to \\(\\mathbf{y}\\)? That’s a projection — dropping a perpendicular from \\(\\mathbf{y}\\) onto the line spanned by \\(\\mathbf{x}\\).\nWith just two observations, we can see this on a page. Suppose we survey two students: we record how long each spent on homework (\\(\\mathbf{h}\\)) and how long reading the textbook (\\(\\mathbf{t}\\)). Each variable is a 2-vector, and we can plot both in \\(\\mathbb{R}^2\\):\n\n# Student 1: 3 hrs homework, 2.5 hrs reading\n# Student 2: 5 hrs homework, 2 hrs reading\nh &lt;- c(3, 5)    # outcome: homework time\ntt &lt;- c(2.5, 2)  # predictor: reading time\n\nWe want to predict \\(\\mathbf{h}\\) using \\(\\mathbf{t}\\): find \\(b\\) so that \\(b\\mathbf{t}\\) is as close to \\(\\mathbf{h}\\) as possible. Geometrically, \\(b\\mathbf{t}\\) must lie on the line through \\(\\mathbf{t}\\) (the dotted line in the plot below), and the best choice is the one where the “miss” — the residual — is perpendicular to that line.\nThe vector projection of \\(\\mathbf{h}\\) onto \\(\\mathbf{t}\\) solves this:\n\\[\\hat{\\mathbf{h}} = \\frac{\\mathbf{t} \\cdot \\mathbf{h}}{\\mathbf{t} \\cdot \\mathbf{t}} \\mathbf{t}\\]\nThe scalar \\(b = \\frac{\\mathbf{t} \\cdot \\mathbf{h}}{\\mathbf{t} \\cdot \\mathbf{t}}\\) minimizes \\(\\|\\mathbf{h} - b\\mathbf{t}\\|^2\\) — the sum of squared errors. This is the least squares solution, derived purely from geometry.\n\n# The projection coefficient\nb &lt;- as.numeric(crossprod(tt, h) / crossprod(tt))\nb\n\n[1] 1.71\n\n# The projected vector (fitted values)\nh_hat &lt;- b * tt\nh_hat\n\n[1] 4.27 3.41\n\n\nAnd this is exactly what lm() computes when we regress \\(\\mathbf{h}\\) on \\(\\mathbf{t}\\) without an intercept:\n\n# OLS without intercept gives the same b\ncoef(lm(h ~ tt - 1))\n\n  tt \n1.71 \n\n# Fitted values = the projection\ncbind(projection = h_hat, fitted = fitted(lm(h ~ tt - 1)))\n\n  projection fitted\n1       4.27   4.27\n2       3.41   3.41\n\n\nThe residual \\(\\mathbf{e} = \\mathbf{h} - \\hat{\\mathbf{h}}\\) is orthogonal to \\(\\mathbf{t}\\) — their dot product is zero:\n\ne_vec &lt;- h - h_hat\nas.numeric(crossprod(tt, e_vec))\n\n[1] -4.44e-16\n\n\nThis orthogonality is the geometric content of the normal equations \\(X'e = 0\\). Here’s the full picture:\n\ndf_arrows &lt;- data.frame(\n  x0 = c(0, 0, 0, h_hat[1]),\n  y0 = c(0, 0, 0, h_hat[2]),\n  x1 = c(h[1], tt[1], h_hat[1], h[1]),\n  y1 = c(h[2], tt[2], h_hat[2], h[2]),\n  label = c(\"h (outcome)\", \"t (regressor)\", \"h-hat (fitted)\", \"e (residual)\"),\n  color = c(\"black\", \"gray50\", \"forestgreen\", \"tomato\")\n)\n\nslope_t &lt;- tt[2] / tt[1]\n\nggplot() +\n  geom_abline(intercept = 0, slope = slope_t, linetype = \"dotted\", alpha = 0.3) +\n  geom_segment(data = df_arrows,\n               aes(x = x0, y = y0, xend = x1, yend = y1, color = label),\n               arrow = arrow(length = unit(0.15, \"inches\")),\n               linewidth = 1.1) +\n  scale_color_manual(values = c(\"h (outcome)\" = \"black\",\n                                \"t (regressor)\" = \"gray50\",\n                                \"h-hat (fitted)\" = \"forestgreen\",\n                                \"e (residual)\" = \"tomato\"),\n                     name = \"\") +\n  coord_fixed(xlim = c(-1, 5), ylim = c(-1, 6)) +\n  labs(x = \"Observation 1\", y = \"Observation 2\",\n       title = \"OLS finds the closest point on the line of t to h\") +\n  theme_minimal()\n\n\n\n\nOLS finds the closest point on the line of t to h\n\n\n\n\nThe green vector (\\(\\hat{\\mathbf{h}}\\)) is the best prediction in the “column space” of \\(\\mathbf{t}\\), and the red vector (\\(\\mathbf{e}\\)) is the part of \\(\\mathbf{h}\\) that \\(\\mathbf{t}\\) cannot explain. With \\(n = 100\\) observations, these vectors live in \\(\\mathbb{R}^{100}\\) and we can’t draw them — but the geometry is identical. With multiple regressors, the “line” becomes a plane (or hyperplane), and the projection lands on the closest point in that plane."
  },
  {
    "objectID": "ch03-ols.html#building-the-design-matrix",
    "href": "ch03-ols.html#building-the-design-matrix",
    "title": "3. Multivariate OLS",
    "section": "3 Building the design matrix",
    "text": "3 Building the design matrix\nThe model \\(y = X\\beta + e\\) stacks \\(n\\) observations into a matrix. Each row of \\(X\\) is one observation; each column is one variable. The first column is typically all ones (the intercept).\nWe’ll use the Canadian Prestige dataset: the Pineo-Porter prestige score of occupations, predicted by average education (years) and average income (dollars) of workers in each occupation.\n\ndata(Prestige)\nn &lt;- nrow(Prestige)\nK &lt;- 3  # intercept + education + income\n\nX &lt;- cbind(1, Prestige$education, Prestige$income)\ny &lt;- Prestige$prestige\n\ndim(X)   # n x K\n\n[1] 102   3\n\nhead(X)\n\n     [,1] [,2]  [,3]\n[1,]    1 13.1 12351\n[2,]    1 12.3 25879\n[3,]    1 12.8  9271\n[4,]    1 11.4  8865\n[5,]    1 14.6  8403\n[6,]    1 15.6 11030\n\n\nThe two fundamental products in OLS are \\(X'X\\) (a \\(K \\times K\\) matrix) and \\(X'y\\) (a \\(K \\times 1\\) vector):\n\nXtX &lt;- crossprod(X)       # K x K: t(X) %*% X\nXtX\n\n       [,1]    [,2]     [,3]\n[1,]    102    1095 6.93e+05\n[2,]   1095   12513 8.12e+06\n[3,] 693386 8121410 6.53e+09\n\nXty &lt;- crossprod(X, y)    # K x 1: t(X) %*% y\nXty\n\n         [,1]\n[1,]     4777\n[2,]    55326\n[3,] 37748108\n\n\n\\(X'X\\) encodes the relationships among the regressors. The diagonal holds \\(\\sum X_k^2\\) for each variable; the off-diagonals hold \\(\\sum X_j X_k\\). Dividing by \\(n\\) gives the sample second-moment matrix."
  },
  {
    "objectID": "ch03-ols.html#bivariate-ols-the-formula-connection",
    "href": "ch03-ols.html#bivariate-ols-the-formula-connection",
    "title": "3. Multivariate OLS",
    "section": "4 Bivariate OLS: the formula connection",
    "text": "4 Bivariate OLS: the formula connection\nBefore the matrix derivation, recall the bivariate OLS formula: \\(\\hat\\beta_1 = \\text{Cov}(X, Y)/\\text{Var}(X)\\). This is the sample analogue of the BLP coefficient from Chapter 2. Let’s verify it matches the matrix formula using just education as a predictor:\n\neduc &lt;- Prestige$education\n\n# Formula approach\nbeta1_formula &lt;- cov(educ, y) / var(educ)\nbeta0_formula &lt;- mean(y) - beta1_formula * mean(educ)\n\n# Matrix approach (2x2 system)\nX_biv &lt;- cbind(1, educ)\nbeta_biv &lt;- solve(crossprod(X_biv), crossprod(X_biv, y))\n\n# lm() approach\nbeta_lm &lt;- coef(lm(prestige ~ education, data = Prestige))\n\ncbind(formula = c(beta0_formula, beta1_formula),\n      matrix = beta_biv,\n      lm = beta_lm)\n\n     formula            lm\n      -10.73 -10.73 -10.73\neduc    5.36   5.36   5.36\n\n\nAll three give the same answer. The matrix formula \\(\\hat\\beta = (X'X)^{-1}X'y\\) generalizes the bivariate \\(\\text{Cov}/\\text{Var}\\) formula to any number of regressors."
  },
  {
    "objectID": "ch03-ols.html#sec-ols-derivation",
    "href": "ch03-ols.html#sec-ols-derivation",
    "title": "3. Multivariate OLS",
    "section": "5 Deriving OLS with matrix calculus",
    "text": "5 Deriving OLS with matrix calculus\nThe sum of squared errors in matrix form is:\n\\[\\text{SSE}(\\beta) = (y - X\\beta)'(y - X\\beta) = \\underbrace{y'y}_{\\text{constant}} - \\underbrace{2y'X\\beta}_{\\text{linear}} + \\underbrace{\\beta'X'X\\beta}_{\\text{quadratic}} \\tag{1}\\]\nLet’s build each piece in R and verify the expansion:\n\nbeta_test &lt;- c(0, 1, 0.001)  # an arbitrary beta to test\n\n# Direct computation\nsse_direct &lt;- as.numeric(crossprod(y - X %*% beta_test))\n\n# Expanded form\npiece1 &lt;- as.numeric(crossprod(y))                         # y'y\npiece2 &lt;- as.numeric(2 * crossprod(y, X %*% beta_test))    # 2y'Xbeta\npiece3 &lt;- as.numeric(t(beta_test) %*% XtX %*% beta_test)  # beta'X'Xbeta\n\nc(direct = sse_direct, expanded = piece1 - piece2 + piece3)\n\n  direct expanded \n  102760   102760 \n\n\nSetting \\(\\partial \\text{SSE}/\\partial \\beta = -2X'y + 2X'X\\hat\\beta = 0\\) gives the normal equations:\n\\[X'X\\hat\\beta = X'y \\tag{2}\\]\nSolving with solve():\n\n# solve(A, b) solves the system Ax = b — better than solve(A) %*% b\nbeta_hat &lt;- solve(XtX, Xty)\nbeta_hat\n\n         [,1]\n[1,] -6.84778\n[2,]  4.13744\n[3,]  0.00136\n\n# lm() gives the same thing\ncoef(lm(prestige ~ education + income, data = Prestige))\n\n(Intercept)   education      income \n   -6.84778     4.13744     0.00136 \n\n\nNote: solve(A, b) is preferred over solve(A) %*% b — it avoids computing the full inverse, which is slower and less numerically stable.\n\nTheorem 1 (The OLS Estimator) The OLS estimator \\(\\hat\\beta = (X'X)^{-1}X'y\\) is the unique minimizer of \\(\\text{SSE}(\\beta) = (y - X\\beta)'(y - X\\beta)\\) when \\(X'X\\) is positive definite.\n\n\n5.1 The second-order condition\nThe second derivative of \\(\\text{SSE}\\) is \\(2X'X\\). This is a minimum when \\(X'X\\) is positive definite — all eigenvalues are positive:\n\neigen(XtX)$values\n\n[1] 6.53e+09 2.44e+03 5.83e+00\n\n\nAll positive, confirming positive definiteness. If any eigenvalue were zero, \\(X'X\\) would be singular and solve() would fail."
  },
  {
    "objectID": "ch03-ols.html#what-collinearity-does-to-xx",
    "href": "ch03-ols.html#what-collinearity-does-to-xx",
    "title": "3. Multivariate OLS",
    "section": "6 What collinearity does to \\(X'X\\)",
    "text": "6 What collinearity does to \\(X'X\\)\nWhen a column of \\(X\\) is a linear combination of others, \\(X'X\\) loses rank:\n\n# Add a redundant column: income2 = 2 * income\nX_bad &lt;- cbind(X, 2 * Prestige$income)\n\nXtX_bad &lt;- crossprod(X_bad)\ndet(XtX_bad)          # essentially zero\n\n[1] 0\n\neigen(XtX_bad)$values  # last eigenvalue collapses\n\n[1] 3.27e+10 2.44e+03 5.83e+00 4.55e-13\n\n\nIn practice, near-collinearity (very small but nonzero eigenvalues) inflates standard errors without crashing solve(). The condition number — ratio of largest to smallest eigenvalue — measures how close to singular:\n\nevals &lt;- eigen(XtX)$values\nc(largest = evals[1], smallest = evals[K], condition = evals[1] / evals[K])\n\n  largest  smallest condition \n 6.53e+09  5.83e+00  1.12e+09 \n\n\nR’s lm() handles exact collinearity by dropping the redundant column:\n\ncoef(lm(prestige ~ education + income + I(2 * income), data = Prestige))\n\n  (Intercept)     education        income I(2 * income) \n     -6.84778       4.13744       0.00136            NA \n\n\n\n\n\n\n\n\nWarningNear-Collinearity Inflates Standard Errors\n\n\n\nWhen columns of \\(X\\) are nearly linearly dependent, \\(X'X\\) has a near-zero eigenvalue, making \\((X'X)^{-1}\\) very large. This inflates the variance of \\(\\hat\\beta\\) without causing solve() to fail — standard errors balloon silently. Check the condition number of \\(X'X\\) to detect this."
  },
  {
    "objectID": "ch03-ols.html#sec-projection-matrix",
    "href": "ch03-ols.html#sec-projection-matrix",
    "title": "3. Multivariate OLS",
    "section": "7 The projection matrix",
    "text": "7 The projection matrix\nThe projection matrix \\(P = X(X'X)^{-1}X'\\) maps any \\(n\\)-vector onto the column space of \\(X\\). In two dimensions (our earlier example), it projected \\(\\mathbf{h}\\) onto the line of \\(\\mathbf{t}\\). With \\(K = 3\\) regressors, it projects \\(\\mathbf{y}\\) onto a 3-dimensional subspace of \\(\\mathbb{R}^n\\).\n\nP &lt;- X %*% solve(XtX) %*% t(X)\ndim(P)  # n x n\n\n[1] 102 102\n\nmod &lt;- lm(prestige ~ education + income, data = Prestige)\n\n\nDefinition 1 (Projection (Hat) Matrix) The projection matrix \\(P = X(X'X)^{-1}X'\\) maps any \\(n\\)-vector onto the column space of \\(X\\). It is symmetric (\\(P' = P\\)) and idempotent (\\(P^2 = P\\)), with eigenvalues in \\(\\{0, 1\\}\\) and \\(\\text{tr}(P) = K\\).\n\nEvery property of \\(P\\) corresponds to a matrix operation:\n\n# P*y = fitted values\nall.equal(as.vector(P %*% y), as.numeric(fitted(mod)))\n\n[1] TRUE\n\n# Symmetric: t(P) = P\nall.equal(t(P), P)\n\n[1] TRUE\n\n# Idempotent: P %*% P = P (projecting twice = projecting once)\nall.equal(P %*% P, P)\n\n[1] TRUE\n\n# P*X = X (X is already in its own column space)\nall.equal(P %*% X, X, check.attributes = FALSE)\n\n[1] TRUE\n\n\n\n7.1 What idempotency means for eigenvalues\nIf \\(Pv = \\lambda v\\), then \\(P^2 v = \\lambda^2 v\\). But \\(P^2 = P\\), so \\(\\lambda^2 = \\lambda\\), which means \\(\\lambda \\in \\{0, 1\\}\\):\n\neig_P &lt;- eigen(P)$values\ntable(round(eig_P, 10))\n\n\n 0  1 \n99  3 \n\n\n\\(K\\) eigenvalues equal 1 (the column space of \\(X\\)) and \\(n - K\\) equal 0 (the null space). The trace counts the 1s:\n\nc(trace_P = tr(P), K = K)\n\ntrace_P       K \n      3       3 \n\n\n\n\n7.2 Projection onto the intercept\nThe simplest projection is onto a column of ones: \\(P_1 = \\mathbf{1}(\\mathbf{1}'\\mathbf{1})^{-1}\\mathbf{1}' = \\frac{1}{n}\\mathbf{1}\\mathbf{1}'\\). This projects every observation onto the sample mean:\n\nones &lt;- rep(1, n)\nP1 &lt;- outer(ones, ones) / n  # outer product: 1*1' / n\n\n# P1 * y = sample mean for every observation\nall.equal(as.vector(P1 %*% y), rep(mean(y), n))\n\n[1] TRUE\n\n\nEvery additional regressor refines this baseline: the full \\(P\\) starts from the mean and adds the directions explained by the other columns of \\(X\\)."
  },
  {
    "objectID": "ch03-ols.html#the-annihilator-matrix",
    "href": "ch03-ols.html#the-annihilator-matrix",
    "title": "3. Multivariate OLS",
    "section": "8 The annihilator matrix",
    "text": "8 The annihilator matrix\nThe annihilator \\(M = I_n - P\\) projects onto the orthogonal complement — the part of \\(y\\) that \\(X\\) cannot explain:\n\nM &lt;- diag(n) - P\n\n# M*y = residuals\nall.equal(as.vector(M %*% y), as.numeric(resid(mod)))\n\n[1] TRUE\n\n# Idempotent and symmetric\nall.equal(M %*% M, M)\n\n[1] TRUE\n\nall.equal(t(M), M)\n\n[1] TRUE\n\n# M annihilates X: M*X = 0\nmax(abs(M %*% X))\n\n[1] 4.82e-11\n\n\n\nDefinition 2 (Annihilator Matrix) The annihilator \\(M = I_n - P\\) projects onto the orthogonal complement of the column space of \\(X\\). It satisfies \\(MX = 0\\) (annihilates \\(X\\)), is idempotent and symmetric, and has \\(\\text{tr}(M) = n - K\\).\n\nEigenvalues are complementary to \\(P\\): \\(n - K\\) ones and \\(K\\) zeros:\n\nc(trace_M = tr(M), n_minus_K = n - K)\n\n  trace_M n_minus_K \n       99        99 \n\n\nThe demeaning matrix \\(M_1 = I - P_1\\) is a special case — it subtracts the mean:\n\nM1 &lt;- diag(n) - P1\nall.equal(as.vector(M1 %*% y), as.numeric(y - mean(y)))\n\n[1] TRUE"
  },
  {
    "objectID": "ch03-ols.html#application-regression-to-the-mean",
    "href": "ch03-ols.html#application-regression-to-the-mean",
    "title": "3. Multivariate OLS",
    "section": "9 Application: regression to the mean",
    "text": "9 Application: regression to the mean\nHere’s a classic application of bivariate OLS. Galton noticed that children of unusually tall parents tend to be shorter than their parents — and children of unusually short parents tend to be taller. This “regression to the mean” is not a causal mechanism; it’s a consequence of the BLP slope being less than 1 when the correlation is less than 1.\n\n# Simulate parent-child heights (jointly normal)\nset.seed(307)\nn_ht &lt;- 1000\nrho &lt;- 0.5   # correlation between parent and child height\n\nlibrary(MASS)\nheights &lt;- mvrnorm(n_ht, mu = c(68, 68),\n                   Sigma = matrix(c(9, rho * 9, rho * 9, 9), 2, 2))\nparent_ht &lt;- heights[, 1]\nchild_ht &lt;- heights[, 2]\n\n# OLS by matrix formula\nX_ht &lt;- cbind(1, parent_ht)\nbeta_ht &lt;- solve(crossprod(X_ht), crossprod(X_ht, child_ht))\nbeta_ht\n\n            [,1]\n          37.065\nparent_ht  0.456\n\n\nThe slope is \\(\\hat\\beta_1 \\approx\\) 0.46, less than 1. So a parent who is 1 inch above average has a child who is only about 0.46 inches above average — regression toward the mean.\n\ndf_ht &lt;- data.frame(parent = parent_ht, child = child_ht)\n\nggplot(df_ht, aes(parent, child)) +\n  geom_point(alpha = 0.15, size = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"steelblue\", linewidth = 1.2) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", alpha = 0.5) +\n  annotate(\"text\", x = 74, y = 74.5, label = \"slope = 1 (no regression)\", alpha = 0.5) +\n  labs(x = \"Parent height (in)\", y = \"Child height (in)\",\n       title = \"Regression to the mean\",\n       subtitle = paste0(\"Slope = \", round(beta_ht[2], 2),\n                         \" &lt; 1: children of tall parents are less extreme\")) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe tallest parents (above the 95th percentile) have children who are closer to the mean:\n\ntall &lt;- parent_ht &gt; quantile(parent_ht, 0.95)\nc(parent_mean = mean(parent_ht[tall]),\n  child_mean = mean(child_ht[tall]),\n  difference = mean(parent_ht[tall]) - mean(child_ht[tall]))\n\nparent_mean  child_mean  difference \n      74.19       71.15        3.04"
  },
  {
    "objectID": "ch03-ols.html#residuals-vs.-disturbances",
    "href": "ch03-ols.html#residuals-vs.-disturbances",
    "title": "3. Multivariate OLS",
    "section": "10 Residuals vs. disturbances",
    "text": "10 Residuals vs. disturbances\nThe true model is \\(y = X\\beta + e\\) where \\(e\\) is unobservable. The residuals \\(\\hat{e} = My\\) relate to the disturbances through:\n\\[\\hat{e} = My = M(X\\beta + e) = \\underbrace{MX}_{= 0}\\beta + Me = Me\\]\nLet’s simulate to see this. We know \\(\\beta\\) and \\(e\\) because we generate the data:\n\nset.seed(307)\nn_sim &lt;- 100\nK_sim &lt;- 2\nX_sim &lt;- cbind(1, rnorm(n_sim))\nbeta_true &lt;- c(2, 3)\ne_true &lt;- rnorm(n_sim, sd = 2)\ny_sim &lt;- X_sim %*% beta_true + e_true\n\n# Build M for this design\nP_sim &lt;- X_sim %*% solve(crossprod(X_sim)) %*% t(X_sim)\nM_sim &lt;- diag(n_sim) - P_sim\n\n# Residuals = M * disturbances\ne_hat &lt;- as.vector(M_sim %*% y_sim)\nall.equal(e_hat, as.vector(M_sim %*% e_true))\n\n[1] TRUE\n\n# Residuals have smaller variance — M zeroes out K dimensions\nc(var_disturbances = var(e_true), var_residuals = var(e_hat))\n\nvar_disturbances    var_residuals \n             4.5              4.5"
  },
  {
    "objectID": "ch03-ols.html#estimating-sigma2-the-trace-trick",
    "href": "ch03-ols.html#estimating-sigma2-the-trace-trick",
    "title": "3. Multivariate OLS",
    "section": "11 Estimating \\(\\sigma^2\\): the trace trick",
    "text": "11 Estimating \\(\\sigma^2\\): the trace trick\nThe natural estimator \\(\\hat\\sigma^2 = \\hat{e}'\\hat{e}/n\\) is biased downward because \\(\\hat{e}'\\hat{e} = e'Me \\leq e'e\\) (\\(M\\) is positive semi-definite). The unbiased estimator divides by \\(n - K\\).\nThe proof is a chain of matrix operations. Every step translates to R:\n\n# Step 1: e'Me is a scalar = its own trace\nscalar_form &lt;- as.numeric(t(e_true) %*% M_sim %*% e_true)\ntrace_form &lt;- tr(M_sim %*% tcrossprod(e_true))  # tr(M * ee')\n\nc(scalar = scalar_form, trace = trace_form)\n\nscalar  trace \n   445    445 \n\n# Step 2: E[ee'] = sigma^2 * I, so E[tr(Mee')] = sigma^2 * tr(M)\n# tr(M) = n - K, so E[e'hat * e'hat] = sigma^2 * (n - K)\nc(trace_M = tr(M_sim), n_minus_K = n_sim - K_sim)\n\n  trace_M n_minus_K \n       98        98 \n\n\n\n\n\n\n\n\nNoteThe \\(n - K\\) Divisor\n\n\n\nThe unbiased variance estimator divides by \\(n - K\\) (not \\(n\\)) because the residuals live in an \\((n - K)\\)-dimensional subspace. The \\(K\\) “lost” dimensions are consumed by estimating \\(\\hat\\beta\\). This is the matrix version of Bessel’s correction.\n\n\nThis is why the unbiased estimator is \\(s_e^2 = \\hat{e}'\\hat{e}/(n-K)\\):\n\n# Back to the Prestige data\ne_hat_prestige &lt;- resid(mod)\n\nsigma2_biased &lt;- as.numeric(crossprod(e_hat_prestige)) / n\nsigma2_unbiased &lt;- as.numeric(crossprod(e_hat_prestige)) / (n - K)\n\nc(biased = sigma2_biased,\n  unbiased = sigma2_unbiased,\n  R_sigma2 = sigma(mod)^2)\n\n  biased unbiased R_sigma2 \n    59.2     61.0     61.0 \n\n\nThe underestimation shows up in the eigenvalues of \\(M\\): \\(n - K\\) eigenvalues are 1, and \\(K\\) are 0. The residuals live in an \\((n-K)\\)-dimensional subspace:\n\neig_M &lt;- round(eigen(M)$values, 10)\nc(eigenvalues_equal_1 = sum(eig_M == 1),\n  eigenvalues_equal_0 = sum(eig_M == 0))\n\neigenvalues_equal_1 eigenvalues_equal_0 \n                 99                   3"
  },
  {
    "objectID": "ch03-ols.html#variance-of-hatbeta-building-s_e2xx-1",
    "href": "ch03-ols.html#variance-of-hatbeta-building-s_e2xx-1",
    "title": "3. Multivariate OLS",
    "section": "12 Variance of \\(\\hat\\beta\\): building \\(s_e^2(X'X)^{-1}\\)",
    "text": "12 Variance of \\(\\hat\\beta\\): building \\(s_e^2(X'X)^{-1}\\)\nUnder homoskedasticity, \\(\\text{Var}(\\hat\\beta|X) = \\sigma^2(X'X)^{-1}\\). Each piece is a matrix operation:\n\n# Step 1: (X'X)^{-1}\nXtX_inv &lt;- solve(XtX)\nXtX_inv\n\n          [,1]      [,2]      [,3]\n[1,]  1.70e-01 -1.64e-02  2.35e-06\n[2,] -1.64e-02  2.00e-03 -7.41e-07\n[3,]  2.35e-06 -7.41e-07  8.24e-10\n\n# Step 2: multiply by s_e^2\nvcov_manual &lt;- sigma(mod)^2 * XtX_inv\n\n# Step 3: compare to R\nall.equal(vcov_manual, vcov(mod), check.attributes = FALSE)\n\n[1] TRUE\n\n\nStandard errors are the square roots of the diagonal:\n\nse_manual &lt;- sqrt(diag(vcov_manual))\nse_R &lt;- coef(summary(mod))[, \"Std. Error\"]\n\ncbind(manual = se_manual, R = se_R)\n\n              manual        R\n(Intercept) 3.218977 3.218977\neducation   0.348912 0.348912\nincome      0.000224 0.000224\n\n\n\n12.1 Why \\((X'X)^{-1}\\) determines precision\nThe eigenvalues of \\((X'X)^{-1}\\) are the reciprocals of those of \\(X'X\\). Large eigenvalues of \\(X'X\\) (strong signal) become small eigenvalues of \\((X'X)^{-1}\\) (precise estimates). Near-collinearity creates a tiny eigenvalue in \\(X'X\\), which inflates variance:\n\neig_XtX &lt;- eigen(XtX)$values\neig_inv &lt;- eigen(XtX_inv)$values\n\ncbind(XtX = eig_XtX, XtX_inv = eig_inv, product = eig_XtX * eig_inv)\n\n          XtX  XtX_inv  product\n[1,] 6.53e+09 1.71e-01 1.12e+09\n[2,] 2.44e+03 4.10e-04 1.00e+00\n[3,] 5.83e+00 1.53e-10 8.93e-10\n\n\nThe products are all 1: the eigenvalues invert exactly."
  },
  {
    "objectID": "ch03-ols.html#application-the-prestige-regression",
    "href": "ch03-ols.html#application-the-prestige-regression",
    "title": "3. Multivariate OLS",
    "section": "13 Application: the Prestige regression",
    "text": "13 Application: the Prestige regression\nLet’s interpret the full regression. Education and income both predict occupational prestige:\n\nsummary(mod)\n\n\nCall:\nlm(formula = prestige ~ education + income, data = Prestige)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.404  -5.331   0.015   4.980  17.689 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.847779   3.218977   -2.13    0.036 *  \neducation    4.137444   0.348912   11.86  &lt; 2e-16 ***\nincome       0.001361   0.000224    6.07  2.4e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.81 on 99 degrees of freedom\nMultiple R-squared:  0.798, Adjusted R-squared:  0.794 \nF-statistic:  196 on 2 and 99 DF,  p-value: &lt;2e-16\n\n\nThe coefficient on education (4.1) says: holding income constant, one additional year of average education is associated with about 4.1 points more prestige. The coefficient on income (0.001) is small in magnitude because income is in dollars — a $1,000 increase predicts about 1.4 points.\nLet’s see which occupations the model fits well and poorly, using the projection and annihilator:\n\nPrestige$fitted &lt;- as.vector(P %*% y)\nPrestige$resid &lt;- as.vector(M %*% y)\n\n# Largest positive residuals: more prestige than education+income predict\nhead(Prestige[order(-Prestige$resid), c(\"education\", \"income\", \"prestige\", \"fitted\", \"resid\")], 5)\n\n                    education income prestige fitted resid\nfarmers                  6.84   3643     44.1   26.4  17.7\nelectronic.workers       8.76   3942     50.8   34.8  16.0\nphysio.therapsts        13.62   5092     72.1   56.4  15.7\nmedical.technicians     12.79   5180     67.5   53.1  14.4\nnurses                  12.46   4614     64.7   51.0  13.7\n\n# Largest negative residuals: less prestige than predicted\nhead(Prestige[order(Prestige$resid), c(\"education\", \"income\", \"prestige\", \"fitted\", \"resid\")], 5)\n\n                          education income prestige fitted resid\nnewsboys                       9.62    918     14.8   34.2 -19.4\ncollectors                    11.20   4741     29.4   45.9 -16.5\nfile.clerks                   12.09   3016     32.7   47.3 -14.6\nservice.station.attendant      9.93   2370     23.3   37.5 -14.2\nbartenders                     8.50   3930     20.2   33.7 -13.5\n\n\n\nggplot(Prestige, aes(fitted, resid)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_text(data = Prestige[abs(Prestige$resid) &gt; 15, ],\n            aes(label = rownames(Prestige)[abs(Prestige$resid) &gt; 15]),\n            hjust = -0.1, size = 3) +\n  labs(x = \"Fitted values (Py)\", y = \"Residuals (My)\",\n       title = \"Prestige: fitted vs. residuals\") +\n  theme_minimal()"
  },
  {
    "objectID": "ch03-ols.html#anova-as-inner-products",
    "href": "ch03-ols.html#anova-as-inner-products",
    "title": "3. Multivariate OLS",
    "section": "14 ANOVA as inner products",
    "text": "14 ANOVA as inner products\nThe decomposition \\(y = \\hat{y} + \\hat{e}\\) is orthogonal. In matrix terms, \\(\\hat{y}'\\hat{e} = (Py)'(My) = y'PMy = 0\\):\n\nas.numeric(crossprod(fitted(mod), resid(mod)))\n\n[1] -1.07e-13\n\n\nAfter centering, the inner products give sums of squares:\n\n# SST = ||M1 * y||^2  (total variation around the mean)\nSST &lt;- as.numeric(crossprod(M1 %*% y))\n\n# SSR = ||(P - P1) * y||^2  (variation explained by regressors beyond the mean)\nSSR &lt;- as.numeric(crossprod(fitted(mod) - mean(y)))\n\n# SSE = ||M * y||^2  (unexplained variation)\nSSE &lt;- as.numeric(crossprod(resid(mod)))\n\nc(SST = SST, SSR_plus_SSE = SSR + SSE)\n\n         SST SSR_plus_SSE \n       29895        29895 \n\n# Cross term is zero when X includes a constant\nas.numeric(crossprod(fitted(mod) - mean(y), resid(mod)))\n\n[1] 8.74e-13"
  },
  {
    "objectID": "ch03-ols.html#r2",
    "href": "ch03-ols.html#r2",
    "title": "3. Multivariate OLS",
    "section": "15 \\(R^2\\)",
    "text": "15 \\(R^2\\)\n\\(R^2 = \\text{SSR}/\\text{SST} = 1 - \\text{SSE}/\\text{SST}\\):\n\nc(R2 = SSR / SST,\n  R2_alt = 1 - SSE / SST,\n  R_reports = summary(mod)$r.squared)\n\n       R2    R2_alt R_reports \n    0.798     0.798     0.798 \n\n\nAdding regressors can only increase \\(R^2\\), even if the variable is noise. The adjusted \\(R^2\\) penalizes for \\(K\\):\n\nset.seed(42)\nPrestige$noise &lt;- rnorm(n)\nmod_noise &lt;- lm(prestige ~ education + income + noise, data = Prestige)\n\nc(R2_original = summary(mod)$r.squared,\n  R2_with_noise = summary(mod_noise)$r.squared,\n  adj_R2_original = summary(mod)$adj.r.squared,\n  adj_R2_with_noise = summary(mod_noise)$adj.r.squared)\n\n      R2_original     R2_with_noise   adj_R2_original adj_R2_with_noise \n            0.798             0.799             0.794             0.792 \n\n\nRaw \\(R^2\\) ticks up; adjusted \\(R^2\\) drops — correctly penalizing the useless variable.\n\\(R^2\\) measures descriptive fit, not causal validity. Typical values: cross-sectional micro data \\(\\approx 0.2\\)–\\(0.4\\), aggregate time series \\(\\approx 0.7\\)–\\(0.9\\)."
  },
  {
    "objectID": "ch03-ols.html#naming-conventions-a-warning",
    "href": "ch03-ols.html#naming-conventions-a-warning",
    "title": "3. Multivariate OLS",
    "section": "16 Naming conventions: a warning",
    "text": "16 Naming conventions: a warning\nDifferent textbooks use SSE and SSR with opposite meanings. In this course (following Hansen), SSR is “regression” (explained) and SSE is “error” (unexplained). Some texts reverse these. The math is always \\(\\text{SST} = \\text{explained} + \\text{unexplained}\\)."
  },
  {
    "objectID": "ch03-ols.html#summary",
    "href": "ch03-ols.html#summary",
    "title": "3. Multivariate OLS",
    "section": "17 Summary",
    "text": "17 Summary\nThe OLS estimator is a sequence of matrix operations:\n\n\n\n\n\n\n\n\nMath\nR\nWhat it does\n\n\n\n\n\\(X'X\\)\ncrossprod(X)\nGram matrix of regressors\n\n\n\\((X'X)^{-1}\\)\nsolve(crossprod(X))\nInverse\n\n\n\\(\\hat\\beta = (X'X)^{-1}X'y\\)\nsolve(crossprod(X), crossprod(X, y))\nOLS coefficients\n\n\n\\(P = X(X'X)^{-1}X'\\)\nX %*% solve(crossprod(X)) %*% t(X)\nProjection (hat matrix)\n\n\n\\(M = I - P\\)\ndiag(n) - P\nAnnihilator\n\n\n\\(\\text{tr}(M)\\)\nsum(diag(M))\nDegrees of freedom (\\(n - K\\))\n\n\neigenvalues of \\(P\\)\neigen(P)$values\nAll 0 or 1\n\n\n\\(s_e^2(X'X)^{-1}\\)\nsigma(mod)^2 * solve(crossprod(X))\nVariance-covariance of \\(\\hat\\beta\\)\n\n\n\\(\\text{SE}(\\hat\\beta_k)\\)\nsqrt(diag(vcov(mod)))\nStandard errors\n\n\n\nKey facts:\n\nOLS is projection: \\(\\hat{y} = Py\\) is the closest point to \\(y\\) in the column space of \\(X\\), and \\(\\hat{e} = My\\) is the orthogonal residual.\n\\(P\\) and \\(M\\) are symmetric and idempotent, with eigenvalues in \\(\\{0, 1\\}\\).\n\\(\\text{tr}(P) = K\\) and \\(\\text{tr}(M) = n - K\\) count dimensions.\nThe trace trick proves \\(s_e^2\\) is unbiased: \\(\\mathbb{E}[e'Me] = \\sigma^2 \\text{tr}(M) = \\sigma^2(n-K)\\).\nEigenvalues of \\((X'X)^{-1}\\) are reciprocals of those of \\(X'X\\): near-collinearity inflates variance.\nRegression to the mean is a consequence of \\(\\hat\\beta_1 &lt; 1\\) when \\(|\\rho| &lt; 1\\).\n\nNext: Sensitivity and Leverage — the Frisch-Waugh-Lovell theorem, partial \\(R^2\\), and influential observations."
  },
  {
    "objectID": "ch01-review.html",
    "href": "ch01-review.html",
    "title": "1. Probability and Linear Algebra",
    "section": "",
    "text": "library(ggplot2)\nlibrary(MASS)\noptions(digits = 3)\nThis chapter builds up the machinery behind OLS from scratch. We start with expectation and variance, show how the conditional expectation function emerges from a joint distribution, and then shift to linear algebra — matrices, projection, and the geometry that makes regression work.\nQuestions this chapter answers:"
  },
  {
    "objectID": "ch01-review.html#expectation-as-a-best-guess",
    "href": "ch01-review.html#expectation-as-a-best-guess",
    "title": "1. Probability and Linear Algebra",
    "section": "1 Expectation as a best guess",
    "text": "1 Expectation as a best guess\nIf you had to predict a random variable \\(Y\\) with a single number \\(\\mu\\), and your penalty for being wrong is squared error, you’d choose the mean. Let’s verify this computationally.\nSuppose \\(Y\\) takes values \\(\\{1, 2, 3, 4, 5\\}\\) with probabilities \\(\\{0.1, 0.2, 0.3, 0.25, 0.15\\}\\).\n\nvalues &lt;- 1:5\nprobs &lt;- c(0.1, 0.2, 0.3, 0.25, 0.15)\n\n# The expected value\nmu &lt;- sum(values * probs)\nmu\n\n[1] 3.15\n\n# Mean squared error as a function of the guess\nmse &lt;- function(guess) {\n  sum(probs * (values - guess)^2)\n}\n\n# Evaluate over a grid\nguesses &lt;- seq(1, 5, by = 0.01)\nerrors &lt;- sapply(guesses, mse)\n\nggplot(data.frame(guess = guesses, mse = errors), aes(guess, mse)) +\n  geom_line(linewidth = 0.8) +\n  geom_vline(xintercept = mu, linetype = \"dashed\", color = \"steelblue\") +\n  annotate(\"text\", x = mu + 0.3, y = max(errors) * 0.9,\n           label = paste(\"E[Y] =\", mu), color = \"steelblue\") +\n  labs(x = \"Guess (mu)\", y = \"Mean Squared Error\",\n       title = \"The expectation minimizes mean squared prediction error\") +\n  theme_minimal()\n\n\n\n\nThe expectation minimizes mean squared prediction error\n\n\n\n\nThe minimum of the parabola lands exactly at \\(\\mathbb{E}[Y]\\). This is the simplest version of a result that recurs throughout the course: the conditional expectation function minimizes MSE given information.\n\nTheorem 1 (E[Y] Minimizes MSE) Among all constants \\(\\mu\\), the expected value \\(\\mathbb{E}[Y]\\) uniquely minimizes the mean squared error \\(\\mathbb{E}[(Y - \\mu)^2]\\)."
  },
  {
    "objectID": "ch01-review.html#variance-and-the-deviation-identity",
    "href": "ch01-review.html#variance-and-the-deviation-identity",
    "title": "1. Probability and Linear Algebra",
    "section": "2 Variance and the deviation identity",
    "text": "2 Variance and the deviation identity\nVariance measures the average squared distance from the mean:\n\\[\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\tag{1}\\]\nLet’s verify both formulas give the same answer.\n\nEY &lt;- sum(values * probs)\nEY2 &lt;- sum(values^2 * probs)\n\n# Definition form\nvar_def &lt;- sum(probs * (values - EY)^2)\n\n# Shortcut form\nvar_short &lt;- EY2 - EY^2\n\nc(definition = var_def, shortcut = var_short)\n\ndefinition   shortcut \n      1.43       1.43 \n\n\nIn a sample, we can also write the variance as \\(\\frac{1}{n}\\sum(x_i - \\bar{x})x_i\\). This works because deviations from the mean always sum to zero: \\(\\sum(x_i - \\bar{x}) = 0\\), so shifting the second factor by any constant changes nothing. This same algebra produces the OLS normal equations later.\nThe interactive visualization below makes this concrete using the sample \\(x = \\{1, 5, 6, 8\\}\\) with mean \\(\\bar{x} = 5\\). Think of each term as the signed area of a rectangle with width \\((x_i - \\bar{x})\\) and height \\((x_i - c)\\). Slide \\(c\\) from \\(\\bar{x}\\) down to \\(0\\): the rectangles change shape, but the total signed area stays constant.\n\nviewof ref = Inputs.range([0, 5], {value: 5, step: 0.1, label: \"Reference point (c)\"})\n\n\n\n\n\n\n\n{\n  const width = 500;\n  const height = 420;\n  const margin = {top: 30, right: 40, bottom: 50, left: 50};\n\n  const data = [1, 5, 6, 8];\n  const xbar = 5;\n  const c = ref;\n\n  const svg = d3.create(\"svg\")\n    .attr(\"viewBox\", [0, 0, width, height])\n    .attr(\"width\", width)\n    .attr(\"height\", height)\n    .style(\"font-family\", \"sans-serif\");\n\n  const xScale = d3.scaleLinear()\n    .domain([-1, 9.5])\n    .range([margin.left, width - margin.right]);\n\n  const yScale = d3.scaleLinear()\n    .domain([-1, 9.5])\n    .range([height - margin.bottom, margin.top]);\n\n  // Axes\n  svg.append(\"g\")\n    .attr(\"transform\", `translate(0,${yScale(0)})`)\n    .call(d3.axisBottom(xScale).ticks(10))\n    .append(\"text\")\n    .attr(\"x\", width - margin.right)\n    .attr(\"y\", -10)\n    .attr(\"fill\", \"black\")\n    .attr(\"text-anchor\", \"end\")\n    .text(\"x\");\n\n  svg.append(\"g\")\n    .attr(\"transform\", `translate(${xScale(0)},0)`)\n    .call(d3.axisLeft(yScale).ticks(10));\n\n  // Draw x-bar vertical line\n  svg.append(\"line\")\n    .attr(\"x1\", xScale(xbar)).attr(\"x2\", xScale(xbar))\n    .attr(\"y1\", yScale(-0.5)).attr(\"y2\", yScale(9))\n    .attr(\"stroke\", \"steelblue\").attr(\"stroke-dasharray\", \"4,4\").attr(\"stroke-width\", 1.5);\n\n  svg.append(\"text\")\n    .attr(\"x\", xScale(xbar)).attr(\"y\", yScale(-0.8))\n    .attr(\"text-anchor\", \"middle\").attr(\"fill\", \"steelblue\").attr(\"font-size\", 13)\n    .text(\"x̄ = 5\");\n\n  // Draw x-bar horizontal line\n  svg.append(\"line\")\n    .attr(\"x1\", xScale(-0.5)).attr(\"x2\", xScale(9))\n    .attr(\"y1\", yScale(xbar)).attr(\"y2\", yScale(xbar))\n    .attr(\"stroke\", \"steelblue\").attr(\"stroke-dasharray\", \"4,4\").attr(\"stroke-width\", 1.5);\n\n  // Draw reference vertical line\n  svg.append(\"line\")\n    .attr(\"x1\", xScale(c)).attr(\"x2\", xScale(c))\n    .attr(\"y1\", yScale(-0.5)).attr(\"y2\", yScale(9))\n    .attr(\"stroke\", \"tomato\").attr(\"stroke-dasharray\", \"2,3\").attr(\"stroke-width\", 1.5);\n\n  svg.append(\"text\")\n    .attr(\"x\", xScale(c)).attr(\"y\", yScale(9.3))\n    .attr(\"text-anchor\", \"middle\").attr(\"fill\", \"tomato\").attr(\"font-size\", 12)\n    .text(`c = ${c.toFixed(1)}`);\n\n  // Positive/negative color for rectangles\n  const posColor = \"rgba(70, 130, 180, 0.3)\";\n  const negColor = \"rgba(220, 80, 80, 0.3)\";\n  const posStroke = \"steelblue\";\n  const negStroke = \"tomato\";\n\n  // Draw rectangles\n  for (const xi of data) {\n    const dev = xi - xbar;\n    const ht = xi - c;\n    const area = dev * ht;\n\n    if (Math.abs(dev) &lt; 0.001) continue;\n\n    const isPositive = area &gt;= 0;\n\n    const rx2 = xScale(Math.min(xbar, xi));\n    const ry2 = yScale(Math.max(c, xi));\n    const rw2 = Math.abs(xScale(xi) - xScale(xbar));\n    const rh2 = Math.abs(yScale(c) - yScale(xi));\n\n    svg.append(\"rect\")\n      .attr(\"x\", rx2).attr(\"y\", ry2)\n      .attr(\"width\", rw2).attr(\"height\", rh2)\n      .attr(\"fill\", isPositive ? posColor : negColor)\n      .attr(\"stroke\", isPositive ? posStroke : negStroke)\n      .attr(\"stroke-width\", 1.5);\n\n    const cx = (Math.min(xbar, xi) + Math.max(xbar, xi)) / 2;\n    const cy = (Math.min(c, xi) + Math.max(c, xi)) / 2;\n\n    if (rw2 &gt; 15 && rh2 &gt; 15) {\n      svg.append(\"text\")\n        .attr(\"x\", xScale(cx)).attr(\"y\", yScale(cy))\n        .attr(\"text-anchor\", \"middle\").attr(\"dominant-baseline\", \"middle\")\n        .attr(\"font-size\", 11).attr(\"fill\", \"#333\")\n        .text(`${dev.toFixed(0)}×${ht.toFixed(1)}`);\n    }\n  }\n\n  // Draw data points on the diagonal\n  for (const xi of data) {\n    svg.append(\"circle\")\n      .attr(\"cx\", xScale(xi)).attr(\"cy\", yScale(xi))\n      .attr(\"r\", 5).attr(\"fill\", \"black\");\n  }\n\n  // Compute total and display\n  let total = 0;\n  for (const xi of data) {\n    total += (xi - xbar) * (xi - c);\n  }\n  const variance = total / data.length;\n\n  svg.append(\"text\")\n    .attr(\"x\", xScale(7)).attr(\"y\", yScale(1.5))\n    .attr(\"text-anchor\", \"middle\").attr(\"font-size\", 14).attr(\"fill\", \"#333\")\n    .html(`Total area = ${total.toFixed(1)}`);\n\n  svg.append(\"text\")\n    .attr(\"x\", xScale(7)).attr(\"y\", yScale(0.5))\n    .attr(\"text-anchor\", \"middle\").attr(\"font-size\", 14).attr(\"font-weight\", \"bold\").attr(\"fill\", \"#333\")\n    .text(`Var = ${total.toFixed(1)} / ${data.length} = ${variance.toFixed(2)}`);\n\n  return svg.node();\n}"
  },
  {
    "objectID": "ch01-review.html#joint-distributions-and-the-cef",
    "href": "ch01-review.html#joint-distributions-and-the-cef",
    "title": "1. Probability and Linear Algebra",
    "section": "3 Joint distributions and the CEF",
    "text": "3 Joint distributions and the CEF\nThe lecture works through an example where two parties (A and B) mobilize voters, with joint density \\(f(x, y) = 6(1 - x - y)\\) for \\(x, y \\geq 0\\) and \\(x + y \\leq 1\\). Let’s visualize this and compute the CEF.\n\n# Create a grid\ngrid &lt;- expand.grid(x = seq(0, 1, length = 200), y = seq(0, 1, length = 200))\ngrid$density &lt;- ifelse(grid$x + grid$y &lt;= 1,\n                       6 * (1 - grid$x - grid$y), NA)\n\nggplot(grid, aes(x, y, fill = density)) +\n  geom_tile() +\n  scale_fill_viridis_c(na.value = \"white\", name = \"f(x,y)\") +\n  labs(title = \"Joint density: f(x, y) = 6(1 - x - y)\",\n       x = \"Party A mobilization (x)\",\n       y = \"Party B mobilization (y)\") +\n  coord_fixed() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom the joint density we can derive the marginal \\(f_X(x) = 3(1-x)^2\\), the conditional \\(f_{Y|X}(y|x) = \\frac{2(1-x-y)}{(1-x)^2}\\), and the CEF \\(\\mathbb{E}[Y|X=x] = \\frac{1-x}{3}\\). Let’s verify by simulation.\n\n# Simulate from f(x,y) = 6(1-x-y) using rejection sampling\nset.seed(123)\nn_sim &lt;- 50000\nsamples &lt;- data.frame(x = numeric(0), y = numeric(0))\n\nwhile (nrow(samples) &lt; n_sim) {\n  x_prop &lt;- runif(n_sim)\n  y_prop &lt;- runif(n_sim)\n  valid &lt;- (x_prop + y_prop &lt;= 1)\n  x_prop &lt;- x_prop[valid]\n  y_prop &lt;- y_prop[valid]\n  # Accept with probability proportional to density\n  accept_prob &lt;- 6 * (1 - x_prop - y_prop) / 6  # max density is 6\n\n  keep &lt;- runif(length(x_prop)) &lt; accept_prob\n  samples &lt;- rbind(samples, data.frame(x = x_prop[keep], y = y_prop[keep]))\n}\nsamples &lt;- samples[1:n_sim, ]\n\n# Bin x and compute mean of y in each bin\nsamples$x_bin &lt;- cut(samples$x, breaks = 30)\ncef_empirical &lt;- aggregate(y ~ x_bin, data = samples, FUN = mean)\ncef_empirical$x_mid &lt;- seq(0.02, 0.98, length = nrow(cef_empirical))\n\n# Plot simulation vs. theoretical CEF\nggplot(cef_empirical, aes(x_mid, y)) +\n  geom_point(alpha = 0.7) +\n  stat_function(fun = function(x) (1 - x) / 3, color = \"red\", linewidth = 1) +\n  labs(x = \"x\", y = \"E[Y | X = x]\",\n       title = \"CEF: simulation vs. theory\",\n       subtitle = \"Red line: (1 - x)/3\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe simulated conditional means track the theoretical CEF closely. This is a case where the CEF happens to be linear — but that’s a special property of this density, not a general fact."
  },
  {
    "objectID": "ch01-review.html#sec-lie",
    "href": "ch01-review.html#sec-lie",
    "title": "1. Probability and Linear Algebra",
    "section": "4 The law of iterated expectations",
    "text": "4 The law of iterated expectations\nThe LIE says \\(\\mathbb{E}[\\mathbb{E}[Y|X]] = \\mathbb{E}[Y]\\). In words: the average of the conditional averages equals the overall average. Let’s check.\n\n# Overall mean of Y from simulation\noverall_mean &lt;- mean(samples$y)\n\n# Theoretical E[Y]: integrate y * f_Y(y)\n# f_Y(y) = 3(1-y)^2 by symmetry, so E[Y] = integral of y * 3(1-y)^2 from 0 to 1\nEY_theory &lt;- integrate(function(y) y * 3 * (1 - y)^2, 0, 1)$value\n\n# E[E[Y|X]] = E[(1-X)/3] = integral of (1-x)/3 * 3(1-x)^2 dx = integral of (1-x)^3 dx\nEEY_X &lt;- integrate(function(x) (1 - x)^3, 0, 1)$value\n\nc(simulation = overall_mean, `E[Y] theory` = EY_theory, `E[E[Y|X]]` = EEY_X)\n\n simulation E[Y] theory   E[E[Y|X]] \n       0.25        0.25        0.25 \n\n\nAll three agree: the LIE holds.\n\nTheorem 2 (Law of Iterated Expectations) For any random variables \\(X\\) and \\(Y\\), \\(\\mathbb{E}[\\mathbb{E}[Y|X]] = \\mathbb{E}[Y]\\). The average of the conditional averages equals the unconditional average."
  },
  {
    "objectID": "ch01-review.html#law-of-total-variance",
    "href": "ch01-review.html#law-of-total-variance",
    "title": "1. Probability and Linear Algebra",
    "section": "5 Law of total variance",
    "text": "5 Law of total variance\nThe law of total variance decomposes \\(\\text{Var}(Y) = \\mathbb{E}[\\text{Var}(Y|X)] + \\text{Var}(\\mathbb{E}[Y|X])\\). This is the foundation of \\(R^2\\).\n\n# Var(Y) from simulation\nvar_y_sim &lt;- var(samples$y)\n\n# Compute within each bin: variance of y and mean of y\nbin_stats &lt;- aggregate(y ~ x_bin, data = samples, FUN = function(z) c(m = mean(z), v = var(z), n = length(z)))\nbin_stats &lt;- do.call(data.frame, bin_stats)\n\n# E[Var(Y|X)] - average of conditional variances (weighted by bin size)\nweights &lt;- bin_stats$y.n / sum(bin_stats$y.n)\nE_var_YX &lt;- sum(weights * bin_stats$y.v)\n\n# Var(E[Y|X]) - variance of conditional means\nvar_EYX &lt;- sum(weights * (bin_stats$y.m - sum(weights * bin_stats$y.m))^2)\n\nc(`Var(Y)` = var_y_sim,\n  `E[Var(Y|X)] + Var(E[Y|X])` = E_var_YX + var_EYX,\n  `explained fraction` = var_EYX / var_y_sim)\n\n                   Var(Y) E[Var(Y|X)] + Var(E[Y|X])        explained fraction \n                   0.0375                    0.0375                    0.1125 \n\n\nThe decomposition works. The “explained fraction” is essentially \\(R^2\\): how much of the total variance of \\(Y\\) is captured by the CEF.\n\nTheorem 3 (Law of Total Variance) \\(\\text{Var}(Y) = \\mathbb{E}[\\text{Var}(Y|X)] + \\text{Var}(\\mathbb{E}[Y|X])\\). Total variance decomposes into average conditional variance plus variance of conditional means."
  },
  {
    "objectID": "ch01-review.html#from-probability-to-matrices",
    "href": "ch01-review.html#from-probability-to-matrices",
    "title": "1. Probability and Linear Algebra",
    "section": "6 From probability to matrices",
    "text": "6 From probability to matrices\nNow we shift gears. The CEF tells us what we’re trying to estimate. Linear algebra tells us how.\n\n6.1 The design matrix\nIn regression, we organize our data as \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{e}\\). Let’s build a design matrix and see what \\(\\mathbf{X}'\\mathbf{X}\\) and \\(\\mathbf{X}'\\mathbf{y}\\) look like.\n\nset.seed(2026)\nn &lt;- 50\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\ny &lt;- 2 + 3 * x1 - 1.5 * x2 + rnorm(n)\n\n# Build the design matrix (intercept + two regressors)\nX &lt;- cbind(1, x1, x2)\ncolnames(X) &lt;- c(\"intercept\", \"x1\", \"x2\")\nhead(X)\n\n     intercept      x1     x2\n[1,]         1  0.5206 -1.170\n[2,]         1 -1.0797 -0.206\n[3,]         1  0.1392 -0.931\n[4,]         1 -0.0847  0.449\n[5,]         1 -0.6666 -0.645\n[6,]         1 -2.5161 -0.231\n\n\n\n\n6.2 The Gram matrix \\(\\mathbf{X}'\\mathbf{X}\\)\nThis matrix encodes all second-moment information about the regressors. Its \\((i,j)\\) entry is the inner product of columns \\(i\\) and \\(j\\).\n\nXtX &lt;- t(X) %*% X\nXtX\n\n          intercept     x1    x2\nintercept    50.000 -0.813 -8.99\nx1           -0.813 46.685 11.93\nx2           -8.991 11.926 53.88\n\n# It's symmetric\nall.equal(XtX, t(XtX))\n\n[1] TRUE\n\n# And positive definite (all eigenvalues positive)\neigen(XtX)$values\n\n[1] 66.5 48.1 36.0\n\n\nAll eigenvalues are positive, so \\(\\mathbf{X}'\\mathbf{X}\\) is positive definite and invertible.\n\n\n\n\n\n\nNoteFull Rank Assumption\n\n\n\nOLS requires \\(\\mathbf{X}'\\mathbf{X}\\) to be invertible, which holds when \\(\\mathbf{X}\\) has full column rank — no column is a perfect linear combination of others. Equivalently, all eigenvalues of \\(\\mathbf{X}'\\mathbf{X}\\) must be strictly positive.\n\n\n\n\n6.3 Computing OLS by hand\nThe OLS estimator is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\\). Let’s compute it step by step.\n\nXty &lt;- t(X) %*% y\nbeta_hat &lt;- solve(XtX) %*% Xty\n\n# Compare to lm()\nbeta_lm &lt;- coef(lm(y ~ x1 + x2))\n\ncbind(manual = beta_hat, lm = beta_lm)\n\n                   lm\nintercept  2.13  2.13\nx1         2.93  2.93\nx2        -1.50 -1.50\n\n\nThey match. The solve() function computes the matrix inverse, and %*% does matrix multiplication.\n\n\n\n\n\n\nTipUse crossprod() for Speed\n\n\n\ncrossprod(X) computes \\(X'X\\) more efficiently than t(X) %*% X, and crossprod(X, y) computes \\(X'y\\). For large matrices, the speedup is substantial."
  },
  {
    "objectID": "ch01-review.html#sec-projection",
    "href": "ch01-review.html#sec-projection",
    "title": "1. Probability and Linear Algebra",
    "section": "7 Projection and residuals",
    "text": "7 Projection and residuals\nOLS is a projection. The hat matrix \\(\\mathbf{P} = \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\) projects \\(\\mathbf{y}\\) onto the column space of \\(\\mathbf{X}\\), and \\(\\mathbf{M} = \\mathbf{I} - \\mathbf{P}\\) projects onto its orthogonal complement.\n\nP &lt;- X %*% solve(XtX) %*% t(X)\nM &lt;- diag(n) - P\n\ny_hat &lt;- P %*% y      # fitted values\ne_hat &lt;- M %*% y      # residuals\n\n# Residuals are orthogonal to every column of X\nt(X) %*% e_hat\n\n               [,1]\nintercept -3.29e-14\nx1         4.09e-14\nx2         5.55e-15\n\n\nAll zeros (up to floating point). This is the matrix version of the OLS first-order conditions: \\(\\mathbf{X}'\\mathbf{e} = \\mathbf{0}\\).\n\n7.1 Idempotency\nBoth \\(\\mathbf{P}\\) and \\(\\mathbf{M}\\) are idempotent: applying them twice is the same as applying them once. Projecting an already-projected vector does nothing.\n\n# P^2 = P\nmax(abs(P %*% P - P))\n\n[1] 5.55e-17\n\n# M^2 = M\nmax(abs(M %*% M - M))\n\n[1] 4.44e-16\n\n# Eigenvalues of P are 0 or 1\nround(eigen(P)$values, 6)\n\n [1] 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 0\n\n\nThe hat matrix has \\(k = 3\\) eigenvalues equal to 1 (one per regressor) and \\(n - k = 47\\) equal to 0. The trace — and therefore the rank — equals \\(k\\).\n\nc(trace_P = sum(diag(P)), k = ncol(X))\n\ntrace_P       k \n      3       3 \n\nc(trace_M = sum(diag(M)), `n - k` = n - ncol(X))\n\ntrace_M   n - k \n     47      47 \n\n\nThis is where degrees of freedom come from. The residuals live in an \\((n-k)\\)-dimensional space, which is why we divide the sum of squared residuals by \\(n - k\\) to get an unbiased estimate of \\(\\sigma^2\\).\n\nsigma2_biased &lt;- sum(e_hat^2) / n\nsigma2_unbiased &lt;- sum(e_hat^2) / (n - ncol(X))\n\nc(biased = sigma2_biased, unbiased = sigma2_unbiased,\n  true = 1)  # we set sd = 1 in the DGP\n\n  biased unbiased     true \n    1.09     1.16     1.00"
  },
  {
    "objectID": "ch01-review.html#positive-definiteness-and-the-ols-bowl",
    "href": "ch01-review.html#positive-definiteness-and-the-ols-bowl",
    "title": "1. Probability and Linear Algebra",
    "section": "8 Positive definiteness and the OLS bowl",
    "text": "8 Positive definiteness and the OLS bowl\nWe saw that \\(\\mathbf{X}'\\mathbf{X}\\) has all positive eigenvalues — it’s positive definite. This has a visual payoff: the sum of squared residuals, as a function of \\(\\boldsymbol{\\beta}\\), is a bowl with a unique minimum.\n\n# Fix intercept at its OLS value, vary beta1 and beta2\nb0_hat &lt;- beta_hat[1]\nb1_grid &lt;- seq(1.5, 4.5, length = 50)\nb2_grid &lt;- seq(-3, 0, length = 50)\n\nssr_surface &lt;- outer(b1_grid, b2_grid, Vectorize(function(b1, b2) {\n  sum((y - X %*% c(b0_hat, b1, b2))^2)\n}))\n\ncontour(b1_grid, b2_grid, ssr_surface, nlevels = 25,\n        xlab = expression(beta[1]), ylab = expression(beta[2]),\n        main = \"Sum of Squared Residuals\")\npoints(beta_hat[2], beta_hat[3], pch = 19, col = \"red\", cex = 1.5)\n\n\n\n\n\n\n\n\nThe contours are ellipses (because \\(\\mathbf{X}'\\mathbf{X}\\) is positive definite), and the red dot marks the OLS solution at the center."
  },
  {
    "objectID": "ch01-review.html#the-normal-distribution",
    "href": "ch01-review.html#the-normal-distribution",
    "title": "1. Probability and Linear Algebra",
    "section": "9 The normal distribution",
    "text": "9 The normal distribution\nMany results in the course rely on normality, especially for small-sample inference. Here’s what different normal densities look like, and the key property that linear transformations preserve normality.\n\nggplot(data.frame(x = c(-5, 5)), aes(x)) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1),\n                aes(color = \"N(0, 1)\"), linewidth = 0.8) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = sqrt(0.5)),\n                aes(color = \"N(0, 0.5)\"), linewidth = 0.8) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = sqrt(2)),\n                aes(color = \"N(0, 2)\"), linewidth = 0.8) +\n  scale_color_manual(values = c(\"steelblue\", \"tomato\", \"forestgreen\"),\n                     name = \"\") +\n  labs(x = \"x\", y = \"f(x)\",\n       title = \"Normal densities with different variances\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# If X ~ N(2, 3), then aX + b ~ N(a*2 + b, a^2 * 3)\nset.seed(99)\nX_samp &lt;- rnorm(10000, mean = 2, sd = sqrt(3))\nY_samp &lt;- 4 * X_samp + 1  # a = 4, b = 1\n\nc(empirical_mean = mean(Y_samp), theoretical_mean = 4 * 2 + 1)\n\n  empirical_mean theoretical_mean \n            9.02             9.00 \n\nc(empirical_var = var(Y_samp), theoretical_var = 16 * 3)\n\n  empirical_var theoretical_var \n           48.1            48.0"
  },
  {
    "objectID": "ch01-review.html#summary",
    "href": "ch01-review.html#summary",
    "title": "1. Probability and Linear Algebra",
    "section": "10 Summary",
    "text": "10 Summary\nThis chapter covered the computational foundations:\n\nExpectation minimizes mean squared error — the simplest prediction problem.\nThe CEF generalizes this: the best prediction of \\(Y\\) given \\(X\\).\nThe law of iterated expectations connects conditional and unconditional means.\nThe law of total variance decomposes variance into explained and unexplained parts (\\(R^2\\)).\nThe design matrix \\(\\mathbf{X}\\) and Gram matrix \\(\\mathbf{X}'\\mathbf{X}\\) encode the regression problem.\nOLS is a projection of \\(\\mathbf{y}\\) onto the column space of \\(\\mathbf{X}\\).\nPositive definiteness guarantees a unique solution and a bowl-shaped objective.\nDegrees of freedom (\\(n - k\\)) come from the dimension of the residual space.\n\nNext: The CEF and Best Linear Predictor — what happens when the CEF isn’t linear, and why linear regression still works."
  },
  {
    "objectID": "ch10-iv.html",
    "href": "ch10-iv.html",
    "title": "10. Instrumental Variables and 2SLS",
    "section": "",
    "text": "When a regressor is correlated with the error — whether from omitted variables, measurement error, or simultaneity — OLS is inconsistent. No amount of data fixes this. Instrumental variables (IV) offer a way out: find a variable \\(Z\\) that shifts the endogenous regressor but is otherwise unrelated to the outcome.\nThis chapter builds the IV toolkit through simulation and application:\nlibrary(ggplot2)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(estimatr)\nlibrary(car)\nlibrary(carData)\nlibrary(AER)\noptions(digits = 4)"
  },
  {
    "objectID": "ch10-iv.html#the-endogeneity-problem",
    "href": "ch10-iv.html#the-endogeneity-problem",
    "title": "10. Instrumental Variables and 2SLS",
    "section": "1 The endogeneity problem",
    "text": "1 The endogeneity problem\nSuppose the true model is \\(Y = 1 + 0.5 X + e\\), but \\(X\\) and \\(e\\) are correlated. We generate this by letting both \\(X\\) and \\(e\\) depend on an unobserved confounder:\n\nset.seed(42)\nn &lt;- 500\n\n## Data-generating process\nz &lt;- rnorm(n)               # instrument (exogenous)\nv &lt;- rnorm(n)               # first-stage error\ne &lt;- 0.8 * v + rnorm(n, sd = 0.5)  # structural error (correlated with v!)\nx &lt;- 1.5 * z + v            # endogenous regressor\ny &lt;- 1 + 0.5 * x + e        # structural equation\ndat &lt;- data.frame(y, x, z)\n\nBecause \\(v\\) appears in both \\(x\\) and (through \\(e\\)), \\(\\text{Cor}(x, e) \\neq 0\\). OLS is biased and inconsistent:\n\nols &lt;- lm(y ~ x, data = dat)\ncat(\"True β =\", 0.5, \"\\n\")\n\nTrue β = 0.5 \n\ncat(\"OLS  β =\", round(coef(ols)[\"x\"], 4), \"\\n\")\n\nOLS  β = 0.7753 \n\n\nThe bias does not vanish with more data — OLS converges to the wrong value:\n\nset.seed(1)\nn_vals &lt;- c(50, 200, 1000, 5000, 20000)\nB &lt;- 500\n\nsim_ols &lt;- function(n) {\n  z &lt;- rnorm(n)\n  v &lt;- rnorm(n)\n  e &lt;- 0.8 * v + rnorm(n, sd = 0.5)\n  x &lt;- 1.5 * z + v\n  y &lt;- 1 + 0.5 * x + e\n  coef(lm(y ~ x))[2]\n}\n\ndf_ols &lt;- do.call(rbind, lapply(n_vals, function(nn) {\n  data.frame(estimate = replicate(B, sim_ols(nn)), n = nn)\n}))\n\nggplot(df_ols, aes(x = estimate)) +\n  geom_histogram(bins = 40, alpha = 0.6, fill = \"steelblue\") +\n  geom_vline(xintercept = 0.5, linetype = \"dashed\", color = \"coral\", linewidth = 1) +\n  facet_wrap(~ n, labeller = label_both, scales = \"free_y\") +\n  labs(title = \"OLS is inconsistent under endogeneity\",\n       subtitle = \"The distribution concentrates on the wrong value as n grows\",\n       x = expression(hat(beta)[OLS]))\n\n\n\n\nOLS is inconsistent under endogeneity — the distribution concentrates on the wrong value\n\n\n\n\n\n\n\n\n\n\nNoteEndogeneity Means Inconsistency\n\n\n\nWhen \\(\\text{Cov}(X, e) \\neq 0\\), OLS converges to the wrong value — bias does not shrink with larger samples. This is fundamentally different from finite-sample bias (which vanishes as \\(n \\to \\infty\\)). IV trades precision for consistency."
  },
  {
    "objectID": "ch10-iv.html#sec-iv-estimator",
    "href": "ch10-iv.html#sec-iv-estimator",
    "title": "10. Instrumental Variables and 2SLS",
    "section": "2 The IV estimator",
    "text": "2 The IV estimator\nThe IV estimator uses the instrument \\(Z\\) (which is uncorrelated with \\(e\\)) to isolate the exogenous variation in \\(X\\):\n\\[\\hat{\\beta}_{IV} = (Z'X)^{-1} Z'Y \\tag{1}\\]\nThis is the sample analogue of \\(\\beta = \\text{Cov}(Z, Y) / \\text{Cov}(Z, X)\\).\n\nTheorem 1 (IV Estimator) The IV estimator \\(\\hat\\beta_{IV} = (Z'X)^{-1}Z'Y\\) is consistent for \\(\\beta\\) when \\(\\mathbb{E}[Z_i e_i] = 0\\) (instrument exogeneity) and \\(\\mathbb{E}[Z_i X_i'] \\neq 0\\) (instrument relevance). It is the sample analogue of \\(\\beta = \\text{Cov}(Z, Y) / \\text{Cov}(Z, X)\\).\n\n\n## IV by hand (just-identified: one instrument, one endogenous regressor)\nZ &lt;- cbind(1, dat$z)   # instrument matrix (with intercept)\nX &lt;- cbind(1, dat$x)   # regressor matrix\n\nbeta_iv &lt;- solve(t(Z) %*% X) %*% t(Z) %*% dat$y\ncat(\"IV estimate (by hand):\", round(beta_iv[2], 4), \"\\n\")\n\nIV estimate (by hand): 0.5086 \n\ncat(\"True β:               \", 0.5, \"\\n\")\n\nTrue β:                0.5 \n\n\n\n2.1 Using iv_robust()\nThe estimatr package provides iv_robust(), which handles 2SLS estimation with heteroskedasticity-robust standard errors:\n\n## Syntax: Y ~ endogenous + exogenous | instruments + exogenous\niv_fit &lt;- iv_robust(y ~ x | z, data = dat)\nsummary(iv_fit)\n\n\nCall:\niv_robust(formula = y ~ x | z, data = dat)\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper  DF\n(Intercept)    0.963     0.0426    22.6 2.95e-78    0.879    1.046 498\nx              0.509     0.0286    17.8 4.67e-55    0.452    0.565 498\n\nMultiple R-squared:  0.65 , Adjusted R-squared:  0.65 \nF-statistic:  316 on 1 and 498 DF,  p-value: &lt;2e-16\n\n\nThe coefficient on x is close to the true value of 0.5. Compare to OLS:\n\ndata.frame(\n  Estimator = c(\"OLS\", \"IV\"),\n  Estimate = c(coef(ols)[\"x\"], coef(iv_fit)[\"x\"]),\n  SE = c(sqrt(vcovHC(ols)[\"x\", \"x\"]), iv_fit$std.error[\"x\"]),\n  row.names = NULL\n)\n\n  Estimator Estimate      SE\n1       OLS   0.7753 0.02043\n2        IV   0.5086 0.02863\n\n\nIV is consistent but less precise — the standard error is larger. This is the cost of correcting for endogeneity.\n\n\n2.2 The Wald estimator\nWhen the instrument is binary, the IV estimator simplifies to the Wald ratio: the reduced-form effect on \\(Y\\) divided by the first-stage effect on \\(X\\).\n\n## Create a binary instrument version\nset.seed(42)\nn &lt;- 1000\nz_bin &lt;- rbinom(n, 1, 0.5)          # binary instrument\nv &lt;- rnorm(n)\ne &lt;- 0.8 * v + rnorm(n, sd = 0.5)\nx &lt;- 0.6 * z_bin + v                 # first stage\ny &lt;- 1 + 0.5 * x + e                 # structural equation\n\n## Wald estimator by hand\ny1 &lt;- mean(y[z_bin == 1]); y0 &lt;- mean(y[z_bin == 0])\nx1 &lt;- mean(x[z_bin == 1]); x0 &lt;- mean(x[z_bin == 0])\nwald &lt;- (y1 - y0) / (x1 - x0)\n\ncat(\"Reduced form (Y):  \", round(y1 - y0, 4), \"\\n\")\n\nReduced form (Y):   0.3984 \n\ncat(\"First stage (X):   \", round(x1 - x0, 4), \"\\n\")\n\nFirst stage (X):    0.6712 \n\ncat(\"Wald estimate:     \", round(wald, 4), \"\\n\")\n\nWald estimate:      0.5936 \n\ncat(\"True β:             0.5\\n\")\n\nTrue β:             0.5\n\n\nThe Wald ratio is the intent-to-treat (ITT) effect on \\(Y\\) rescaled by the first-stage compliance rate. This is the building block of LATE, which we develop below."
  },
  {
    "objectID": "ch10-iv.html#sec-2sls",
    "href": "ch10-iv.html#sec-2sls",
    "title": "10. Instrumental Variables and 2SLS",
    "section": "3 Two-stage least squares",
    "text": "3 Two-stage least squares\nWhen there are more instruments than endogenous regressors (\\(l &gt; k\\), overidentification), we use 2SLS:\n\nStage 1: Regress \\(X\\) on all instruments \\(Z\\) to get fitted values \\(\\hat{X} = P_Z X\\)\nStage 2: Regress \\(Y\\) on \\(\\hat{X}\\) (but compute SEs using the original \\(X\\))\n\n\nTheorem 2 (Two-Stage Least Squares) The 2SLS estimator \\(\\hat\\beta_{2SLS} = (X'P_Z X)^{-1}X'P_Z Y\\), where \\(P_Z = Z(Z'Z)^{-1}Z'\\), handles overidentification (\\(\\ell &gt; k\\)) by projecting \\(X\\) onto the instrument space before applying OLS. It equals the GMM estimator with weight matrix \\((Z'Z)^{-1}\\).\n\n\n## DGP with two instruments for one endogenous regressor\nset.seed(42)\nn &lt;- 500\nz1 &lt;- rnorm(n)\nz2 &lt;- rnorm(n)\nv &lt;- rnorm(n)\ne &lt;- 0.8 * v + rnorm(n, sd = 0.5)\nx &lt;- 0.8 * z1 + 0.6 * z2 + v   # two instruments\ny &lt;- 1 + 0.5 * x + e\n\ndat2 &lt;- data.frame(y, x, z1, z2)\n\n\n3.1 2SLS by hand\n\n## Stage 1: regress X on instruments\nstage1 &lt;- lm(x ~ z1 + z2, data = dat2)\ndat2$x_hat &lt;- fitted(stage1)\n\n## Stage 2: regress Y on fitted X\nstage2_naive &lt;- lm(y ~ x_hat, data = dat2)\n\n## The coefficient is correct, but the SEs are wrong!\n## Correct SEs use the original X, not x_hat\ncat(\"2SLS coefficient (from two stages):\", round(coef(stage2_naive)[\"x_hat\"], 4), \"\\n\")\n\n2SLS coefficient (from two stages): 0.5194 \n\ncat(\"Naive SE (wrong):                  \", round(summary(stage2_naive)$coef[\"x_hat\", 2], 4), \"\\n\")\n\nNaive SE (wrong):                   0.0586 \n\n\nThe naive second-stage SEs are incorrect because they treat \\(\\hat{X}\\) as if it were \\(X\\). The iv_robust() function handles this automatically:\n\niv_fit2 &lt;- iv_robust(y ~ x | z1 + z2, data = dat2)\ncat(\"2SLS coefficient (iv_robust):      \", round(coef(iv_fit2)[\"x\"], 4), \"\\n\")\n\n2SLS coefficient (iv_robust):       0.5194 \n\ncat(\"Correct robust SE:                 \", round(iv_fit2$std.error[\"x\"], 4), \"\\n\")\n\nCorrect robust SE:                  0.0384 \n\n\n\n\n3.2 Verifying the matrix formula\nThe 2SLS estimator is \\(\\hat{\\beta}_{2SLS} = (X'P_Z X)^{-1} X'P_Z Y\\) where \\(P_Z = Z(Z'Z)^{-1}Z'\\):\n\nZ &lt;- cbind(1, dat2$z1, dat2$z2)\nX &lt;- cbind(1, dat2$x)\nY &lt;- dat2$y\n\nPZ &lt;- Z %*% solve(t(Z) %*% Z) %*% t(Z)\nbeta_2sls &lt;- solve(t(X) %*% PZ %*% X) %*% t(X) %*% PZ %*% Y\n\ncat(\"Matrix formula:\", round(beta_2sls[2], 4), \"\\n\")\n\nMatrix formula: 0.5194 \n\ncat(\"iv_robust():  \", round(coef(iv_fit2)[\"x\"], 4), \"\\n\")\n\niv_robust():   0.5194"
  },
  {
    "objectID": "ch10-iv.html#instrument-strength-and-weak-instruments",
    "href": "ch10-iv.html#instrument-strength-and-weak-instruments",
    "title": "10. Instrumental Variables and 2SLS",
    "section": "4 Instrument strength and weak instruments",
    "text": "4 Instrument strength and weak instruments\nIV pays for consistency with precision. The asymptotic variance of IV relative to OLS is:\n\\[\\text{Avar}(\\hat{\\beta}_{IV}) = \\frac{\\text{Avar}(\\hat{\\beta}_{OLS})}{\\rho^2_{ZX}}\\]\nWhen the instrument is weak (\\(\\rho_{ZX} \\approx 0\\)), IV becomes wildly imprecise and biased. The first-stage F-statistic measures instrument strength.\n\n4.1 Simulation: bias grows as instruments weaken\n\nset.seed(12)\nB &lt;- 1000\nn &lt;- 200\n\n## Vary the first-stage coefficient (instrument strength)\npi_vals &lt;- c(0.02, 0.05, 0.1, 0.2, 0.5, 1.0)\n\nresults &lt;- do.call(rbind, lapply(pi_vals, function(pi) {\n  estimates &lt;- replicate(B, {\n    z &lt;- rnorm(n)\n    v &lt;- rnorm(n)\n    e &lt;- 0.8 * v + rnorm(n, sd = 0.5)\n    x &lt;- pi * z + v\n    y &lt;- 1 + 0.5 * x + e\n    fit &lt;- iv_robust(y ~ x | z, data = data.frame(y, x, z))\n    c(iv = coef(fit)[\"x\"],\n      F_stat = summary(lm(x ~ z))$fstatistic[1])\n  })\n  data.frame(\n    pi = pi,\n    iv_est = estimates[\"iv.x\", ],\n    F_stat = estimates[\"F_stat.value\", ],\n    label = paste0(\"π = \", pi, \"\\nmed F = \", round(median(estimates[\"F_stat.value\", ]), 1))\n  )\n}))\n\nggplot(results, aes(iv_est)) +\n  geom_histogram(bins = 50, alpha = 0.6, fill = \"steelblue\") +\n  geom_vline(xintercept = 0.5, color = \"coral\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = coef(lm(y ~ x, data = dat))[\"x\"],\n             color = \"gray50\", linetype = \"dotted\", linewidth = 0.8) +\n  facet_wrap(~ label, scales = \"free_y\") +\n  coord_cartesian(xlim = c(-2, 3)) +\n  labs(title = \"Weak instruments bias IV toward OLS\",\n       subtitle = \"Dashed red = true β. Dotted gray = OLS probability limit.\",\n       x = expression(hat(beta)[IV]))\n\n\n\n\n\n\n\n\nKey patterns:\n\nStrong instruments (large \\(\\pi\\), high F): IV is centered on the true value\nWeak instruments (small \\(\\pi\\), low F): IV is biased toward OLS and has enormous variance\nThe F &gt; 10 rule of thumb (Staiger & Stock 1997) marks the boundary of acceptable instrument strength\n\n\n\n\n\n\n\nWarningWeak Instruments: F &lt; 10 Rule of Thumb\n\n\n\nWhen the first-stage F-statistic is below 10 (Staiger & Stock, 1997), IV is biased toward OLS, wildly imprecise, and confidence intervals have poor coverage. Always report the first-stage F and treat results with F &lt; 10 as unreliable.\n\n\n\n\n4.2 The bias formula\nThe approximate bias of IV relative to OLS is:\n\\[\\frac{\\text{Bias}(\\hat{\\beta}_{IV})}{\\text{Bias}(\\hat{\\beta}_{OLS})} \\approx \\frac{1}{F} \\tag{2}\\]\n\n## Compute median bias ratio by F-stat bins\nresults$F_bin &lt;- cut(results$F_stat, breaks = c(0, 5, 10, 20, 50, 100, Inf),\n                     labels = c(\"&lt;5\", \"5-10\", \"10-20\", \"20-50\", \"50-100\", \"&gt;100\"))\n\nbias_ols &lt;- 0.8 / (0.8^2 + 1)  # approximate OLS bias (rho_xe * sigma_e / sigma_x)\nbias_by_F &lt;- aggregate(iv_est ~ F_bin, data = results, function(x) mean(x) - 0.5)\nbias_by_F$bias_ratio &lt;- bias_by_F$iv_est / bias_ols\n\nggplot(bias_by_F, aes(F_bin, bias_ratio)) +\n  geom_col(fill = \"steelblue\", alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"IV bias as a fraction of OLS bias\",\n       subtitle = \"Weak instruments (low F) leave most of the OLS bias uncorrected\",\n       x = \"First-stage F-statistic\", y = \"Bias(IV) / Bias(OLS)\")"
  },
  {
    "objectID": "ch10-iv.html#testing-endogeneity-and-instrument-validity",
    "href": "ch10-iv.html#testing-endogeneity-and-instrument-validity",
    "title": "10. Instrumental Variables and 2SLS",
    "section": "5 Testing: endogeneity and instrument validity",
    "text": "5 Testing: endogeneity and instrument validity\n\n5.1 The Hausman test via the control function\nThe control function approach makes endogeneity testing transparent. Add the first-stage residuals \\(\\hat{v}\\) to the structural equation. If their coefficient \\(\\hat{\\alpha}\\) is significant, \\(X\\) is endogenous:\n\n## Use the overidentified simulation data\nstage1 &lt;- lm(x ~ z1 + z2, data = dat2)\ndat2$v_hat &lt;- residuals(stage1)\n\n## Control function regression\ncf_reg &lt;- lm(y ~ x + v_hat, data = dat2)\ncoeftest(cf_reg, vcov. = vcovHC(cf_reg, type = \"HC1\"))\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.9838     0.0224    43.9   &lt;2e-16 ***\nx             0.5194     0.0226    22.9   &lt;2e-16 ***\nv_hat         0.7616     0.0318    23.9   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe coefficient on x is the IV estimate of \\(\\beta\\). The t-test on v_hat is the Hausman endogeneity test — if significant, \\(X\\) is endogenous and IV is needed.\n\ncat(\"Control function β on x:\", round(coef(cf_reg)[\"x\"], 4), \"\\n\")\n\nControl function β on x: 0.5194 \n\ncat(\"2SLS β:                 \", round(coef(iv_fit2)[\"x\"], 4), \"\\n\")\n\n2SLS β:                  0.5194 \n\ncat(\"Hausman p-value (α = 0):\", round(coeftest(cf_reg)[\"v_hat\", 4], 4), \"\\n\")\n\nHausman p-value (α = 0): 0 \n\n\nThe two estimates are numerically identical (under homoskedasticity). The Hausman test rejects, confirming that OLS is inconsistent here.\n\n\n5.2 The Sargan test for overidentifying restrictions\nWith more instruments than endogenous regressors (\\(l &gt; k\\)), we can test whether the extra instruments are valid. The Sargan/J-test checks whether the overidentifying restrictions hold:\n\n## Sargan test by hand\n## iv_robust stores residuals as y - X %*% beta_2sls\ne_hat &lt;- dat2$y - iv_fit2$fitted.values\nsargan_reg &lt;- lm(e_hat ~ z1 + z2, data = dat2)\nJ_stat &lt;- n * summary(sargan_reg)$r.squared\nq &lt;- 1  # l - k = 3 instruments - 2 parameters = 1 overidentifying restriction\np_sargan &lt;- 1 - pchisq(J_stat, q)\n\ncat(\"Sargan J-statistic:\", round(J_stat, 3), \"\\n\")\n\nSargan J-statistic: 0.162 \n\ncat(\"p-value:           \", round(p_sargan, 3), \"\\n\")\n\np-value:            0.688 \n\ncat(\"(Fail to reject =&gt; instruments are consistent with validity)\\n\")\n\n(Fail to reject =&gt; instruments are consistent with validity)\n\n\nThe Sargan test cannot detect invalid instruments if all instruments are invalid in the same way. It only has power when some instruments are valid and others are not."
  },
  {
    "objectID": "ch10-iv.html#late-what-does-iv-estimate-under-heterogeneity",
    "href": "ch10-iv.html#late-what-does-iv-estimate-under-heterogeneity",
    "title": "10. Instrumental Variables and 2SLS",
    "section": "6 LATE: what does IV estimate under heterogeneity?",
    "text": "6 LATE: what does IV estimate under heterogeneity?\nWhen treatment effects vary across individuals, IV does not estimate the average treatment effect (ATE). With a binary instrument, it estimates the Local Average Treatment Effect (LATE) — the effect for compliers, those whose treatment status is changed by the instrument.\n\nDefinition 1 (Local Average Treatment Effect (LATE)) With a binary instrument, IV estimates the LATE: the average treatment effect for compliers — individuals whose treatment status is changed by the instrument. LATE is instrument-dependent: different instruments identify different complier populations.\n\n\n\n\n\n\n\nWarningLATE Is Local\n\n\n\nIV does not estimate the average treatment effect (ATE) for the full population. It estimates the effect for compliers only — a subgroup that cannot be identified individually. With heterogeneous effects, different instruments give different LATEs.\n\n\n\n6.1 Simulation with heterogeneous effects\n\nset.seed(303)\nn &lt;- 5000\n\n## Individual treatment effects: drawn from N(0.5, 0.3^2)\ntau_i &lt;- rnorm(n, mean = 0.5, sd = 0.3)\n\n## Potential outcomes\ny0 &lt;- rnorm(n, mean = 2)\ny1 &lt;- y0 + tau_i\n\n## Binary instrument\nz &lt;- rbinom(n, 1, 0.5)\n\n## Compliance types:\n## Define individual-level threshold (unobserved resistance)\n## Low threshold = always-taker, high threshold = never-taker\nu_resist &lt;- runif(n)\nalways_taker &lt;- (u_resist &lt; 0.2)\nnever_taker  &lt;- (u_resist &gt; 0.7)\ncomplier     &lt;- !always_taker & !never_taker\n\n## Treatment assignment: compliers follow instrument, others don't\nd &lt;- ifelse(always_taker, 1,\n     ifelse(never_taker, 0,\n     z))  # compliers: D = Z\n\n## Make compliers have systematically different treatment effects\n## (essential heterogeneity: those who comply have larger effects)\ntau_i[complier] &lt;- tau_i[complier] + 0.2\n\n## Observed outcome\ny &lt;- ifelse(d == 1, y1, y0)\ndat_late &lt;- data.frame(y, d, z, complier, tau_i)\n\ncat(\"True ATE:                    \", round(mean(tau_i), 3), \"\\n\")\n\nTrue ATE:                     0.609 \n\ncat(\"True ATT:                    \", round(mean(tau_i[d == 1]), 3), \"\\n\")\n\nTrue ATT:                     0.626 \n\ncat(\"True LATE (complier effect): \", round(mean(tau_i[complier]), 3), \"\\n\")\n\nTrue LATE (complier effect):  0.708 \n\n\nNow compare OLS, the naive difference in means, and IV:\n\n## OLS (biased: always-takers inflate the treated group)\nols_est &lt;- coef(lm(y ~ d, data = dat_late))[\"d\"]\n\n## Wald / IV estimate\nwald_est &lt;- (mean(y[z == 1]) - mean(y[z == 0])) / (mean(d[z == 1]) - mean(d[z == 0]))\n\n## iv_robust\niv_late &lt;- iv_robust(y ~ d | z, data = dat_late)\n\ndata.frame(\n  Estimand = c(\"ATE (true)\", \"LATE (true)\", \"OLS\", \"IV / Wald\"),\n  Value = round(c(mean(tau_i), mean(tau_i[complier]), ols_est, coef(iv_late)[\"d\"]), 3),\n  row.names = NULL\n)\n\n     Estimand Value\n1  ATE (true) 0.609\n2 LATE (true) 0.708\n3         OLS 0.515\n4   IV / Wald 0.576\n\n\nIV recovers the LATE — the effect for compliers — not the ATE. This is instrument-dependent: a different instrument would identify a different set of compliers with potentially different effects.\n\n\n6.2 Visualizing compliance types\n\ndat_late$type &lt;- ifelse(always_taker, \"Always-taker\",\n                 ifelse(never_taker, \"Never-taker\", \"Complier\"))\ndat_late$type &lt;- factor(dat_late$type, levels = c(\"Never-taker\", \"Complier\", \"Always-taker\"))\n\nggplot(dat_late, aes(tau_i, fill = type)) +\n  geom_histogram(bins = 40, alpha = 0.6, position = \"identity\") +\n  geom_vline(aes(xintercept = mean(tau_i[complier]), color = \"LATE\"),\n             linetype = \"dashed\", linewidth = 1) +\n  geom_vline(aes(xintercept = mean(tau_i), color = \"ATE\"),\n             linetype = \"dashed\", linewidth = 1) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_manual(values = c(\"ATE\" = \"coral\", \"LATE\" = \"steelblue\"),\n                     name = \"Estimand\") +\n  labs(title = \"Treatment effects by compliance type\",\n       subtitle = \"IV estimates the LATE (complier average), not the ATE (overall average)\",\n       x = expression(tau[i] == Y[i](1) - Y[i](0)), fill = \"Type\")"
  },
  {
    "objectID": "ch10-iv.html#applied-example-cigarette-demand-elasticity",
    "href": "ch10-iv.html#applied-example-cigarette-demand-elasticity",
    "title": "10. Instrumental Variables and 2SLS",
    "section": "7 Applied example: cigarette demand elasticity",
    "text": "7 Applied example: cigarette demand elasticity\nWe estimate the price elasticity of cigarette demand using state-level panel data from Stock and Watson. Prices are endogenous (supply and demand are jointly determined), so we instrument with cigarette-specific sales taxes.\n\ndata(\"CigarettesSW\", package = \"AER\")\n\n## Use 1995 cross-section\ncig95 &lt;- subset(CigarettesSW, year == \"1995\")\ncig85 &lt;- subset(CigarettesSW, year == \"1985\")\n\n## Create real variables (deflate by CPI)\ncig95$rprice &lt;- cig95$price / cig95$cpi\ncig95$rtaxs  &lt;- cig95$taxs / cig95$cpi   # general + cig-specific tax\ncig95$rtax   &lt;- cig95$tax / cig95$cpi    # cigarette-specific tax only\ncig95$rincome &lt;- cig95$income / (cig95$population * cig95$cpi)\n\n## Log transform for elasticity interpretation\ncig95$lnpacks &lt;- log(cig95$packs)\ncig95$lnprice &lt;- log(cig95$rprice)\ncig95$lnrincome &lt;- log(cig95$rincome)\n\n\n7.1 OLS: biased demand elasticity\n\nols_cig &lt;- lm(lnpacks ~ lnprice + lnrincome, data = cig95)\ncoeftest(ols_cig, vcov. = vcovHC(ols_cig, type = \"HC1\"))\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   10.342      0.966   10.70  6.0e-14 ***\nlnprice       -1.407      0.261   -5.39  2.5e-06 ***\nlnrincome      0.344      0.260    1.32     0.19    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe OLS price elasticity is likely biased toward zero (or even positive) because high-demand states drive up prices — simultaneity bias.\n\n\n7.2 First stage: do taxes predict prices?\n\nfirst_stage &lt;- lm(lnprice ~ rtax + lnrincome, data = cig95)\ncoeftest(first_stage, vcov. = vcovHC(first_stage, type = \"HC1\"))\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.170512   0.121836   34.23  &lt; 2e-16 ***\nrtax        0.011226   0.000896   12.54  2.8e-16 ***\nlnrincome   0.080330   0.054332    1.48     0.15    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncat(\"\\nFirst-stage F-statistic:\", round(summary(first_stage)$fstatistic[1], 1), \"\\n\")\n\n\nFirst-stage F-statistic: 203.5 \n\n\nTaxes strongly predict prices (F well above 10). The instrument is relevant.\n\n\n7.3 2SLS: IV demand elasticity\n\niv_cig &lt;- iv_robust(lnpacks ~ lnprice + lnrincome | rtax + lnrincome,\n                    data = cig95)\nsummary(iv_cig)\n\n\nCall:\niv_robust(formula = lnpacks ~ lnprice + lnrincome | rtax + lnrincome, \n    data = cig95)\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF\n(Intercept)   10.024      1.025    9.78 1.05e-12    7.958   12.089 45\nlnprice       -1.315      0.259   -5.08 7.16e-06   -1.836   -0.793 45\nlnrincome      0.299      0.248    1.20 2.35e-01   -0.201    0.798 45\n\nMultiple R-squared:  0.431 ,    Adjusted R-squared:  0.406 \nF-statistic: 14.9 on 2 and 45 DF,  p-value: 1.05e-05\n\n\n\n\n7.4 With two instruments (overidentified)\n\n## Use both cigarette-specific tax and general sales tax\niv_cig2 &lt;- iv_robust(lnpacks ~ lnprice + lnrincome | rtax + rtaxs + lnrincome,\n                     data = cig95)\nsummary(iv_cig2)\n\n\nCall:\niv_robust(formula = lnpacks ~ lnprice + lnrincome | rtax + rtaxs + \n    lnrincome, data = cig95)\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF\n(Intercept)     9.89      0.978   10.12 3.57e-13    7.926   11.864 45\nlnprice        -1.28      0.255   -5.02 8.74e-06   -1.790   -0.764 45\nlnrincome       0.28      0.255    1.10 2.77e-01   -0.233    0.793 45\n\nMultiple R-squared:  0.429 ,    Adjusted R-squared:  0.404 \nF-statistic: 15.5 on 2 and 45 DF,  p-value: 7.55e-06\n\n\n\n\n7.5 Comparing OLS and IV\n\ncomparison &lt;- data.frame(\n  Estimator = c(\"OLS\", \"IV (one instrument)\", \"IV (two instruments)\"),\n  Elasticity = c(coef(ols_cig)[\"lnprice\"],\n                 coef(iv_cig)[\"lnprice\"],\n                 coef(iv_cig2)[\"lnprice\"]),\n  SE = c(sqrt(vcovHC(ols_cig)[\"lnprice\", \"lnprice\"]),\n         iv_cig$std.error[\"lnprice\"],\n         iv_cig2$std.error[\"lnprice\"]),\n  row.names = NULL\n)\ncomparison\n\n             Estimator Elasticity     SE\n1                  OLS     -1.407 0.2750\n2  IV (one instrument)     -1.315 0.2590\n3 IV (two instruments)     -1.277 0.2547\n\n\nThe IV estimate of the price elasticity is more negative than OLS, consistent with the simultaneity story: OLS understates the (negative) demand response because high demand pushes prices up.\n\n\n7.6 Diagnostics\n\n## Hausman test via control function\ncig95$v_hat &lt;- residuals(lm(lnprice ~ rtax + lnrincome, data = cig95))\ncf_cig &lt;- lm(lnpacks ~ lnprice + lnrincome + v_hat, data = cig95)\ncat(\"Hausman test (t on v_hat):\\n\")\n\nHausman test (t on v_hat):\n\ncoeftest(cf_cig, vcov. = vcovHC(cf_cig, type = \"HC1\"))[\"v_hat\", , drop = FALSE]\n\n      Estimate Std. Error t value Pr(&gt;|t|)\nv_hat  -0.6682     0.6949 -0.9616   0.3415\n\n\n\n## Sargan test (overidentified model with 2 instruments)\ne_iv &lt;- cig95$lnpacks - iv_cig2$fitted.values\nsargan_cig &lt;- lm(e_iv ~ rtax + rtaxs + lnrincome, data = cig95)\nJ &lt;- nrow(cig95) * summary(sargan_cig)$r.squared\ncat(\"\\nSargan J-stat:\", round(J, 3), \"  p-value:\", round(1 - pchisq(J, 1), 3), \"\\n\")\n\n\nSargan J-stat: 0.333   p-value: 0.564"
  },
  {
    "objectID": "ch10-iv.html#connection-to-gmm",
    "href": "ch10-iv.html#connection-to-gmm",
    "title": "10. Instrumental Variables and 2SLS",
    "section": "8 Connection to GMM",
    "text": "8 Connection to GMM\nIV and 2SLS are special cases of the Generalized Method of Moments (GMM). We will see in Chapter 11 that 2SLS is a special case of GMM. The moment condition \\(E[Z_i(Y_i - X_i'\\beta)] = 0\\) defines both:\n\n\n\nMethod\nWeighting matrix \\(\\hat{W}\\)\nWhen optimal?\n\n\n\n\nIV (just-identified)\n\\((Z'Z/n)^{-1}\\)\nAlways (unique solution)\n\n\n2SLS\n\\((Z'Z/n)^{-1}\\)\nUnder homoskedasticity\n\n\nEfficient GMM\n\\(\\hat{\\Omega}^{-1}\\)\nUnder heteroskedasticity\n\n\n\nUnder homoskedasticity, 2SLS is efficient among all IV estimators. Under heteroskedasticity, efficient GMM does better by weighting moment conditions inversely to their variance.\nThe Sargan/J-test is the overidentification test of GMM: it checks whether the extra moment conditions (beyond what is needed for identification) are satisfied. We develop the full GMM framework in Chapter 11."
  },
  {
    "objectID": "ch05-gls.html",
    "href": "ch05-gls.html",
    "title": "5. Efficiency and GLS",
    "section": "",
    "text": "When error variances differ across observations, OLS is still unbiased but no longer efficient. This chapter develops WLS and GLS as the natural response: weight observations by their precision. We build everything from matrix algebra, connect the estimator to the method of moments, and implement feasible GLS when the variance structure must be estimated from data. Along the way we introduce the sandwich and estimatr packages — the practical tools for robust inference in R.\nQuestions this chapter answers:\nlibrary(ggplot2)\nlibrary(MASS)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(estimatr)\nlibrary(carData)\ndata(Prestige)\noptions(digits = 3)\ntr &lt;- function(M) sum(diag(M))"
  },
  {
    "objectID": "ch05-gls.html#sec-sandwich",
    "href": "ch05-gls.html#sec-sandwich",
    "title": "5. Efficiency and GLS",
    "section": "1 The variance of OLS under non-spherical errors",
    "text": "1 The variance of OLS under non-spherical errors\nRecall the OLS estimator: \\(\\hat{\\beta} = \\beta + (X'X)^{-1}X'e\\). The variance is:\n\\[\\text{Var}[\\hat{\\beta} \\mid X] = (X'X)^{-1} X' \\text{Var}[e \\mid X] \\, X (X'X)^{-1}\\]\nUnder homoskedasticity (\\(\\text{Var}[e \\mid X] = \\sigma^2 I\\)), this simplifies to \\(\\sigma^2(X'X)^{-1}\\). But when \\(\\text{Var}[e \\mid X] = \\Omega \\neq \\sigma^2 I\\), we get the sandwich formula:\n\\[(X'X)^{-1} (X' \\Omega X) (X'X)^{-1} \\tag{1}\\]\nThe “bread” is \\((X'X)^{-1}\\) and the “meat” is \\(X'\\Omega X = \\sum_{i=1}^n \\sigma_i^2 x_i x_i'\\). Let’s see what happens when we ignore heteroskedasticity and use the classical formula anyway.\n\nDefinition 1 (Sandwich Variance Estimator) Under heteroskedasticity (\\(\\text{Var}[e|X] = \\Omega \\neq \\sigma^2 I\\)), the variance of OLS is \\((X'X)^{-1}(X'\\Omega X)(X'X)^{-1}\\). The HC estimators replace \\(\\Omega\\) with diagonal matrices of squared residuals, possibly adjusted for leverage.\n\n\n1.1 Simulation: when classical standard errors lie\nWe design a DGP with strong heteroskedasticity: the error standard deviation grows as \\(x^2\\), so variance ranges from 1 (at \\(x = 1\\)) to 10,000 (at \\(x = 10\\)). This makes the problem impossible to miss.\n\nset.seed(42)\nn &lt;- 200\nx &lt;- runif(n, 1, 10)\nX &lt;- cbind(1, x)\n\n# Strongly heteroskedastic DGP: SD = x^2\nsigma_i &lt;- x^2   # variance = x^4, ratio of 10000:1\ny &lt;- 2 + 3 * x + rnorm(n, 0, sigma_i)\nmod &lt;- lm(y ~ x)\n\nFirst, let’s build the sandwich by hand to see the matrix algebra:\n\n# Classical variance: s^2 * (X'X)^{-1}\ns2 &lt;- sum(resid(mod)^2) / (n - 2)\nV_classical &lt;- s2 * solve(crossprod(X))\n\n# Sandwich variance (HC0): (X'X)^{-1} X' diag(e^2) X (X'X)^{-1}\ne_hat &lt;- resid(mod)\nbread &lt;- solve(crossprod(X))\nmeat &lt;- t(X) %*% diag(e_hat^2) %*% X\nV_HC0 &lt;- bread %*% meat %*% bread\n\n# Compare standard errors for the slope\nc(classical = sqrt(V_classical[2, 2]),\n  HC0       = sqrt(V_HC0[2, 2]))\n\nclassical       HC0 \n     1.31      1.48 \n\n\nNow the practical way — sandwich::vcovHC() computes this in one line:\n\n# HC0 (White's original)\nsqrt(diag(vcovHC(mod, type = \"HC0\")))\n\n(Intercept)           x \n       5.48        1.48 \n\n# HC1 (small-sample correction: multiply by n/(n-k))\nsqrt(diag(vcovHC(mod, type = \"HC1\")))\n\n(Intercept)           x \n       5.51        1.49 \n\n# HC2 (recommended default — adjusts for leverage)\nsqrt(diag(vcovHC(mod, type = \"HC2\")))\n\n(Intercept)           x \n       5.52        1.49 \n\n\nOr even simpler — estimatr::lm_robust() fits the model and computes robust SEs in one step:\n\nmod_robust &lt;- lm_robust(y ~ x, se_type = \"HC2\")\nsummary(mod_robust)\n\n\nCall:\nlm_robust(formula = y ~ x, se_type = \"HC2\")\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper  DF\n(Intercept)     4.07       5.52   0.737    0.462   -6.816    14.95 198\nx               2.30       1.49   1.540    0.125   -0.644     5.24 198\n\nMultiple R-squared:  0.0154 ,   Adjusted R-squared:  0.0104 \nF-statistic: 2.37 on 1 and 198 DF,  p-value: 0.125\n\n\nCompare the standard errors side by side:\n\nse_table &lt;- data.frame(\n  Classical = summary(mod)$coefficients[, 2],\n  HC0 = sqrt(diag(vcovHC(mod, type = \"HC0\"))),\n  HC1 = sqrt(diag(vcovHC(mod, type = \"HC1\"))),\n  HC2 = sqrt(diag(vcovHC(mod, type = \"HC2\"))),\n  row.names = c(\"(Intercept)\", \"x\")\n)\nround(se_table, 3)\n\n            Classical  HC0  HC1  HC2\n(Intercept)      8.20 5.48 5.51 5.52\nx                1.31 1.48 1.49 1.49\n\n\nThe classical SE for the slope is far too small — it ignores that the high-\\(x\\) observations (which pull the slope) are exactly the noisiest ones.\n\n\n\n\n\n\nWarningClassical Standard Errors Can Be Dangerously Wrong\n\n\n\nUnder heteroskedasticity, classical SEs can be too small by a factor of 2 or more, producing confidence intervals with far below nominal coverage. Always use robust SEs (HC2) as the default for cross-sectional data.\n\n\n\ndf &lt;- data.frame(x = x, residual = resid(mod))\nggplot(df, aes(x, residual)) +\n  geom_point(alpha = 0.4) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Residuals fan out dramatically with x\",\n       subtitle = \"SD grows as x², so variance ratio is 10000:1\")\n\n\n\n\n\n\n\n\n\n\n1.2 Monte Carlo: coverage of classical vs. robust intervals\n\nset.seed(1)\nB &lt;- 2000\ncover_classical &lt;- cover_HC0 &lt;- cover_HC2 &lt;- logical(B)\nbeta_true &lt;- 3\n\nfor (b in 1:B) {\n  x_sim &lt;- runif(n, 1, 10)\n  X_sim &lt;- cbind(1, x_sim)\n  sigma_sim &lt;- x_sim^2\n  y_sim &lt;- 2 + beta_true * x_sim + rnorm(n, 0, sigma_sim)\n\n  fit &lt;- lm(y_sim ~ x_sim)\n  b_hat &lt;- coef(fit)[2]\n\n  # Classical CI\n  se_class &lt;- summary(fit)$coefficients[2, 2]\n  ci_class &lt;- b_hat + c(-1, 1) * 1.96 * se_class\n  cover_classical[b] &lt;- ci_class[1] &lt; beta_true & beta_true &lt; ci_class[2]\n\n  # HC0 (White)\n  se_hc0 &lt;- sqrt(vcovHC(fit, type = \"HC0\")[2, 2])\n  ci_hc0 &lt;- b_hat + c(-1, 1) * 1.96 * se_hc0\n  cover_HC0[b] &lt;- ci_hc0[1] &lt; beta_true & beta_true &lt; ci_hc0[2]\n\n  # HC2 (recommended)\n  se_hc2 &lt;- sqrt(vcovHC(fit, type = \"HC2\")[2, 2])\n  ci_hc2 &lt;- b_hat + c(-1, 1) * 1.96 * se_hc2\n  cover_HC2[b] &lt;- ci_hc2[1] &lt; beta_true & beta_true &lt; ci_hc2[2]\n}\n\nc(classical = mean(cover_classical),\n  HC0       = mean(cover_HC0),\n  HC2       = mean(cover_HC2))\n\nclassical       HC0       HC2 \n    0.894     0.953     0.954 \n\n\nClassical intervals have terrible coverage. HC0 does better. HC2 gets closest to the nominal 95% because it corrects for leverage — observations with high \\(x\\) values both have high variance and high leverage. (Chapter 6 develops the HC variants in detail.)\n\n\n1.3 lm_robust vs. coeftest: two workflows\nIn practice, there are two ways to get robust inference. Use whichever fits your workflow:\n\n# Workflow 1: estimatr — one function does everything\nmod_r &lt;- lm_robust(y ~ x, se_type = \"HC2\")\ncoef(summary(mod_r))\n\n            Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper  DF\n(Intercept)     4.07       5.52   0.737    0.462   -6.816    14.95 198\nx               2.30       1.49   1.540    0.125   -0.644     5.24 198\n\n# Workflow 2: sandwich + lmtest — post-hoc correction to a fitted lm\ncoeftest(mod, vcov = vcovHC(mod, type = \"HC2\"))\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)     4.07       5.52    0.74     0.46\nx               2.30       1.49    1.54     0.13\n\n\nThe coeftest() approach is useful when you’ve already fit a model with lm() and want to report robust SEs. lm_robust() is cleaner when you know from the start that you want robust inference."
  },
  {
    "objectID": "ch05-gls.html#weighted-least-squares",
    "href": "ch05-gls.html#weighted-least-squares",
    "title": "5. Efficiency and GLS",
    "section": "2 Weighted least squares",
    "text": "2 Weighted least squares\nThe sandwich formula tells us what the variance is. But can we do better than OLS? Yes — if we know (or can estimate) the variance structure, we should exploit it.\n\n2.1 The idea: weight by precision\nIf observation \\(i\\) has variance \\(\\sigma_i^2\\), it carries less information than an observation with variance \\(\\sigma_j^2 &lt; \\sigma_i^2\\). WLS weights each observation by \\(w_i = 1/\\sigma_i^2\\), downweighting noisy observations:\n\\[\\hat{\\beta}_{WLS} = \\arg\\min_\\beta \\sum_{i=1}^n w_i (y_i - x_i'\\beta)^2 = (X'WX)^{-1} X'Wy \\tag{2}\\]\nwhere \\(W = \\text{diag}(w_1, \\ldots, w_n)\\).\n\n\n2.2 Two-group example\nSuppose we survey two groups: Group A (\\(n_A = 100\\), \\(\\sigma_A = 10\\)) and Group B (\\(n_B = 100\\), \\(\\sigma_B = 1\\)). OLS gives equal weight to every observation. WLS gives Group A weight \\(1/100\\) and Group B weight \\(1\\).\n\nset.seed(99)\nB &lt;- 5000\nb_ols &lt;- b_wls &lt;- numeric(B)\n\nfor (i in 1:B) {\n  x_sim &lt;- rnorm(200)\n  sigma_sim &lt;- c(rep(10, 100), rep(1, 100))\n  y_sim &lt;- 1 + 2 * x_sim + rnorm(200, 0, sigma_sim)\n\n  b_ols[i] &lt;- coef(lm(y_sim ~ x_sim))[2]\n  b_wls[i] &lt;- coef(lm(y_sim ~ x_sim, weights = 1 / sigma_sim^2))[2]\n}\n\n# Both unbiased, but WLS has much lower variance\nc(bias_ols = mean(b_ols) - 2, bias_wls = mean(b_wls) - 2)\n\nbias_ols bias_wls \n-0.00434 -0.00125 \n\nc(sd_ols = sd(b_ols), sd_wls = sd(b_wls))\n\nsd_ols sd_wls \n 0.507  0.103 \n\n\nBoth estimators are unbiased, but WLS standard errors are dramatically smaller. GLS is just common sense: trust precise observations more.\n\ndf_sim &lt;- data.frame(\n  estimate = c(b_ols, b_wls),\n  method = rep(c(\"OLS\", \"WLS\"), each = B)\n)\nggplot(df_sim, aes(estimate, fill = method)) +\n  geom_density(alpha = 0.4) +\n  geom_vline(xintercept = 2, linetype = \"dashed\") +\n  labs(title = \"OLS vs. WLS sampling distributions\",\n       subtitle = \"Both centered on truth, but WLS is much tighter\",\n       x = expression(hat(beta)))\n\n\n\n\nOLS vs. WLS sampling distributions: both unbiased, but WLS is tighter\n\n\n\n\n\n\n2.3 WLS in R: lm(..., weights = )\n\nmod_ols &lt;- lm(prestige ~ education + income + women, data = Prestige)\n\n# Suppose we know variance is proportional to income\n# (higher-income occupations have more variable prestige)\nw &lt;- 1 / Prestige$income\nmod_wls &lt;- lm(prestige ~ education + income + women, data = Prestige, weights = w)\n\n# Compare coefficients\ncbind(OLS = coef(mod_ols), WLS = coef(mod_wls))\n\n                 OLS      WLS\n(Intercept) -6.79433 -9.97061\neducation    4.18664  3.18162\nincome       0.00131  0.00303\nwomen       -0.00891  0.07049\n\n\n\n\n2.4 WLS as a transformed regression\nWLS is equivalent to pre-multiplying the model by \\(W^{1/2}\\):\n\\[W^{1/2} y = W^{1/2} X \\beta + W^{1/2} e\\]\nThe transformed errors have variance \\(W^{1/2} \\Omega W^{1/2} = I\\) (if \\(W = \\Omega^{-1}\\)), so OLS on the transformed data is efficient. Let’s verify:\n\n# Manual transformation\nW_half &lt;- diag(sqrt(w))\ny_tilde &lt;- W_half %*% Prestige$prestige\nX_raw &lt;- cbind(1, Prestige$education, Prestige$income, Prestige$women)\nX_tilde &lt;- W_half %*% X_raw\n\n# OLS on transformed data\nbeta_transformed &lt;- solve(crossprod(X_tilde)) %*% crossprod(X_tilde, y_tilde)\n\n# Compare to lm(..., weights = )\nall.equal(as.numeric(beta_transformed), as.numeric(coef(mod_wls)))\n\n[1] TRUE\n\n\nThe transformation approach makes clear what weights does: it rescales each observation so that the transformed errors are homoskedastic.\nImportant note on the intercept. After transformation, the column of ones becomes \\(W^{1/2} \\mathbf{1} = (\\sqrt{w_1}, \\ldots, \\sqrt{w_n})'\\), which is no longer constant. If you run the transformed regression manually, you must suppress the automatic intercept and include the transformed constant as a regressor:\n\n# Transformed data\ndf_t &lt;- data.frame(\n  y = as.numeric(y_tilde),\n  const = as.numeric(W_half %*% rep(1, nrow(Prestige))),\n  education = as.numeric(W_half %*% Prestige$education),\n  income = as.numeric(W_half %*% Prestige$income),\n  women = as.numeric(W_half %*% Prestige$women)\n)\n\n# -1 suppresses R's intercept; 'const' is the transformed intercept\nmod_manual &lt;- lm(y ~ const + education + income + women - 1, data = df_t)\nall.equal(as.numeric(coef(mod_manual)), as.numeric(coef(mod_wls)))\n\n[1] TRUE"
  },
  {
    "objectID": "ch05-gls.html#sec-gls",
    "href": "ch05-gls.html#sec-gls",
    "title": "5. Efficiency and GLS",
    "section": "3 GLS: The general transformation",
    "text": "3 GLS: The general transformation\nWLS handles the case where \\(\\Omega\\) is diagonal (heteroskedasticity only). GLS handles the general case where errors may also be correlated. The key idea is the same: find a transformation \\(\\Omega^{-1/2}\\) that sphericizes the errors.\n\n3.1 Eigendecomposition of \\(\\Omega\\)\nAny positive definite symmetric matrix can be factored as \\(\\Omega = C \\Lambda C'\\), where \\(C\\) is the matrix of eigenvectors and \\(\\Lambda\\) is diagonal with eigenvalues. Then:\n\\[\\Omega^{-1/2} = C \\Lambda^{-1/2} C'\\]\n\n# A small example: 4x4 covariance matrix with correlation\nn_small &lt;- 4\nrho &lt;- 0.6\nOmega_small &lt;- rho^abs(outer(1:n_small, 1:n_small, \"-\"))  # AR(1) structure\nOmega_small\n\n      [,1] [,2] [,3]  [,4]\n[1,] 1.000 0.60 0.36 0.216\n[2,] 0.600 1.00 0.60 0.360\n[3,] 0.360 0.60 1.00 0.600\n[4,] 0.216 0.36 0.60 1.000\n\n\n\neig &lt;- eigen(Omega_small)\nC &lt;- eig$vectors\nLambda &lt;- diag(eig$values)\n\n# Omega^{-1/2}\nOmega_inv_half &lt;- C %*% diag(1 / sqrt(eig$values)) %*% t(C)\n\n# Verify: Omega^{-1/2} Omega Omega^{-1/2} = I\nall.equal(Omega_inv_half %*% Omega_small %*% Omega_inv_half, diag(n_small),\n          check.attributes = FALSE)\n\n[1] TRUE\n\n\n\n\n3.2 GLS formula\nThe GLS estimator pre-multiplies by \\(\\Omega^{-1/2}\\), then applies OLS:\n\\[\\hat{\\beta}_{GLS} = (X'\\Omega^{-1}X)^{-1} X'\\Omega^{-1}y \\tag{3}\\]\nIts variance is \\(\\sigma^2(X'\\Omega^{-1}X)^{-1}\\), which is the efficiency lower bound — no other linear unbiased estimator can do better.\n\nTheorem 1 (GLS Estimator) The GLS estimator \\(\\hat\\beta_{GLS} = (X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}y\\) is the best linear unbiased estimator (BLUE) when \\(\\text{Var}[e|X] = \\sigma^2\\Omega\\). Its variance \\(\\sigma^2(X'\\Omega^{-1}X)^{-1}\\) achieves the efficiency lower bound among linear unbiased estimators.\n\n\nTheorem 2 (Gauss-Markov Theorem) Under the assumptions \\(\\mathbb{E}[e|X] = 0\\) and \\(\\text{Var}[e|X] = \\sigma^2 I\\) (homoskedasticity), OLS is BLUE — the Best Linear Unbiased Estimator. No other linear unbiased estimator has smaller variance. When homoskedasticity fails, GLS replaces OLS as BLUE.\n\n\n\n3.3 Simulation: GLS with correlated errors\n\nset.seed(7)\nn &lt;- 80\nrho &lt;- 0.8\n\n# AR(1) correlation matrix\nOmega &lt;- rho^abs(outer(1:n, 1:n, \"-\"))\nOmega_inv &lt;- solve(Omega)\n\n# Cholesky factor for generating correlated errors\nL &lt;- t(chol(Omega))\n\nx &lt;- sort(runif(n, 0, 10))\nX &lt;- cbind(1, x)\nbeta_true &lt;- c(1, 2)\n\nB &lt;- 3000\nb_ols &lt;- b_gls &lt;- matrix(NA, B, 2)\n\nfor (b in 1:B) {\n  e &lt;- L %*% rnorm(n)  # correlated errors\n  y_sim &lt;- X %*% beta_true + e\n\n  b_ols[b, ] &lt;- as.numeric(solve(crossprod(X)) %*% crossprod(X, y_sim))\n  b_gls[b, ] &lt;- as.numeric(solve(t(X) %*% Omega_inv %*% X) %*%\n                              (t(X) %*% Omega_inv %*% y_sim))\n}\n\n# Both unbiased\ncolMeans(b_ols) - beta_true\n\n[1]  0.01416 -0.00327\n\ncolMeans(b_gls) - beta_true\n\n[1]  0.01215 -0.00282\n\n# But GLS is more efficient (lower SD for slope)\nc(sd_ols_slope = sd(b_ols[, 2]), sd_gls_slope = sd(b_gls[, 2]))\n\nsd_ols_slope sd_gls_slope \n       0.109        0.101 \n\n\n\ndf_ar &lt;- data.frame(\n  slope = c(b_ols[, 2], b_gls[, 2]),\n  method = rep(c(\"OLS\", \"GLS\"), each = B)\n)\nggplot(df_ar, aes(slope, fill = method)) +\n  geom_density(alpha = 0.4) +\n  geom_vline(xintercept = 2, linetype = \"dashed\") +\n  labs(title = \"OLS vs. GLS with AR(1) errors (ρ = 0.8)\",\n       subtitle = \"GLS recovers the efficiency lost to serial correlation\",\n       x = expression(hat(beta)[1]))\n\n\n\n\n\n\n\n\n\n\n3.4 The GLS projection matrix\nIn Chapter 3, we studied \\(P = X(X'X)^{-1}X'\\). The GLS analog replaces the inner product:\n\\[P_{GLS} = X(X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}\\]\nUnlike the OLS projection, \\(P_{GLS}\\) is not symmetric — but it is still idempotent:\n\nP_gls &lt;- X %*% solve(t(X) %*% Omega_inv %*% X) %*% t(X) %*% Omega_inv\n\n# Not symmetric\nmax(abs(P_gls - t(P_gls)))\n\n[1] 0.272\n\n# But idempotent\nall.equal(P_gls %*% P_gls, P_gls)\n\n[1] TRUE"
  },
  {
    "objectID": "ch05-gls.html#feasible-gls",
    "href": "ch05-gls.html#feasible-gls",
    "title": "5. Efficiency and GLS",
    "section": "4 Feasible GLS",
    "text": "4 Feasible GLS\nGLS requires knowing \\(\\Omega\\). In practice, we never know the true variance structure. Feasible GLS (FGLS) estimates \\(\\Omega\\) from the data in a first step, then applies GLS with \\(\\hat{\\Omega}\\).\n\n4.1 Step-by-step FGLS for heteroskedasticity\nThe most common approach assumes a multiplicative heteroskedasticity model:\n\\[\\sigma_i^2 = \\text{Var}[e_i \\mid x_i] = \\exp(\\gamma_0 + \\gamma_1 z_i)\\]\nwhere \\(z_i\\) is some function of \\(x_i\\).\n\nset.seed(42)\nn &lt;- 300\n\n# DGP: variance depends strongly on x\nx &lt;- runif(n, 1, 10)\nsigma_true &lt;- exp(0.5 + 0.3 * x)  # log-linear variance\ny &lt;- 2 + 3 * x + rnorm(n) * sqrt(sigma_true)\n\n# Step 1: Run OLS, save residuals\nmod_step1 &lt;- lm(y ~ x)\n\n# Step 2: Regress log(e^2) on x to estimate the variance function\nlog_e2 &lt;- log(resid(mod_step1)^2)\nmod_var &lt;- lm(log_e2 ~ x)\n\n# Step 3: Predicted variances -&gt; weights\nsigma2_hat &lt;- exp(fitted(mod_var))\nw_fgls &lt;- 1 / sigma2_hat\n\n# Step 4: Run WLS with estimated weights\nmod_fgls &lt;- lm(y ~ x, weights = w_fgls)\n\ncbind(OLS = coef(mod_step1), FGLS = coef(mod_fgls), Truth = c(2, 3))\n\n             OLS FGLS Truth\n(Intercept) 1.85 2.03     2\nx           3.02 2.98     3\n\n\nCompare standard errors — OLS classical, OLS with HC2, and FGLS:\n\nX_fgls &lt;- cbind(1, x)\n\nse_compare &lt;- data.frame(\n  OLS_classical = summary(mod_step1)$coefficients[, 2],\n  OLS_HC2       = sqrt(diag(vcovHC(mod_step1, type = \"HC2\"))),\n  FGLS          = summary(mod_fgls)$coefficients[, 2],\n  row.names = c(\"(Intercept)\", \"x\")\n)\nround(se_compare, 4)\n\n            OLS_classical OLS_HC2  FGLS\n(Intercept)        0.4559  0.3641 0.255\nx                  0.0753  0.0886 0.062\n\n\nHC2 corrects the SE without changing the point estimate. FGLS changes both — it re-estimates \\(\\hat{\\beta}\\) using the variance information, gaining efficiency.\n\n\n4.2 Implementing FGLS with matrix algebra\n\nX_mat &lt;- cbind(1, x)\nOmega_hat_inv &lt;- diag(1 / sigma2_hat)\n\n# GLS formula with estimated Omega\nbeta_fgls &lt;- solve(t(X_mat) %*% Omega_hat_inv %*% X_mat) %*%\n             (t(X_mat) %*% Omega_hat_inv %*% y)\n\nall.equal(as.numeric(beta_fgls), as.numeric(coef(mod_fgls)))\n\n[1] TRUE\n\n\n\n\n4.3 How well does FGLS estimate the variance function?\n\ndf_var &lt;- data.frame(\n  x = x,\n  log_e2 = log_e2,\n  fitted_log_var = fitted(mod_var),\n  true_log_var = 0.5 + 0.3 * x\n)\n\nggplot(df_var, aes(x)) +\n  geom_point(aes(y = log_e2), alpha = 0.2, size = 1) +\n  geom_line(aes(y = fitted_log_var, color = \"Estimated\"), linewidth = 1) +\n  geom_line(aes(y = true_log_var, color = \"True\"), linewidth = 1, linetype = \"dashed\") +\n  labs(title = \"FGLS variance function estimation\",\n       subtitle = \"Regressing log(ê²) on x recovers the variance structure\",\n       y = \"log(σ²)\", color = NULL)\n\n\n\n\n\n\n\n\n\n\n4.4 Monte Carlo: OLS vs. FGLS efficiency\n\nset.seed(2)\nB &lt;- 2000\nb_ols_mc &lt;- b_fgls_mc &lt;- numeric(B)\n\nfor (b in 1:B) {\n  x_mc &lt;- runif(n, 1, 10)\n  sigma_mc &lt;- exp(0.5 + 0.3 * x_mc)\n  y_mc &lt;- 2 + 3 * x_mc + rnorm(n) * sqrt(sigma_mc)\n\n  # OLS\n  fit_ols &lt;- lm(y_mc ~ x_mc)\n  b_ols_mc[b] &lt;- coef(fit_ols)[2]\n\n  # FGLS\n  log_e2_mc &lt;- log(resid(fit_ols)^2)\n  sigma2_hat_mc &lt;- exp(fitted(lm(log_e2_mc ~ x_mc)))\n  b_fgls_mc[b] &lt;- coef(lm(y_mc ~ x_mc, weights = 1 / sigma2_hat_mc))[2]\n}\n\nc(sd_ols = sd(b_ols_mc), sd_fgls = sd(b_fgls_mc),\n  efficiency_gain = sd(b_ols_mc) / sd(b_fgls_mc))\n\n         sd_ols         sd_fgls efficiency_gain \n         0.0841          0.0687          1.2255 \n\n\nEven though FGLS must estimate the weights, it still substantially improves on OLS when the heteroskedasticity is strong.\n\n\n\n\n\n\nNoteFGLS Requires a Correct Variance Model\n\n\n\nFGLS gains efficiency over OLS + robust SEs only when the variance model is correctly specified. If \\(\\hat\\Omega\\) is misspecified, FGLS point estimates are still consistent but may be less efficient than OLS, and its reported SEs may be wrong. Use FGLS only when you have a substantive reason to model the variance."
  },
  {
    "objectID": "ch05-gls.html#two-strategies-for-heteroskedasticity",
    "href": "ch05-gls.html#two-strategies-for-heteroskedasticity",
    "title": "5. Efficiency and GLS",
    "section": "5 Two strategies for heteroskedasticity",
    "text": "5 Two strategies for heteroskedasticity\nYou have two options when errors may be heteroskedastic:\n\nRobust SEs (agnostic). Keep the OLS point estimates and correct only the standard errors with the sandwich formula. Requires no model for the variance — always valid.\nWLS/FGLS (model-based). Specify a model for the variance, estimate it, and re-weight. More efficient if the variance model is correct; potentially worse if it’s misspecified.\n\nA common older workflow was to first test for heteroskedasticity (Breusch-Pagan, White’s test), then decide whether to apply WLS. This is pre-testing — using the same data to choose the estimator and then to estimate — and it distorts the sampling distribution of the final estimate. The resulting “test, then decide” procedure is neither the OLS distribution nor the WLS distribution; its true coverage and size are hard to characterize.\nThe modern recommendation: always report robust SEs (HC2 by default for cross-sectional data). Use WLS/FGLS only when you have a substantive reason to model the variance — for instance, when observations are group averages with known group sizes, or when a theoretical model predicts the variance form. The decision to use WLS should come from domain knowledge, not from a hypothesis test on the same data.\n\n5.1 The Breusch-Pagan test (for understanding, not for pre-testing)\nThe Breusch-Pagan test is still useful as a descriptive diagnostic — it tells you whether your residuals exhibit systematic patterns in spread. The mechanics are simple: regress \\(\\hat{e}^2 / \\bar{\\hat{e}^2}\\) on \\(X\\) and check whether the \\(R^2\\) is significantly different from zero.\n\nmod_diag &lt;- lm(y ~ x)\n\n# Breusch-Pagan by hand\ne2 &lt;- resid(mod_diag)^2\np &lt;- e2 / mean(e2)  # normalize by average squared residual\naux &lt;- lm(p ~ x)\n\n# Test statistic: explained sum of squares / 2\nbp_stat &lt;- sum((fitted(aux) - mean(p))^2) / 2\nbp_pval &lt;- 1 - pchisq(bp_stat, df = 1)\nc(BP_statistic = bp_stat, p_value = bp_pval)\n\nBP_statistic      p_value \n         111            0 \n\n\n\n# Same thing via lmtest (studentized version is robust to non-normal errors)\nbptest(mod_diag)\n\n\n    studentized Breusch-Pagan test\n\ndata:  mod_diag\nBP = 66, df = 1, p-value = 4e-16\n\n\nA large test statistic tells you heteroskedasticity is present, which is useful information for understanding your data. But the right response is to always use robust SEs — not to condition your estimator on the test result."
  },
  {
    "objectID": "ch05-gls.html#residual-types",
    "href": "ch05-gls.html#residual-types",
    "title": "5. Efficiency and GLS",
    "section": "6 Residual types",
    "text": "6 Residual types\nOLS residuals are \\(\\hat{e} = My\\), but they are not equal to the true errors. The lecture develops three residual types using projection matrices. Each applies a different diagonal scaling matrix \\(M^*\\) to correct for leverage.\n\n6.1 The matrices\n\nX_d &lt;- cbind(1, x)\nK &lt;- ncol(X_d)\nP_d &lt;- X_d %*% solve(crossprod(X_d)) %*% t(X_d)\nM_d &lt;- diag(n) - P_d\nh &lt;- diag(P_d)  # leverage values\n\n# M* = diag{(1 - h_ii)^{-1}} — inflates by leverage\nM_star &lt;- diag(1 / (1 - h))\n\n# (M*)^{1/2} = diag{(1 - h_ii)^{-1/2}} — square root scaling\nM_star_half &lt;- diag(1 / sqrt(1 - h))\n\nThe key relationship: \\(\\text{Var}[\\hat{e} \\mid X] = M \\, \\text{Var}[e \\mid X] \\, M\\). Under homoskedasticity this gives \\(\\text{Var}[\\hat{e}_i \\mid X] = (1 - h_{ii})\\sigma^2\\), so residuals are heteroskedastic even when the errors are not. The diagonal of \\(M\\) shows the uneven scaling:\n\nsummary(1 - h)  # ranges from near 0 (high leverage) to near 1\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.987   0.991   0.994   0.993   0.996   0.997 \n\n\n\n\n6.2 Raw residuals: \\(\\hat{e} = Me\\)\n\ne_raw &lt;- as.numeric(M_d %*% y)\n\n# Same as resid()\nall.equal(e_raw, as.numeric(resid(mod_diag)))\n\n[1] TRUE\n\n\n\n\n6.3 Prediction errors: \\(\\tilde{e} = M^* \\hat{e} = M^* M e\\)\nThe prediction error \\(\\tilde{e}_i = \\hat{e}_i / (1 - h_{ii})\\) is the leave-one-out residual from Chapter 4. In matrix form, pre-multiplying by \\(M^*\\) inflates each residual by the inverse of its leverage correction:\n\ne_pred &lt;- as.numeric(M_star %*% M_d %*% y)\n\n# Equivalently: e_hat / (1 - h)\nall.equal(e_pred, e_raw / (1 - h))\n\n[1] TRUE\n\n\nUnder homoskedasticity, \\(\\text{Var}[\\tilde{e}_i \\mid X] = (1 - h_{ii})^{-1}\\sigma^2\\). These inflate at high-leverage points — the opposite of raw residuals.\n\n\n6.4 Standardized residuals: \\(\\bar{e} = (M^*)^{1/2} \\hat{e} / \\hat{\\sigma}\\)\nThe standardized residual applies the square-root scaling to make residuals have (approximately) unit variance under homoskedasticity:\n\\[\\bar{e} = \\frac{1}{\\hat{\\sigma}}(M^*)^{1/2} M y\\]\n\nsigma_hat &lt;- sqrt(sum(e_raw^2) / (n - K))\n\ne_std &lt;- as.numeric(M_star_half %*% M_d %*% y) / sigma_hat\n\n# Same as rstandard()\nall.equal(e_std, as.numeric(rstandard(mod_diag)))\n\n[1] TRUE\n\n\n\n\n6.5 Comparing the three types\nThe three residual types tell different stories. Raw residuals show the fan pattern of heteroskedasticity. Prediction errors amplify it — high-leverage observations get inflated further. Standardized residuals divide out the leverage effect, putting everything on a common scale.\n\ndf_resid &lt;- data.frame(\n  x = rep(x, 3),\n  residual = c(e_raw, e_pred, e_std),\n  type = factor(rep(c(\"Raw: Me\", \"Prediction: M*Me\", \"Standardized: (M*)^½Me / σ̂\"),\n                    each = n),\n                levels = c(\"Raw: Me\", \"Prediction: M*Me\", \"Standardized: (M*)^½Me / σ̂\"))\n)\n\nggplot(df_resid, aes(x, residual)) +\n  geom_point(alpha = 0.3, size = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  facet_wrap(~ type, scales = \"free_y\") +\n  labs(title = \"Three residual types from the same regression\",\n       subtitle = \"Each panel applies a different diagonal scaling matrix to ê = My\",\n       y = \"Residual value\")\n\n\n\n\n\n\n\n\nThe fan shape is visible in all three (because the true DGP is heteroskedastic), but the scale differs: raw residuals have variance \\((1 - h_{ii})\\sigma_i^2\\), prediction errors have variance \\((1 - h_{ii})^{-1}\\sigma_i^2\\), and standardized residuals remove the leverage component, leaving only the heteroskedasticity \\(\\sigma_i^2 / \\sigma^2\\).\nStudentized residuals. R also provides rstudent(), which replaces \\(\\hat{\\sigma}\\) with the leave-one-out estimate \\(s_{(-i)}\\). In practice, studentized and standardized residuals are nearly identical for moderate \\(n\\) — the difference is just one observation’s contribution to \\(\\hat{\\sigma}^2\\). The studentized version follows a \\(t_{n-K-1}\\) distribution under normality, which is useful for formal outlier tests:\n\n# Studentized ≈ standardized, but uses leave-one-out sigma\nmax(abs(rstudent(mod_diag) - rstandard(mod_diag)))\n\n[1] 0.059"
  },
  {
    "objectID": "ch05-gls.html#estimating-sigma2",
    "href": "ch05-gls.html#estimating-sigma2",
    "title": "5. Efficiency and GLS",
    "section": "7 Estimating \\(\\sigma^2\\)",
    "text": "7 Estimating \\(\\sigma^2\\)\nThree estimators of the error variance:\n\nK &lt;- ncol(X_d)\n\n# 1. Method of moments (biased)\nsigma2_mm &lt;- sum(resid(mod_diag)^2) / n\n\n# 2. Bias-corrected (used by summary.lm)\nsigma2_s2 &lt;- sum(resid(mod_diag)^2) / (n - K)\n\n# 3. Standardized estimator (unbiased under heteroskedasticity)\nsigma2_bar &lt;- mean(resid(mod_diag)^2 / (1 - h))\n\nc(MM = sigma2_mm, s2 = sigma2_s2, standardized = sigma2_bar)\n\n          MM           s2 standardized \n        11.6         11.7         11.7 \n\n\nUnder homoskedasticity, \\(s^2\\) is unbiased: \\(E[s^2 \\mid X] = \\sigma^2\\). The key identity is \\(E[\\hat{e}'\\hat{e} \\mid X] = \\text{tr}(M) \\cdot \\sigma^2 = (n - K)\\sigma^2\\).\n\n# The trace trick: E[e'Me] = tr(M * E[ee']) = tr(M) * sigma^2 under homoskedasticity\nc(trace_M = tr(M_d), n_minus_K = n - K)\n\n  trace_M n_minus_K \n      298       298 \n\n\nThe standardized estimator \\(\\bar{\\sigma}^2 = \\frac{1}{n}\\sum (1-h_{ii})^{-1}\\hat{e}_i^2\\) is unbiased even under heteroskedasticity — it corrects each squared residual for its leverage. This is the logic behind HC2 standard errors: replace \\(\\hat{e}_i^2\\) with \\(\\hat{e}_i^2 / (1 - h_{ii})\\) in the sandwich meat."
  },
  {
    "objectID": "ch05-gls.html#method-of-moments-perspective",
    "href": "ch05-gls.html#method-of-moments-perspective",
    "title": "5. Efficiency and GLS",
    "section": "8 Method of moments perspective",
    "text": "8 Method of moments perspective\n\n8.1 OLS as a method of moments estimator\nOLS solves the sample analog of \\(E[x_i(y_i - x_i'\\beta)] = 0\\):\n\\[\\frac{1}{n} \\sum_{i=1}^n x_i(y_i - x_i'\\hat{\\beta}) = 0 \\quad \\Longleftrightarrow \\quad X'(y - X\\hat{\\beta}) = 0\\]\n\n# The OLS normal equations are moment conditions\nmoment &lt;- t(X_d) %*% resid(mod_diag)\nmoment  # numerically zero\n\n      [,1]\n  2.78e-15\nx 5.62e-13\n\n\n\n\n8.2 GLS as efficient method of moments\nGLS solves a weighted version of the same moment condition:\n\\[\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{\\sigma_i^2} x_i(y_i - x_i'\\hat{\\beta}_{GLS}) = 0 \\quad \\Longleftrightarrow \\quad X'\\Omega^{-1}(y - X\\hat{\\beta}_{GLS}) = 0\\]\nThis is a generalized method of moments (GMM) estimator with weight matrix \\(\\Omega^{-1}\\).\n\n# GLS normal equations\nOmega_hat_inv_diag &lt;- diag(1 / sigma2_hat)  # from our FGLS above\nmoment_gls &lt;- t(X_d) %*% Omega_hat_inv_diag %*% resid(mod_fgls)\nmoment_gls  # numerically zero\n\n       [,1]\n   1.11e-14\nx -2.22e-14\n\n\nWhen we have exactly as many moment conditions as parameters (\\(K\\) equations, \\(K\\) unknowns), the GMM estimator reduces to method of moments. The efficiency of GLS comes from choosing the optimal weight matrix.\nIn Chapter 14, we’ll see that this logic extends to overidentified models: when you have more moment conditions than parameters, GMM finds the optimal combination. The GLS insight — weight by precision — is the same insight that drives GMM."
  },
  {
    "objectID": "ch05-gls.html#application-robust-inference-on-the-prestige-data",
    "href": "ch05-gls.html#application-robust-inference-on-the-prestige-data",
    "title": "5. Efficiency and GLS",
    "section": "9 Application: robust inference on the Prestige data",
    "text": "9 Application: robust inference on the Prestige data\nLet’s apply the practical workflow to the Prestige dataset. We start with lm_robust() as the default — no pre-testing required.\n\n# The default workflow: OLS with HC2 robust SEs\nmod_p_robust &lt;- lm_robust(prestige ~ education + income + women,\n                           data = Prestige, se_type = \"HC2\")\nsummary(mod_p_robust)\n\n\nCall:\nlm_robust(formula = prestige ~ education + income + women, data = Prestige, \n    se_type = \"HC2\")\n\nStandard error type:  HC2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  CI Lower CI Upper DF\n(Intercept) -6.79433   3.216715  -2.112 3.72e-02 -13.17780 -0.41087 98\neducation    4.18664   0.446857   9.369 2.83e-15   3.29986  5.07341 98\nincome       0.00131   0.000375   3.504 6.91e-04   0.00057  0.00206 98\nwomen       -0.00891   0.035522  -0.251 8.03e-01  -0.07940  0.06159 98\n\nMultiple R-squared:  0.798 ,    Adjusted R-squared:  0.792 \nF-statistic:  137 on 3 and 98 DF,  p-value: &lt;2e-16\n\n\nThat’s the complete inference in one line. The residual plot is still useful as a descriptive tool for understanding your data — it just shouldn’t gate your choice of estimator:\n\nmod_p &lt;- lm(prestige ~ education + income + women, data = Prestige)\ndf_p &lt;- data.frame(income = Prestige$income, resid = resid(mod_p))\nggplot(df_p, aes(income, resid)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_smooth(se = FALSE, color = \"steelblue\", method = \"loess\", formula = y ~ x) +\n  labs(title = \"Prestige: residuals vs. income\",\n       subtitle = \"Useful for understanding the data, not for choosing an estimator\")\n\n\n\n\n\n\n\n\nFor comparison, here is what FGLS would give if we had a substantive reason to believe variance is proportional to income (e.g., because occupational prestige surveys sample more respondents for common jobs):\n\n# FGLS: only if we have a theoretical reason for the variance model\nlog_e2_p &lt;- log(resid(mod_p)^2)\nmod_var_p &lt;- lm(log_e2_p ~ income, data = Prestige)\nsigma2_hat_p &lt;- exp(fitted(mod_var_p))\n\nmod_fgls_p &lt;- lm(prestige ~ education + income + women,\n                  data = Prestige, weights = 1 / sigma2_hat_p)\n\n\n# Full comparison: coefficients and standard errors\ncoefs &lt;- cbind(\n  OLS  = coef(mod_p),\n  HC2  = coef(mod_p),      # same point estimates\n  FGLS = coef(mod_fgls_p)  # different point estimates\n)\n\nses &lt;- cbind(\n  Classical = summary(mod_p)$coefficients[, 2],\n  HC2       = summary(mod_p_robust)$coefficients[, 2],\n  FGLS      = summary(mod_fgls_p)$coefficients[, 2]\n)\n\ncat(\"Point estimates:\\n\")\n\nPoint estimates:\n\nround(coefs, 4)\n\n                OLS     HC2    FGLS\n(Intercept) -6.7943 -6.7943 -6.6482\neducation    4.1866  4.1866  4.2360\nincome       0.0013  0.0013  0.0012\nwomen       -0.0089 -0.0089 -0.0132\n\ncat(\"\\nStandard errors:\\n\")\n\n\nStandard errors:\n\nround(ses, 4)\n\n            Classical    HC2   FGLS\n(Intercept)    3.2391 3.2167 3.2241\neducation      0.3887 0.4469 0.3816\nincome         0.0003 0.0004 0.0003\nwomen          0.0304 0.0355 0.0302\n\n\nThe differences here are modest. The pattern illustrates the two strategies:\n\nRobust SEs (HC2): Keep the OLS point estimates, correct only the standard errors. No assumptions about the variance structure — always valid.\nFGLS: Re-estimate \\(\\hat{\\beta}\\) using the variance structure. More efficient if your variance model is correct; potentially worse if it’s wrong.\n\nThe default for cross-sectional data is HC2. Use FGLS when you have a substantive reason to model the variance — not because a test told you to.\n\n\n\n\n\n\nTipHC2 as Default\n\n\n\nFor cross-sectional data, use lm_robust(y ~ x, se_type = \"HC2\") or vcovHC(mod, type = \"HC2\") as the default. HC2 adjusts for leverage and provides better finite-sample coverage than HC0 or HC1."
  },
  {
    "objectID": "ch05-gls.html#summary",
    "href": "ch05-gls.html#summary",
    "title": "5. Efficiency and GLS",
    "section": "10 Summary",
    "text": "10 Summary\n\n\n\n\n\n\n\n\nConcept\nMatrix formula\nR code\n\n\n\n\nSandwich variance\n\\((X'X)^{-1}X'\\Omega X(X'X)^{-1}\\)\nvcovHC(mod, type = \"HC2\")\n\n\nRobust SEs\n\\(\\sqrt{\\text{diag}(\\hat{V}_{HC})}\\)\nlm_robust(y ~ x, se_type = \"HC2\")\n\n\nRobust t-test\n—\ncoeftest(mod, vcov = vcovHC)\n\n\nWLS\n\\((X'WX)^{-1}X'Wy\\)\nlm(y ~ x, weights = w)\n\n\nGLS\n\\((X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}y\\)\nsolve(t(X) %*% Oi %*% X) %*% t(X) %*% Oi %*% y\n\n\nEigendecomposition\n\\(\\Omega = C\\Lambda C'\\)\neigen(Omega)\n\n\n\\(\\Omega^{-1/2}\\)\n\\(C\\Lambda^{-1/2}C'\\)\nC %*% diag(1/sqrt(lam)) %*% t(C)\n\n\nFGLS\nEstimate \\(\\hat{\\Omega}\\), then GLS\nlm(log(e^2) ~ z) then lm(y ~ x, weights = ...)\n\n\nBreusch-Pagan\n\\(nR^2\\) from \\(\\hat{e}^2/\\bar{\\hat{e}^2} \\sim X\\)\nbptest(mod)\n\n\nStandardized residual\n\\(\\hat{e}_i / (\\hat{\\sigma}\\sqrt{1-h_{ii}})\\)\nrstandard(mod)\n\n\nStudentized residual\n\\(\\hat{e}_i / (s_{(-i)}\\sqrt{1-h_{ii}})\\)\nrstudent(mod)\n\n\n\nKey takeaway. When you know (or can estimate) the variance structure, exploit it: WLS/GLS gives you tighter estimates by trusting precise observations more. When you don’t trust your variance model, use lm_robust() with HC2 standard errors — it’s always valid and requires no assumptions about the form of heteroskedasticity. In either case, the method of moments logic — choosing the right weight matrix — connects directly to GMM (Chapter 14)."
  },
  {
    "objectID": "ch06-small-sample.html",
    "href": "ch06-small-sample.html",
    "title": "6. Small Sample Inference",
    "section": "",
    "text": "The previous chapters derived the OLS estimator using only moment conditions — no distributional assumptions required. This chapter adds the assumption that errors are normal, which unlocks exact finite-sample distributions for \\(\\hat{\\beta}\\), \\(s^2\\), and the \\(t\\)- and \\(F\\)-statistics. We build everything computationally: write the likelihood, verify MLE = OLS, simulate the distributional results, and apply them to real data.\nQuestions this chapter answers:\nlibrary(ggplot2)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(estimatr)\nlibrary(car)\noptions(digits = 3)"
  },
  {
    "objectID": "ch06-small-sample.html#likelihood",
    "href": "ch06-small-sample.html#likelihood",
    "title": "6. Small Sample Inference",
    "section": "1 Likelihood",
    "text": "1 Likelihood\nA parametric model specifies a probability density \\(f(x \\mid \\theta)\\) for the data. The likelihood reverses the role of data and parameters: given observed data, how probable is each parameter value?\n\n1.1 Example: binomial likelihood\nSuppose we observe the number of terms served by 5 legislators: \\(x = \\{1, 0, 1, 2, 0\\}\\), each out of 2 possible terms. The binomial probability is \\(P(x_i \\mid p) = \\binom{2}{x_i} p^{x_i}(1-p)^{2-x_i}\\).\n\nx_obs &lt;- c(1, 0, 1, 2, 0)\nn_trials &lt;- 2\n\n# Likelihood as a function of p\nlik &lt;- function(p) {\n  prod(dbinom(x_obs, size = n_trials, prob = p))\n}\n\np_grid &lt;- seq(0.01, 0.99, by = 0.01)\nlik_vals &lt;- sapply(p_grid, lik)\n\ndf_lik &lt;- data.frame(p = p_grid, likelihood = lik_vals)\nggplot(df_lik, aes(p, likelihood)) +\n  geom_line(linewidth = 1) +\n  geom_vline(xintercept = p_grid[which.max(lik_vals)], linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Binomial likelihood for x = {1, 0, 1, 2, 0}\",\n       subtitle = paste(\"MLE: p̂ =\", p_grid[which.max(lik_vals)]),\n       x = \"p\", y = \"L(p | x)\")\n\n\n\n\n\n\n\n\nThe MLE is \\(\\hat{p} = \\sum x_i / (n \\cdot 2) = 4/10 = 0.4\\) — the sample proportion.\n\n\n1.2 The log-likelihood\nWe almost always work with the log-likelihood \\(\\ell(\\theta) = \\sum \\log f(x_i \\mid \\theta)\\), which turns products into sums and is easier to optimize:\n\nloglik &lt;- function(p) sum(dbinom(x_obs, size = n_trials, prob = p, log = TRUE))\n\n# Optimize\nopt &lt;- optimize(loglik, interval = c(0, 1), maximum = TRUE)\nopt$maximum\n\n[1] 0.4\n\n\n\n\n1.3 Score, Hessian, and Fisher information\nThe score is the derivative of the log-likelihood — it tells you the slope at any parameter value. At the MLE, the score equals zero:\n\n# Score for binomial: d/dp [sum(x*log(p) + (2-x)*log(1-p))]\n# = sum(x)/p - sum(2-x)/(1-p)\nscore &lt;- function(p) sum(x_obs) / p - sum(n_trials - x_obs) / (1 - p)\nscore(0.4)  # zero at the MLE\n\n[1] 0\n\n# Hessian (negative second derivative): observed information\nhessian &lt;- function(p) sum(x_obs) / p^2 + sum(n_trials - x_obs) / (1 - p)^2\nobserved_info &lt;- hessian(0.4)\n\n# Cramér-Rao bound: variance &gt;= 1 / information\nc(observed_information = observed_info,\n  CR_bound_variance    = 1 / observed_info,\n  CR_bound_se          = 1 / sqrt(observed_info))\n\nobserved_information    CR_bound_variance          CR_bound_se \n              41.667                0.024                0.155 \n\n\nThe Fisher information measures how sharply the likelihood peaks — more information means more precise estimation."
  },
  {
    "objectID": "ch06-small-sample.html#the-normal-linear-model",
    "href": "ch06-small-sample.html#the-normal-linear-model",
    "title": "6. Small Sample Inference",
    "section": "2 The normal linear model",
    "text": "2 The normal linear model\nThe classic normal regression model adds a distributional assumption to OLS:\n\\[y \\mid X \\sim N(X\\beta, \\, \\sigma^2 I)\\]\nThis gives us the likelihood:\n\\[\\mathcal{L}(\\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\!\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i'\\beta)^2\\right) \\tag{1}\\]\n\n2.1 Writing the log-likelihood in R\n\ndata(swiss)\ny &lt;- swiss$Fertility\nX &lt;- model.matrix(~ Education + Agriculture + Catholic + Infant.Mortality,\n                  data = swiss)\nn &lt;- nrow(X)\nK &lt;- ncol(X)\n\n# Log-likelihood as a function of beta and sigma^2\nnormal_loglik &lt;- function(beta, sigma2) {\n  resid &lt;- y - X %*% beta\n  -n/2 * log(2 * pi * sigma2) - sum(resid^2) / (2 * sigma2)\n}\n\n\n\n2.2 MLE = OLS\nMaximizing the normal log-likelihood with respect to \\(\\beta\\) gives \\(\\hat{\\beta}_{MLE} = (X'X)^{-1}X'y\\) — exactly OLS. For \\(\\sigma^2\\), the MLE is \\(\\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\sum \\hat{e}_i^2\\) (biased, unlike \\(s^2 = \\frac{1}{n-K}\\sum \\hat{e}_i^2\\)).\n\nTheorem 1 (MLE Equals OLS) Under the normal linear model \\(y|X \\sim N(X\\beta, \\sigma^2 I)\\), the MLE of \\(\\beta\\) is identical to the OLS estimator: \\(\\hat\\beta_{MLE} = (X'X)^{-1}X'y\\). The MLE of \\(\\sigma^2\\) is \\(\\hat\\sigma^2_{MLE} = \\frac{1}{n}\\sum\\hat{e}_i^2\\) (biased downward).\n\n\nmod &lt;- lm(Fertility ~ Education + Agriculture + Catholic + Infant.Mortality,\n          data = swiss)\n\n# OLS coefficients\nbeta_ols &lt;- coef(mod)\n\n# MLE sigma^2 (biased) vs s^2 (unbiased)\nsigma2_mle &lt;- sum(resid(mod)^2) / n\nsigma2_s2 &lt;- sum(resid(mod)^2) / (n - K)\n\nc(sigma2_MLE = sigma2_mle, s2 = sigma2_s2, sigma_R = sigma(mod))\n\nsigma2_MLE         s2    sigma_R \n     45.92      51.38       7.17 \n\n\nR’s sigma() returns \\(\\sqrt{s^2}\\), the bias-corrected RMSE.\n\n\n2.3 logLik() and model comparison\nR computes the maximized log-likelihood directly:\n\n# R's logLik uses s^2 (REML-style), but logLik.lm uses MLE sigma^2\nlogLik(mod)\n\n'log Lik.' -157 (df=6)\n\n# Verify by hand\nnormal_loglik(beta_ols, sigma2_mle)\n\n[1] -157\n\n\nThe log-likelihood is useful for comparing nested models via AIC and BIC:\n\nmod1 &lt;- lm(Fertility ~ Education, data = swiss)\nmod2 &lt;- lm(Fertility ~ Education + Agriculture, data = swiss)\nmod3 &lt;- lm(Fertility ~ Education + Agriculture + Catholic + Infant.Mortality,\n           data = swiss)\n\ndata.frame(\n  Model = c(\"Education only\", \"+ Agriculture\", \"+ Catholic + Infant.Mortality\"),\n  logLik = sapply(list(mod1, mod2, mod3), logLik),\n  AIC    = sapply(list(mod1, mod2, mod3), AIC),\n  BIC    = sapply(list(mod1, mod2, mod3), BIC),\n  K      = sapply(list(mod1, mod2, mod3), function(m) length(coef(m)))\n)\n\n                          Model logLik AIC BIC K\n1                Education only   -171 348 354 2\n2                 + Agriculture   -171 350 357 3\n3 + Catholic + Infant.Mortality   -157 325 336 5\n\n\nAIC = \\(-2\\ell + 2K\\) penalizes complexity; BIC = \\(-2\\ell + K \\log n\\) penalizes it more heavily. Lower is better for both."
  },
  {
    "objectID": "ch06-small-sample.html#scores-as-moment-conditions",
    "href": "ch06-small-sample.html#scores-as-moment-conditions",
    "title": "6. Small Sample Inference",
    "section": "3 Scores as moment conditions",
    "text": "3 Scores as moment conditions\nThe score of the normal regression model for \\(\\beta\\) is:\n\\[\\frac{\\partial}{\\partial \\beta} \\ell(\\beta, \\sigma^2) = \\frac{1}{\\sigma^2} X'(y - X\\beta)\\]\nSetting this to zero gives \\(X'(y - X\\hat{\\beta}) = 0\\) — the OLS normal equations. This means:\n\nMLE solves: score = 0\nOLS solves: \\(X'e = 0\\) (normal equations)\nMethod of moments solves: \\(\\frac{1}{n}\\sum x_i(y_i - x_i'\\hat{\\beta}) = 0\\)\n\nAll three are the same equation.\n\nDefinition 1 (Score and Fisher Information) The score is \\(S(\\theta) = \\partial \\ell / \\partial \\theta\\), the gradient of the log-likelihood. Setting \\(S(\\hat\\theta) = 0\\) defines the MLE. The Fisher information \\(\\mathcal{I}(\\theta) = -\\mathbb{E}[\\partial^2 \\ell / \\partial\\theta\\partial\\theta']\\) measures how sharply the likelihood peaks. Under correct specification, \\(\\text{Var}(\\hat\\theta) \\approx \\mathcal{I}(\\theta)^{-1}/n\\).\n\nThe likelihood-based view adds two things: (1) the information matrix tells us the variance of \\(\\hat{\\beta}\\), and (2) under correct specification, the information equals the Hessian, giving us the classical formula \\(\\sigma^2(X'X)^{-1}\\). Under misspecification, \\(\\mathcal{I} \\neq \\mathcal{H}\\), and we need the sandwich \\(\\mathcal{H}^{-1}\\mathcal{I}\\mathcal{H}^{-1}\\) — exactly the robust variance from Chapter 5.\n\n# Score at the MLE = 0 = normal equations\nscore_at_mle &lt;- t(X) %*% resid(mod)\nscore_at_mle\n\n                      [,1]\n(Intercept)       3.02e-14\nEducation         5.68e-14\nAgriculture       7.96e-13\nCatholic          9.09e-13\nInfant.Mortality -3.98e-13"
  },
  {
    "objectID": "ch06-small-sample.html#sec-exact-distributions",
    "href": "ch06-small-sample.html#sec-exact-distributions",
    "title": "6. Small Sample Inference",
    "section": "4 Exact distributions under normality",
    "text": "4 Exact distributions under normality\nThe normality assumption gives us exact finite-sample distributions — no asymptotics needed.\n\nTheorem 2 (Exact Sampling Distributions) Under \\(e|X \\sim N(0, \\sigma^2 I)\\): (i) \\(\\hat\\beta|X \\sim N(\\beta, \\sigma^2(X'X)^{-1})\\); (ii) \\((n-K)s^2/\\sigma^2 \\sim \\chi^2_{n-K}\\); (iii) \\(\\hat\\beta\\) and \\(s^2\\) are independent; (iv) \\(t_j = (\\hat\\beta_j - \\beta_j)/\\text{SE}(\\hat\\beta_j) \\sim t_{n-K}\\).\n\n\n\n\n\n\n\nNoteExact Means No Asymptotics Required\n\n\n\nThese distributions hold for any sample size \\(n\\) — no “large \\(n\\)” approximation needed. The \\(t\\)-distribution automatically accounts for the extra uncertainty from estimating \\(\\sigma^2\\), with heavier tails when \\(n - K\\) is small.\n\n\n\n4.1 \\(\\hat{\\beta}\\) is normal\nSince \\(\\hat{\\beta} = \\beta + (X'X)^{-1}X'e\\) is a linear function of the normal vector \\(e\\):\n\\[\\hat{\\beta} \\mid X \\sim N(\\beta, \\, \\sigma^2 (X'X)^{-1})\\]\n\nset.seed(1)\nB &lt;- 10000\nbeta_true &lt;- coef(mod)\nsigma_true &lt;- sigma(mod)\n\n# Simulate under the normal model using the actual Swiss X matrix\nb_sim &lt;- matrix(NA, B, K)\nfor (b in 1:B) {\n  y_sim &lt;- X %*% beta_true + rnorm(n, 0, sigma_true)\n  b_sim[b, ] &lt;- as.numeric(solve(crossprod(X)) %*% crossprod(X, y_sim))\n}\n\n# Focus on Education coefficient (column 2)\nj &lt;- 2\nse_theory &lt;- sigma_true * sqrt(solve(crossprod(X))[j, j])\n\ndf_beta &lt;- data.frame(b_hat = b_sim[, j])\nggplot(df_beta, aes(b_hat)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.5) +\n  stat_function(fun = dnorm, args = list(mean = beta_true[j], sd = se_theory),\n                color = \"red\", linewidth = 1) +\n  labs(title = \"Sampling distribution of β̂_Education\",\n       subtitle = paste(\"Simulated vs. N(β, σ²[(X'X)⁻¹]_jj), SE =\", round(se_theory, 3)),\n       x = expression(hat(beta)[Education]))\n\n\n\n\n\n\n\n\nThe histogram of simulated \\(\\hat{\\beta}\\) matches the theoretical normal density perfectly.\n\n\n4.2 \\((n-K)s^2/\\sigma^2 \\sim \\chi^2_{n-K}\\)\nThe residual sum of squares, scaled by \\(\\sigma^2\\), follows a chi-squared distribution:\n\nscaled_s2 &lt;- numeric(B)\nfor (b in 1:B) {\n  y_sim &lt;- X %*% beta_true + rnorm(n, 0, sigma_true)\n  e_hat &lt;- resid(lm(y_sim ~ X - 1))\n  scaled_s2[b] &lt;- sum(e_hat^2) / sigma_true^2\n}\n\n# Should match chi-squared(n-K)\ndf_chi &lt;- n - K\nc(simulated_mean = mean(scaled_s2), theory_mean = df_chi,\n  simulated_var  = var(scaled_s2),  theory_var  = 2 * df_chi)\n\nsimulated_mean    theory_mean  simulated_var     theory_var \n          41.9           42.0           82.8           84.0 \n\n\n\ndf_s2 &lt;- data.frame(scaled_s2 = scaled_s2)\nggplot(df_s2, aes(scaled_s2)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50, alpha = 0.5) +\n  stat_function(fun = dchisq, args = list(df = df_chi),\n                color = \"red\", linewidth = 1) +\n  labs(title = expression((n-K)*s^2/sigma^2 ~ \"follows\" ~ chi[n-K]^2),\n       subtitle = paste(\"df =\", df_chi),\n       x = expression((n-K)*s^2/sigma^2))\n\n\n\n\n\n\n\n\n\n\n4.3 Independence of \\(\\hat{\\beta}\\) and \\(s^2\\)\nA crucial fact: \\(\\hat{\\beta}\\) and \\(\\hat{e}\\) are independent under normality (because their joint covariance is zero and they are jointly normal). This is what allows us to form the \\(t\\)-statistic.\n\n# Correlation between beta_hat and s^2 across simulations\ns2_sim &lt;- numeric(B)\nfor (b in 1:B) {\n  y_sim &lt;- X %*% beta_true + rnorm(n, 0, sigma_true)\n  fit &lt;- lm(y_sim ~ X - 1)\n  s2_sim[b] &lt;- sum(resid(fit)^2) / (n - K)\n}\n\ncor(b_sim[, 2], s2_sim)\n\n[1] -0.0182\n\n\n\n\n4.4 The \\(t\\)-statistic\nSince \\(\\hat{\\beta}_j\\) is normal and \\(s^2\\) is independent chi-squared, their ratio has an exact \\(t\\)-distribution:\n\\[T = \\frac{\\hat{\\beta}_j - \\beta_j}{\\text{SE}(\\hat{\\beta}_j)} = \\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{s^2 [(X'X)^{-1}]_{jj}}} \\sim t_{n-K} \\tag{2}\\]\n\n# Compute t-statistics across simulations\nt_sim &lt;- numeric(B)\nfor (b in 1:B) {\n  y_sim &lt;- X %*% beta_true + rnorm(n, 0, sigma_true)\n  fit &lt;- lm(y_sim ~ X - 1)\n  # t-stat for Education coefficient\n  se_b &lt;- sqrt(vcov(fit)[j, j])\n  t_sim[b] &lt;- (coef(fit)[j] - beta_true[j]) / se_b\n}\n\ndf_t &lt;- data.frame(t_stat = t_sim)\nggplot(df_t, aes(t_stat)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 60, alpha = 0.5) +\n  stat_function(fun = dt, args = list(df = n - K),\n                color = \"red\", linewidth = 1) +\n  stat_function(fun = dnorm, color = \"blue\", linewidth = 0.8, linetype = \"dashed\") +\n  labs(title = expression(\"Simulated t-statistics follow \" * t[n-K]),\n       subtitle = paste(\"Red = t(\", n-K, \"), blue dashed = N(0,1). Heavier tails with small df.\"),\n       x = \"t-statistic\")\n\n\n\n\nSimulated t-statistics follow the t distribution with n-K degrees of freedom\n\n\n\n\nWith \\(n - K = 42\\) degrees of freedom, the \\(t\\)-distribution is close to the normal but has heavier tails — the extra uncertainty from estimating \\(\\sigma^2\\)."
  },
  {
    "objectID": "ch06-small-sample.html#applied-inference-with-the-swiss-data",
    "href": "ch06-small-sample.html#applied-inference-with-the-swiss-data",
    "title": "6. Small Sample Inference",
    "section": "5 Applied inference with the Swiss data",
    "text": "5 Applied inference with the Swiss data\n\n5.1 The regression table\n\nsummary(mod)\n\n\nCall:\nlm(formula = Fertility ~ Education + Agriculture + Catholic + \n    Infant.Mortality, data = swiss)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.676  -6.052   0.751   3.166  16.142 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       62.1013     9.6049    6.47  8.5e-08 ***\nEducation         -0.9803     0.1481   -6.62  5.1e-08 ***\nAgriculture       -0.1546     0.0682   -2.27   0.0286 *  \nCatholic           0.1247     0.0289    4.31  9.5e-05 ***\nInfant.Mortality   1.0784     0.3819    2.82   0.0072 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.17 on 42 degrees of freedom\nMultiple R-squared:  0.699, Adjusted R-squared:  0.671 \nF-statistic: 24.4 on 4 and 42 DF,  p-value: 1.72e-10\n\n\nEach row reports \\(\\hat{\\beta}_j\\), \\(\\text{SE}(\\hat{\\beta}_j)\\), the \\(t\\)-statistic \\(\\hat{\\beta}_j / \\text{SE}(\\hat{\\beta}_j)\\) (testing \\(H_0: \\beta_j = 0\\)), and the two-sided \\(p\\)-value.\n\n\n5.2 Reading the output\nThe Education coefficient is \\(-0.98\\) with SE \\(= 0.15\\):\n\nbeta_hat &lt;- coef(mod)[\"Education\"]\nse_hat &lt;- sqrt(vcov(mod)[\"Education\", \"Education\"])\nt_stat &lt;- beta_hat / se_hat\np_val &lt;- 2 * (1 - pt(abs(t_stat), df = n - K))\n\nc(estimate = beta_hat, se = se_hat, t = t_stat, p = p_val)\n\nestimate.Education                 se        t.Education        p.Education \n         -9.80e-01           1.48e-01          -6.62e+00           5.14e-08 \n\n\nFor each additional percentage point of post-primary education, fertility is about 1 point lower, and this is highly significant.\n\n\n5.3 Confidence intervals\nA \\(95\\%\\) confidence interval is \\(\\hat{\\beta}_j \\pm t_{n-K, 0.975} \\cdot \\text{SE}(\\hat{\\beta}_j)\\):\n\n# Critical value from t-distribution\nc_val &lt;- qt(0.975, df = n - K)\nc(critical_value = c_val, normal_approx = qnorm(0.975))\n\ncritical_value  normal_approx \n          2.02           1.96 \n\n# confint() does this automatically\nconfint(mod)\n\n                   2.5 % 97.5 %\n(Intercept)      42.7179 81.485\nEducation        -1.2792 -0.681\nAgriculture      -0.2922 -0.017\nCatholic          0.0664  0.183\nInfant.Mortality  0.3078  1.849\n\n\n\n# By hand for Education\nci_lo &lt;- beta_hat - c_val * se_hat\nci_hi &lt;- beta_hat + c_val * se_hat\nc(lower = ci_lo, upper = ci_hi)\n\nlower.Education upper.Education \n         -1.279          -0.681 \n\n\nThe correct interpretation: if we repeated this study many times, 95% of the computed intervals would contain the true \\(\\beta\\).\n\n\n5.4 Robust confidence intervals\nUnder heteroskedasticity, the \\(t\\)-distribution is only approximate. Use lm_robust() or coeftest() with sandwich SEs:\n\n# Robust SEs\nmod_r &lt;- lm_robust(Fertility ~ Education + Agriculture + Catholic + Infant.Mortality,\n                    data = swiss, se_type = \"HC2\")\n\n# Compare classical vs. robust CIs\nci_classical &lt;- confint(mod)\nci_robust &lt;- confint(mod_r)\n\ndata.frame(\n  Variable = rownames(ci_classical),\n  Classical_lo = ci_classical[, 1],\n  Classical_hi = ci_classical[, 2],\n  Robust_lo = ci_robust[, 1],\n  Robust_hi = ci_robust[, 2]\n)\n\n                         Variable Classical_lo Classical_hi Robust_lo Robust_hi\n(Intercept)           (Intercept)      42.7179       81.485   44.6132   79.5894\nEducation               Education      -1.2792       -0.681   -1.2837   -0.6769\nAgriculture           Agriculture      -0.2922       -0.017   -0.2888   -0.0204\nCatholic                 Catholic       0.0664        0.183    0.0681    0.1813\nInfant.Mortality Infant.Mortality       0.3078        1.849    0.2865    1.8704\n\n\n\n\n5.5 Visualizing confidence intervals\n\nci_df &lt;- data.frame(\n  term = rep(names(coef(mod))[-1], 2),\n  estimate = rep(coef(mod)[-1], 2),\n  lo = c(ci_classical[-1, 1], ci_robust[-1, 1]),\n  hi = c(ci_classical[-1, 2], ci_robust[-1, 2]),\n  type = rep(c(\"Classical\", \"HC2 Robust\"), each = K - 1)\n)\n\nggplot(ci_df, aes(x = estimate, y = term, color = type)) +\n  geom_point(position = position_dodge(0.3)) +\n  geom_errorbarh(aes(xmin = lo, xmax = hi),\n                 height = 0.2, position = position_dodge(0.3)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Swiss fertility: classical vs. robust confidence intervals\",\n       x = \"Coefficient estimate\", y = NULL, color = NULL)\n\nWarning: `geom_errorbarh()` was deprecated in ggplot2 4.0.0.\nℹ Please use the `orientation` argument of `geom_errorbar()` instead.\n\n\n`height` was translated to `width`."
  },
  {
    "objectID": "ch06-small-sample.html#the-f-test",
    "href": "ch06-small-sample.html#the-f-test",
    "title": "6. Small Sample Inference",
    "section": "6 The \\(F\\)-test",
    "text": "6 The \\(F\\)-test\nThe \\(t\\)-test handles one coefficient at a time. The \\(F\\)-test tests joint restrictions — for instance, “do Agriculture and Catholic together predict fertility, controlling for the other variables?”\n\n6.1 \\(F\\)-test as a likelihood ratio\nThe \\(F\\)-statistic compares the restricted (null) and unrestricted models:\n\\[F = \\frac{(\\tilde{\\sigma}^2 - \\hat{\\sigma}^2) / q}{\\hat{\\sigma}^2 / (n - K)} \\sim F_{q, \\, n-K} \\tag{3}\\]\nwhere \\(\\tilde{\\sigma}^2\\) is the residual variance from the restricted model and \\(q\\) is the number of restrictions.\n\n# Unrestricted model\nmod_full &lt;- lm(Fertility ~ Education + Agriculture + Catholic + Infant.Mortality,\n               data = swiss)\n\n# Restricted model: drop Agriculture and Catholic\nmod_restricted &lt;- lm(Fertility ~ Education + Infant.Mortality, data = swiss)\n\n# F-statistic by hand\nRSS_full &lt;- sum(resid(mod_full)^2)\nRSS_restr &lt;- sum(resid(mod_restricted)^2)\nq &lt;- 2  # number of restrictions\ndf_full &lt;- n - K\n\nF_stat &lt;- ((RSS_restr - RSS_full) / q) / (RSS_full / df_full)\np_F &lt;- 1 - pf(F_stat, q, df_full)\n\nc(F_statistic = F_stat, df1 = q, df2 = df_full, p_value = p_F)\n\nF_statistic         df1         df2     p_value \n   9.40e+00    2.00e+00    4.20e+01    4.23e-04 \n\n\n\n\n6.2 Using anova() and linearHypothesis()\n\n# anova() compares nested models\nanova(mod_restricted, mod_full)\n\nAnalysis of Variance Table\n\nModel 1: Fertility ~ Education + Infant.Mortality\nModel 2: Fertility ~ Education + Agriculture + Catholic + Infant.Mortality\n  Res.Df  RSS Df Sum of Sq   F  Pr(&gt;F)    \n1     44 3124                             \n2     42 2158  2       966 9.4 0.00042 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# linearHypothesis() tests restrictions directly\nlinearHypothesis(mod_full, c(\"Agriculture = 0\", \"Catholic = 0\"))\n\n\nLinear hypothesis test:\nAgriculture = 0\nCatholic = 0\n\nModel 1: restricted model\nModel 2: Fertility ~ Education + Agriculture + Catholic + Infant.Mortality\n\n  Res.Df  RSS Df Sum of Sq   F  Pr(&gt;F)    \n1     44 3124                             \n2     42 2158  2       966 9.4 0.00042 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n6.3 The overall \\(F\\)-test\nThe \\(F\\)-statistic at the bottom of summary(lm) tests whether all slope coefficients are jointly zero:\n\n# Overall F-test: H0: all slopes = 0\nmod_null &lt;- lm(Fertility ~ 1, data = swiss)\nanova(mod_null, mod_full)\n\nAnalysis of Variance Table\n\nModel 1: Fertility ~ 1\nModel 2: Fertility ~ Education + Agriculture + Catholic + Infant.Mortality\n  Res.Df  RSS Df Sum of Sq    F  Pr(&gt;F)    \n1     46 7178                              \n2     42 2158  4      5020 24.4 1.7e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# When q = 1, F = t^2\nt_education &lt;- summary(mod_full)$coefficients[\"Education\", \"t value\"]\nF_education &lt;- linearHypothesis(mod_full, \"Education = 0\")$F[2]\n\nc(t_squared = t_education^2, F_stat = F_education)\n\nt_squared    F_stat \n     43.8      43.8 \n\n\n\n\n6.4 Robust \\(F\\)-tests\nUnder heteroskedasticity, use linearHypothesis() with a robust covariance matrix:\n\n# Classical F-test\nlinearHypothesis(mod_full, c(\"Agriculture = 0\", \"Catholic = 0\"))\n\n\nLinear hypothesis test:\nAgriculture = 0\nCatholic = 0\n\nModel 1: restricted model\nModel 2: Fertility ~ Education + Agriculture + Catholic + Infant.Mortality\n\n  Res.Df  RSS Df Sum of Sq   F  Pr(&gt;F)    \n1     44 3124                             \n2     42 2158  2       966 9.4 0.00042 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Robust F-test (HC2)\nlinearHypothesis(mod_full, c(\"Agriculture = 0\", \"Catholic = 0\"),\n                 vcov = vcovHC(mod_full, type = \"HC2\"))\n\n\nLinear hypothesis test:\nAgriculture = 0\nCatholic = 0\n\nModel 1: restricted model\nModel 2: Fertility ~ Education + Agriculture + Catholic + Infant.Mortality\n\nNote: Coefficient covariance matrix supplied.\n\n  Res.Df Df    F  Pr(&gt;F)    \n1     44                    \n2     42  2 10.3 0.00023 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "ch06-small-sample.html#prediction-intervals",
    "href": "ch06-small-sample.html#prediction-intervals",
    "title": "6. Small Sample Inference",
    "section": "7 Prediction intervals",
    "text": "7 Prediction intervals\nA confidence interval for \\(E[y \\mid x_0]\\) captures uncertainty about the regression line. A prediction interval for a new observation \\(y_0\\) also includes the error variance \\(\\sigma^2\\):\n\\[\\hat{y}_0 \\pm t_{n-K, 1-\\alpha/2} \\cdot \\sqrt{s^2 (1 + x_0'(X'X)^{-1}x_0)}\\]\n\n# Predict fertility for a district with median characteristics\nnew_data &lt;- data.frame(\n  Education  = median(swiss$Education),\n  Agriculture = median(swiss$Agriculture),\n  Catholic   = median(swiss$Catholic),\n  Infant.Mortality = median(swiss$Infant.Mortality)\n)\n\n# Confidence interval for E[y|x]\npredict(mod_full, newdata = new_data, interval = \"confidence\")\n\n   fit  lwr  upr\n1 69.4 66.6 72.1\n\n# Prediction interval for a new y\npredict(mod_full, newdata = new_data, interval = \"prediction\")\n\n   fit  lwr  upr\n1 69.4 54.6 84.1\n\n\nThe prediction interval is much wider — it accounts for the irreducible noise \\(\\sigma^2\\).\n\n# Simple model for visualization\nmod_simple &lt;- lm(Fertility ~ Education, data = swiss)\npred_grid &lt;- data.frame(Education = seq(1, 55, length.out = 100))\npred_ci &lt;- predict(mod_simple, newdata = pred_grid, interval = \"confidence\")\npred_pi &lt;- predict(mod_simple, newdata = pred_grid, interval = \"prediction\")\n\ndf_pred &lt;- data.frame(\n  Education = pred_grid$Education,\n  fit = pred_ci[, 1],\n  ci_lo = pred_ci[, 2], ci_hi = pred_ci[, 3],\n  pi_lo = pred_pi[, 2], pi_hi = pred_pi[, 3]\n)\n\nggplot() +\n  geom_ribbon(data = df_pred, aes(Education, ymin = pi_lo, ymax = pi_hi),\n              alpha = 0.15, fill = \"steelblue\") +\n  geom_ribbon(data = df_pred, aes(Education, ymin = ci_lo, ymax = ci_hi),\n              alpha = 0.3, fill = \"steelblue\") +\n  geom_line(data = df_pred, aes(Education, fit), color = \"steelblue\", linewidth = 1) +\n  geom_point(data = swiss, aes(Education, Fertility), alpha = 0.6) +\n  labs(title = \"Swiss fertility: confidence and prediction intervals\",\n       subtitle = \"Dark band = CI for E[y|x], light band = PI for new observation\",\n       y = \"Fertility\")"
  },
  {
    "objectID": "ch06-small-sample.html#when-do-exact-tests-break-down",
    "href": "ch06-small-sample.html#when-do-exact-tests-break-down",
    "title": "6. Small Sample Inference",
    "section": "8 When do exact tests break down?",
    "text": "8 When do exact tests break down?\nThe \\(t\\)- and \\(F\\)-distributions are exact only under:\n\nNormality of errors\nHomoskedasticity\nKnown, correct model specification\n\nWhat happens when normality fails? Let’s simulate with heavy-tailed errors:\n\nset.seed(42)\nB &lt;- 5000\ncover_normal &lt;- cover_t5 &lt;- logical(B)\nbeta_ed &lt;- coef(mod)[\"Education\"]\n\nfor (b in 1:B) {\n  # Normal errors\n  e_norm &lt;- rnorm(n, 0, sigma(mod))\n  y_norm &lt;- X %*% coef(mod) + e_norm\n  fit_norm &lt;- lm(y_norm ~ X - 1)\n  ci_norm &lt;- confint(fit_norm)[j, ]\n  cover_normal[b] &lt;- ci_norm[1] &lt; beta_ed & beta_ed &lt; ci_norm[2]\n\n  # t(5) errors (heavy tails, same variance)\n  e_t5 &lt;- rt(n, df = 5) * sigma(mod) / sqrt(5/3)\n  y_t5 &lt;- X %*% coef(mod) + e_t5\n  fit_t5 &lt;- lm(y_t5 ~ X - 1)\n  ci_t5 &lt;- confint(fit_t5)[j, ]\n  cover_t5[b] &lt;- ci_t5[1] &lt; beta_ed & beta_ed &lt; ci_t5[2]\n}\n\nc(normal_errors = mean(cover_normal),\n  t5_errors     = mean(cover_t5))\n\nnormal_errors     t5_errors \n        0.949         0.950 \n\n\nWith normal errors, coverage is close to 95%. With \\(t_5\\) errors (heavier tails), coverage degrades — the exact \\(t\\)-distribution no longer applies. In practice, this is where robust SEs and asymptotic approximations (Chapters 9–11) take over.\n\n\n\n\n\n\nWarningHeavy Tails Break Exact Tests\n\n\n\nThe \\(t\\)- and \\(F\\)-distributions are exact only under normality. With heavy-tailed errors (e.g., \\(t_5\\)), coverage of confidence intervals degrades noticeably. In practice, use robust SEs and asymptotic theory (Chapters 8-9) when normality is suspect. The CLT will justify this asymptotically in Chapter 8."
  },
  {
    "objectID": "ch06-small-sample.html#summary",
    "href": "ch06-small-sample.html#summary",
    "title": "6. Small Sample Inference",
    "section": "9 Summary",
    "text": "9 Summary\n\n\n\n\n\n\n\n\nConcept\nFormula\nR code\n\n\n\n\nLog-likelihood\n\\(\\ell = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum e_i^2\\)\nlogLik(mod)\n\n\nMLE of \\(\\beta\\)\n\\((X'X)^{-1}X'y\\) (= OLS)\ncoef(lm(...))\n\n\nMLE of \\(\\sigma^2\\)\n\\(\\frac{1}{n}\\sum \\hat{e}_i^2\\) (biased)\nsum(resid(mod)^2) / n\n\n\nAIC / BIC\n\\(-2\\ell + 2K\\) / \\(-2\\ell + K\\log n\\)\nAIC(mod), BIC(mod)\n\n\n\\(t\\)-statistic\n\\(\\hat{\\beta}_j / \\text{SE}(\\hat{\\beta}_j) \\sim t_{n-K}\\)\nsummary(mod)$coefficients\n\n\nConfidence interval\n\\(\\hat{\\beta}_j \\pm t_{n-K,\\,0.975} \\cdot \\text{SE}\\)\nconfint(mod)\n\n\n\\(F\\)-test\n\\(\\frac{(RSS_R - RSS_U)/q}{RSS_U/(n-K)} \\sim F_{q,n-K}\\)\nanova(mod_r, mod_u)\n\n\nJoint hypothesis\n\\(R\\beta = r\\)\nlinearHypothesis(mod, ...)\n\n\nRobust \\(F\\)-test\n—\nlinearHypothesis(mod, ..., vcov = vcovHC)\n\n\nPrediction interval\n\\(\\hat{y}_0 \\pm t \\cdot s\\sqrt{1 + x_0'(X'X)^{-1}x_0}\\)\npredict(mod, interval = \"prediction\")\n\n\n\nKey takeaway. The normal linear model gives us exact finite-sample distributions: \\(\\hat{\\beta}\\) is normal, \\(s^2\\) is chi-squared, their ratio gives \\(t\\), and nested model comparisons give \\(F\\). These are the foundation of every regression table. But they rely on normality and homoskedasticity — when those fail, use robust SEs and asymptotic theory instead."
  },
  {
    "objectID": "ch12-panel.html",
    "href": "ch12-panel.html",
    "title": "12. Panel Data",
    "section": "",
    "text": "This chapter covers panel data methods: pooled OLS, the between and within (fixed effects) estimators, difference-in-differences, clustering, and dynamic panel GMM. We use the plm package for panel estimation, panelView for visualization, and fixest for fast fixed effects with flexible standard errors.\nQuestions this chapter answers:\nlibrary(ggplot2)\nlibrary(plm)\nlibrary(panelView)\nlibrary(fixest)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(estimatr)"
  },
  {
    "objectID": "ch12-panel.html#visualizing-panel-structure-with-panelview",
    "href": "ch12-panel.html#visualizing-panel-structure-with-panelview",
    "title": "12. Panel Data",
    "section": "1 Visualizing panel structure with panelView",
    "text": "1 Visualizing panel structure with panelView\nBefore estimating anything, it pays to look at your panel. The panelView package visualizes treatment patterns, missingness, and outcome dynamics.\n\n# Load the built-in datasets\ndata(panelView)\n\n\n1.1 Treatment status plots\nThe turnout dataset tracks U.S. state voter turnout and election-day registration (EDR) policy adoption—a classic staggered treatment design (see the TWFE bias discussion for the problem under staggered adoption).\n\npanelview(turnout ~ policy_edr + policy_mail_in + policy_motor,\n          data = turnout, index = c(\"abb\", \"year\"),\n          xlab = \"Year\", ylab = \"State\",\n          main = \"Election-day registration adoption by state\")\n\nWarning in fortify(data, ...): Arguments in `...` must be used.\n✖ Problematic argument:\n• position = \"identity\"\nℹ Did you misspell an argument name?\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the panelView package.\n  Please report the issue to the authors.\n\n\nWarning: In `margin()`, the argument `t` should have length 1, not length 4.\nℹ Argument get(s) truncated to length 1.\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nℹ The deprecated feature was likely used in the panelView package.\n  Please report the issue to the authors.\n\n\n\n\n\n\n\n\n\nSorting by treatment timing reveals the staggered adoption pattern:\n\npanelview(turnout ~ policy_edr + policy_mail_in + policy_motor,\n          data = turnout, index = c(\"abb\", \"year\"),\n          by.timing = TRUE,\n          xlab = \"Year\", ylab = \"State\",\n          main = \"EDR adoption sorted by timing\")\n\nWarning in fortify(data, ...): Arguments in `...` must be used.\n✖ Problematic argument:\n• position = \"identity\"\nℹ Did you misspell an argument name?\n\n\nWarning: In `margin()`, the argument `t` should have length 1, not length 4.\nℹ Argument get(s) truncated to length 1.\n\n\n\n\n\n\n\n\n\n\n\n1.2 Missing data plots\nThe capacity dataset (country-level state capacity) has substantial missingness. panelView reveals the pattern:\n\npanelview(Capacity ~ demo + lngdp + lnpop,\n          data = capacity, index = c(\"ccode\", \"year\"),\n          type = \"miss\", axis.lab = \"off\",\n          main = \"Missing data in state capacity panel\")\n\nWarning in fortify(data, ...): Arguments in `...` must be used.\n✖ Problematic argument:\n• position = \"identity\"\nℹ Did you misspell an argument name?\n\n\nWarning: In `margin()`, the argument `t` should have length 1, not length 4.\nℹ Argument get(s) truncated to length 1.\n\n\n\n\n\n\n\n\n\n\n\n1.3 Outcome dynamics\nWe can also plot the outcome variable over time, colored by treatment status:\n\npanelview(turnout ~ policy_edr,\n          data = turnout, index = c(\"abb\", \"year\"),\n          type = \"outcome\",\n          main = \"Voter turnout by EDR status\",\n          ylab = \"Turnout (%)\", xlab = \"Year\")"
  },
  {
    "objectID": "ch12-panel.html#panel-data-basics-the-produc-dataset",
    "href": "ch12-panel.html#panel-data-basics-the-produc-dataset",
    "title": "12. Panel Data",
    "section": "2 Panel data basics: the Produc dataset",
    "text": "2 Panel data basics: the Produc dataset\nWe use the Produc dataset from the plm package: 48 U.S. states observed from 1970–1986, with gross state product, public capital, private capital, employment, and unemployment.\n\ndata(\"Produc\", package = \"plm\")\ncat(\"States:\", length(unique(Produc$state)),\n    \" Years:\", min(Produc$year), \"-\", max(Produc$year),\n    \" Obs:\", nrow(Produc), \"\\n\")\n\nStates: 48  Years: 1970 - 1986  Obs: 816 \n\nhead(Produc[, c(\"state\", \"year\", \"gsp\", \"pcap\", \"emp\", \"unemp\")])\n\n    state year   gsp     pcap    emp unemp\n1 ALABAMA 1970 28418 15032.67 1010.5   4.7\n2 ALABAMA 1971 29375 15501.94 1021.9   5.2\n3 ALABAMA 1972 31303 15972.41 1072.3   4.7\n4 ALABAMA 1973 33430 16406.26 1135.5   3.9\n5 ALABAMA 1974 33749 16762.67 1169.8   5.5\n6 ALABAMA 1975 33604 17316.26 1155.4   7.7\n\n\nWe model log gross state product as a function of log public capital, log private capital, log employment, and unemployment rate:\n\\[\\ln(\\text{gsp})_{it} = \\beta_1 \\ln(\\text{pcap})_{it} + \\beta_2 \\ln(\\text{pc})_{it} + \\beta_3 \\ln(\\text{emp})_{it} + \\beta_4 \\text{unemp}_{it} + \\alpha_i + \\varepsilon_{it}\\]\nThe question is whether \\(\\alpha_i\\) (the state-specific intercept) is correlated with the regressors.\n\n2.1 Visualizing the panel\n\n# Log GSP over time by state\nggplot(Produc, aes(x = year, y = log(gsp), group = state)) +\n  geom_line(alpha = 0.3) +\n  labs(title = \"Log gross state product over time\",\n       x = \"Year\", y = \"log(GSP)\") +\n  theme_minimal()"
  },
  {
    "objectID": "ch12-panel.html#pooled-ols",
    "href": "ch12-panel.html#pooled-ols",
    "title": "12. Panel Data",
    "section": "3 Pooled OLS",
    "text": "3 Pooled OLS\nPooled OLS ignores the panel structure entirely—it treats all \\(NT\\) observations as independent:\n\npooled &lt;- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n              data = Produc, index = c(\"state\", \"year\"),\n              model = \"pooling\")\nsummary(pooled)\n\nPooling Model\n\nCall:\nplm(formula = log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, \n    data = Produc, model = \"pooling\", index = c(\"state\", \"year\"))\n\nBalanced Panel: n = 48, T = 17, N = 816\n\nResiduals:\n       Min.     1st Qu.      Median     3rd Qu.        Max. \n-0.23176215 -0.06103699 -0.00010248  0.05085197  0.35111348 \n\nCoefficients:\n              Estimate Std. Error t-value  Pr(&gt;|t|)    \n(Intercept)  1.6433023  0.0575873 28.5359 &lt; 2.2e-16 ***\nlog(pcap)    0.1550070  0.0171538  9.0363 &lt; 2.2e-16 ***\nlog(pc)      0.3091902  0.0102720 30.1003 &lt; 2.2e-16 ***\nlog(emp)     0.5939349  0.0137475 43.2032 &lt; 2.2e-16 ***\nunemp       -0.0067330  0.0014164 -4.7537 2.363e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    849.81\nResidual Sum of Squares: 6.2942\nR-Squared:      0.99259\nAdj. R-Squared: 0.99256\nF-statistic: 27171.7 on 4 and 811 DF, p-value: &lt; 2.22e-16\n\n\nPooled OLS is consistent only if the unobserved state effects \\(\\alpha_i\\) are uncorrelated with all regressors. If wealthier states invest more in public capital (likely), pooled OLS is biased."
  },
  {
    "objectID": "ch12-panel.html#the-between-estimator",
    "href": "ch12-panel.html#the-between-estimator",
    "title": "12. Panel Data",
    "section": "4 The between estimator",
    "text": "4 The between estimator\nThe between estimator uses only cross-sectional variation—it regresses group means \\(\\bar{y}_i\\) on \\(\\bar{x}_i\\):\n\nbetween_fit &lt;- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n                   data = Produc, index = c(\"state\", \"year\"),\n                   model = \"between\")\nsummary(between_fit)\n\nOneway (individual) effect Between Model\n\nCall:\nplm(formula = log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, \n    data = Produc, model = \"between\", index = c(\"state\", \"year\"))\n\nBalanced Panel: n = 48, T = 17, N = 816\nObservations used in estimation: 48\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-0.1573844 -0.0581566 -0.0055724  0.0461581  0.2176471 \n\nCoefficients:\n              Estimate Std. Error t-value  Pr(&gt;|t|)    \n(Intercept)  1.5894444  0.2329796  6.8222 2.329e-08 ***\nlog(pcap)    0.1793651  0.0719719  2.4922   0.01663 *  \nlog(pc)      0.3019542  0.0418215  7.2201 6.187e-09 ***\nlog(emp)     0.5761274  0.0563746 10.2196 4.446e-13 ***\nunemp       -0.0038903  0.0099084 -0.3926   0.69653    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    48.875\nResidual Sum of Squares: 0.2977\nR-Squared:      0.99391\nAdj. R-Squared: 0.99334\nF-statistic: 1754.11 on 4 and 43 DF, p-value: &lt; 2.22e-16\n\n\nThe between estimator discards all within-state time variation. It is consistent only under the random effects assumption (\\(\\alpha_i \\perp X_{it}\\)). When that assumption fails, the between estimator is biased."
  },
  {
    "objectID": "ch12-panel.html#sec-within",
    "href": "ch12-panel.html#sec-within",
    "title": "12. Panel Data",
    "section": "5 The within (fixed effects) estimator",
    "text": "5 The within (fixed effects) estimator\nThe within estimator removes \\(\\alpha_i\\) by demeaning each variable within each state:\n\\[(\\ln\\text{gsp}_{it} - \\overline{\\ln\\text{gsp}}_i) = \\beta'(x_{it} - \\bar{x}_i) + (\\varepsilon_{it} - \\bar{\\varepsilon}_i) \\tag{1}\\]\n\nDefinition 1 (Within (Fixed Effects) Estimator) The within estimator demeans all variables within each unit: \\((y_{it} - \\bar{y}_i) = \\beta'(x_{it} - \\bar{x}_i) + (\\varepsilon_{it} - \\bar\\varepsilon_i)\\). It is consistent when \\(\\mathbb{E}[\\varepsilon_{it}|X_{i1}, \\ldots, X_{iT}, \\alpha_i] = 0\\), even if \\(\\alpha_i\\) is correlated with \\(X_{it}\\).\n\n\n\n\n\n\n\nNoteWithin Eliminates Time-Invariant Confounders\n\n\n\nThe within estimator removes all time-invariant unit characteristics \\(\\alpha_i\\) — both observed and unobserved. This eliminates omitted variable bias from time-invariant confounders, but it also prevents estimating effects of time-invariant regressors (e.g., region, ethnicity). Use CRE/Mundlak (Chapter 13) to recover these.\n\n\n\nwithin_fit &lt;- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n                  data = Produc, index = c(\"state\", \"year\"),\n                  model = \"within\")\nsummary(within_fit)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, \n    data = Produc, model = \"within\", index = c(\"state\", \"year\"))\n\nBalanced Panel: n = 48, T = 17, N = 816\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-0.120456 -0.023741 -0.002041  0.018144  0.174718 \n\nCoefficients:\n             Estimate  Std. Error t-value  Pr(&gt;|t|)    \nlog(pcap) -0.02614965  0.02900158 -0.9017    0.3675    \nlog(pc)    0.29200693  0.02511967 11.6246 &lt; 2.2e-16 ***\nlog(emp)   0.76815947  0.03009174 25.5273 &lt; 2.2e-16 ***\nunemp     -0.00529774  0.00098873 -5.3582 1.114e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    18.941\nResidual Sum of Squares: 1.1112\nR-Squared:      0.94134\nAdj. R-Squared: 0.93742\nF-statistic: 3064.81 on 4 and 764 DF, p-value: &lt; 2.22e-16\n\n\n\n5.1 Fixed effects by hand\nBy the FWL theorem, the within estimator is equivalent to including dummy variables for each state (LSDV), which is equivalent to the Frisch–Waugh–Lovell projection:\n\n# LSDV approach: include state dummies\nlsdv &lt;- lm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp + factor(state),\n           data = Produc)\n\n# Compare slope coefficients\ncat(\"plm within:  \", round(coef(within_fit), 4), \"\\n\")\n\nplm within:   -0.0261 0.292 0.7682 -0.0053 \n\ncat(\"LSDV slopes: \", round(coef(lsdv)[2:5], 4), \"\\n\")\n\nLSDV slopes:  -0.0261 0.292 0.7682 -0.0053 \n\n\n\n\n5.2 Manual demeaning\n\n# Demean each variable within state\ndemean &lt;- function(x, group) x - ave(x, group)\n\nProduc$lgsp_dm  &lt;- demean(log(Produc$gsp), Produc$state)\nProduc$lpcap_dm &lt;- demean(log(Produc$pcap), Produc$state)\nProduc$lpc_dm   &lt;- demean(log(Produc$pc), Produc$state)\nProduc$lemp_dm  &lt;- demean(log(Produc$emp), Produc$state)\nProduc$unemp_dm &lt;- demean(Produc$unemp, Produc$state)\n\ndm_fit &lt;- lm(lgsp_dm ~ lpcap_dm + lpc_dm + lemp_dm + unemp_dm - 1, data = Produc)\ncat(\"Manual demeaning: \", round(coef(dm_fit), 4), \"\\n\")\n\nManual demeaning:  -0.0261 0.292 0.7682 -0.0053 \n\ncat(\"plm within:       \", round(coef(within_fit), 4), \"\\n\")\n\nplm within:        -0.0261 0.292 0.7682 -0.0053 \n\n\n\n\n5.3 Comparing estimators\n\ncoef_table &lt;- data.frame(\n  Variable = names(coef(within_fit)),\n  Pooled = round(coef(pooled)[-1], 4),\n  Between = round(coef(between_fit)[-1], 4),\n  Within = round(coef(within_fit), 4)\n)\nrownames(coef_table) &lt;- NULL\ncoef_table\n\n   Variable  Pooled Between  Within\n1 log(pcap)  0.1550  0.1794 -0.0261\n2   log(pc)  0.3092  0.3020  0.2920\n3  log(emp)  0.5939  0.5761  0.7682\n4     unemp -0.0067 -0.0039 -0.0053\n\n\nThe pooled and between estimators give a large positive coefficient on public capital (pcap). The within estimator—which controls for unobserved state characteristics—gives a much smaller (or negative) coefficient. This is the classic omitted variable bias story: states with high unobserved productivity invest in more public capital and have higher GSP.\n\n\n5.4 Variance decomposition\nThe total variation in a panel decomposes into between-group and within-group components. The pooled estimator is a matrix-weighted average of the between and within estimators:\n\n# Between variation\nx_bar &lt;- aggregate(log(pcap) ~ state, data = Produc, mean)$`log(pcap)`\nbetween_var &lt;- var(x_bar)\n\n# Within variation\nwithin_var &lt;- var(demean(log(Produc$pcap), Produc$state))\n\n# Total\ntotal_var &lt;- var(log(Produc$pcap))\n\ncat(\"Between variance:\", round(between_var, 4), \"\\n\")\n\nBetween variance: 0.8945 \n\ncat(\"Within variance: \", round(within_var, 4), \"\\n\")\n\nWithin variance:  0.0116 \n\ncat(\"Total variance:  \", round(total_var, 4), \"\\n\")\n\nTotal variance:   0.8885 \n\ncat(\"Between share:   \", round(between_var / total_var * 100, 1), \"%\\n\")\n\nBetween share:    100.7 %\n\n\nWhen the between share is large (as here), pooled OLS is heavily influenced by cross-sectional differences between states, which may be confounded by omitted variables."
  },
  {
    "objectID": "ch12-panel.html#clustering-and-robust-standard-errors",
    "href": "ch12-panel.html#clustering-and-robust-standard-errors",
    "title": "12. Panel Data",
    "section": "6 Clustering and robust standard errors",
    "text": "6 Clustering and robust standard errors\nStandard errors from pooled OLS or even fixed effects may be wrong if errors are correlated within states (serial correlation) or across states (spatial correlation).\n\n6.1 Cluster-robust standard errors\n\n# Default plm standard errors (assume iid errors)\ncat(\"Default within SE:\\n\")\n\nDefault within SE:\n\nround(coef(summary(within_fit))[, \"Std. Error\"], 4)\n\nlog(pcap)   log(pc)  log(emp)     unemp \n   0.0290    0.0251    0.0301    0.0010 \n\n\n\n# Cluster-robust SE (Arellano, 1987)\ncat(\"\\nCluster-robust SE (Arellano):\\n\")\n\n\nCluster-robust SE (Arellano):\n\nround(sqrt(diag(vcovHC(within_fit, method = \"arellano\", type = \"HC1\"))), 4)\n\nlog(pcap)   log(pc)  log(emp)     unemp \n   0.0605    0.0619    0.0819    0.0025 \n\n\n\n# Newey-West SE for serial correlation\ncat(\"\\nNewey-West SE (1 lag):\\n\")\n\n\nNewey-West SE (1 lag):\n\nround(sqrt(diag(vcovNW(within_fit, maxlag = 1))), 4)\n\nlog(pcap)   log(pc)  log(emp)     unemp \n   0.0391    0.0376    0.0503    0.0013 \n\n\n\n\n\n\n\n\nWarningAlways Cluster Standard Errors in Panel Data\n\n\n\nStandard OLS errors assume independence across observations. In panel data, errors are typically correlated within units (serial correlation). Failing to cluster inflates test statistics by the Moulton factor \\(\\sqrt{1 + (T-1)\\rho}\\), which can be substantial.\n\n\nUsing fixest::feols() provides the same estimates with more flexible standard error options:\n\nfe_fixest &lt;- feols(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp | state,\n                   data = Produc, panel.id = ~state + year)\n\n# Various SE types\nsummary(fe_fixest, vcov = \"iid\")\n\nOLS estimation, Dep. Var.: log(gsp)\nObservations: 816\nFixed-effects: state: 48\nStandard-errors: IID \n           Estimate Std. Error   t value   Pr(&gt;|t|)    \nlog(pcap) -0.026150   0.029002 -0.901663 3.6752e-01    \nlog(pc)    0.292007   0.025120 11.624631  &lt; 2.2e-16 ***\nlog(emp)   0.768159   0.030092 25.527254  &lt; 2.2e-16 ***\nunemp     -0.005298   0.000989 -5.358151 1.1139e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.036902     Adj. R2: 0.998605\n                 Within R2: 0.941336\n\n\n\nsummary(fe_fixest, vcov = ~state)  # cluster by state\n\nOLS estimation, Dep. Var.: log(gsp)\nObservations: 816\nFixed-effects: state: 48\nStandard-errors: Clustered (state) \n           Estimate Std. Error   t value   Pr(&gt;|t|)    \nlog(pcap) -0.026150   0.061115 -0.427878 6.7069e-01    \nlog(pc)    0.292007   0.062550  4.668409 2.5563e-05 ***\nlog(emp)   0.768159   0.082733  9.284833 3.3204e-12 ***\nunemp     -0.005298   0.002528 -2.095241 4.1564e-02 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.036902     Adj. R2: 0.998605\n                 Within R2: 0.941336\n\n\n\n# Driscoll-Kraay SE (cross-sectional + serial dependence)\nsummary(fe_fixest, vcov = \"DK\")\n\nOLS estimation, Dep. Var.: log(gsp)\nObservations: 816\nFixed-effects: state: 48\nStandard-errors: Driscoll-Kraay (L=2) \n           Estimate Std. Error   t value   Pr(&gt;|t|)    \nlog(pcap) -0.026150   0.061260 -0.426864 6.7517e-01    \nlog(pc)    0.292007   0.062641  4.661581 2.6059e-04 ***\nlog(emp)   0.768159   0.088195  8.709819 1.8097e-07 ***\nunemp     -0.005298   0.001588 -3.337117 4.1793e-03 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.036902     Adj. R2: 0.998605\n                 Within R2: 0.941336\n\n\n\n\n6.2 The Moulton factor\nWhy do clustered SEs matter? When errors are correlated within clusters, standard SEs are too small by a factor that grows with cluster size and intra-cluster correlation \\(\\rho\\):\n\\[\\text{Moulton factor} \\approx \\sqrt{1 + (n_g - 1)\\rho}\\]\n\n# Estimate intra-state error correlation from within-model residuals\ne_within &lt;- residuals(within_fit)\npdat &lt;- pdata.frame(Produc, index = c(\"state\", \"year\"))\nstate_ids &lt;- as.numeric(pdat$state)\n\n# Compute average within-state residual correlation\nstates &lt;- unique(state_ids)\ncors &lt;- numeric(length(states))\nfor (j in seq_along(states)) {\n  e_j &lt;- e_within[state_ids == states[j]]\n  if (length(e_j) &gt; 1) {\n    pair_cors &lt;- cor(e_j[-length(e_j)], e_j[-1])\n    cors[j] &lt;- pair_cors\n  }\n}\nrho &lt;- mean(cors, na.rm = TRUE)\nT_per_state &lt;- nrow(Produc) / length(unique(Produc$state))\nmoulton &lt;- sqrt(1 + (T_per_state - 1) * rho)\ncat(\"Intra-state correlation (rho):\", round(rho, 3), \"\\n\")\n\nIntra-state correlation (rho): 0.697 \n\ncat(\"Average T per state:\", T_per_state, \"\\n\")\n\nAverage T per state: 17 \n\ncat(\"Moulton factor:\", round(moulton, 2), \"\\n\")\n\nMoulton factor: 3.49 \n\n\nStandard errors should be inflated by roughly this factor when clustering is ignored."
  },
  {
    "objectID": "ch12-panel.html#sec-did",
    "href": "ch12-panel.html#sec-did",
    "title": "12. Panel Data",
    "section": "7 Difference-in-differences",
    "text": "7 Difference-in-differences\nDifference-in-differences (DiD) is a panel method for estimating causal effects when treatment is assigned to some units at some time. The canonical \\(2 \\times 2\\) DiD model is:\n\\[Y_{it} = \\alpha + \\beta \\cdot \\text{Group}_i + \\gamma \\cdot \\text{Post}_t + \\delta \\cdot (\\text{Group}_i \\times \\text{Post}_t) + \\varepsilon_{it} \\tag{2}\\]\n\nTheorem 1 (Difference-in-Differences) The DiD estimator \\(\\hat\\delta = (\\bar{Y}_{1,\\text{post}} - \\bar{Y}_{1,\\text{pre}}) - (\\bar{Y}_{0,\\text{post}} - \\bar{Y}_{0,\\text{pre}})\\) identifies the ATT under the parallel trends assumption: absent treatment, treated and control groups would have followed the same trajectory.\n\nwhere \\(\\delta\\) is the treatment effect, identified under the parallel trends assumption.\n\n7.1 Simulated DiD example\n\nset.seed(42)\nN &lt;- 100  # units\nT_periods &lt;- 10\ntreat_time &lt;- 6  # treatment starts at t = 6\ntreated_units &lt;- 1:(N/2)\n\ndid_data &lt;- expand.grid(unit = 1:N, time = 1:T_periods)\ndid_data$treated &lt;- as.integer(did_data$unit %in% treated_units)\ndid_data$post &lt;- as.integer(did_data$time &gt;= treat_time)\n\n# Generate outcomes with parallel pre-trends\nalpha_i &lt;- rep(rnorm(N, sd = 2), each = T_periods)  # unit effects\ngamma_t &lt;- rep(0.3 * (1:T_periods), times = N)       # common trend\ndelta_true &lt;- 1.5\n\ndid_data$y &lt;- 5 + alpha_i + gamma_t +\n  delta_true * did_data$treated * did_data$post +\n  rnorm(N * T_periods, sd = 1)\n\n\n# Group means over time\ngroup_means &lt;- aggregate(y ~ time + treated, data = did_data, mean)\ngroup_means$group &lt;- factor(group_means$treated, labels = c(\"Control\", \"Treated\"))\n\nggplot(group_means, aes(x = time, y = y, color = group)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  geom_vline(xintercept = treat_time - 0.5, linetype = \"dashed\") +\n  annotate(\"text\", x = treat_time - 0.3, y = max(group_means$y),\n           label = \"Treatment\", hjust = 1, size = 3) +\n  labs(title = \"Difference-in-differences\",\n       x = \"Time\", y = \"Outcome\", color = \"Group\") +\n  theme_minimal()\n\n\n\n\nDifference-in-differences: group means over time with treatment onset\n\n\n\n\n\n\n7.2 DiD estimation\n\n# Method 1: OLS with interaction\ndid_ols &lt;- lm(y ~ treated * post, data = did_data)\ncat(\"DiD coefficient (OLS interaction):\", round(coef(did_ols)[\"treated:post\"], 3),\n    \" (true:\", delta_true, \")\\n\")\n\nDiD coefficient (OLS interaction): 0.483  (true: 1.5 )\n\n\n\n# Method 2: Two-way fixed effects (unit + time FE)\ndid_twfe &lt;- feols(y ~ treated:post | unit + time, data = did_data)\ncat(\"DiD coefficient (TWFE):\", round(coef(did_twfe), 3), \"\\n\")\n\nDiD coefficient (TWFE): 0.483 \n\n\n\n# With cluster-robust SEs (cluster on unit)\nsummary(did_twfe, vcov = ~unit)\n\nOLS estimation, Dep. Var.: y\nObservations: 1,000\nFixed-effects: unit: 100,  time: 10\nStandard-errors: Clustered (unit) \n             Estimate Std. Error t value Pr(&gt;|t|)    \ntreated:post 0.483065   0.245027 1.97148  0.05146 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 2.11024     Adj. R2: 0.237228\n                Within R2: 0.003264\n\n\n\n\n7.3 Visualizing the DiD with panelView\n\n# Treatment pattern\ndid_data$D &lt;- did_data$treated * did_data$post\npanelview(y ~ D, data = did_data, index = c(\"unit\", \"time\"),\n          type = \"treat\",\n          main = \"Treatment pattern in DiD design\")\n\nWarning in fortify(data, ...): Arguments in `...` must be used.\n✖ Problematic argument:\n• position = \"identity\"\nℹ Did you misspell an argument name?\n\n\nWarning: In `margin()`, the argument `t` should have length 1, not length 4.\nℹ Argument get(s) truncated to length 1."
  },
  {
    "objectID": "ch12-panel.html#random-effects",
    "href": "ch12-panel.html#random-effects",
    "title": "12. Panel Data",
    "section": "8 Random effects",
    "text": "8 Random effects\nThe random effects (RE) estimator assumes \\(\\alpha_i \\perp X_{it}\\) and estimates a GLS model with the composite error \\(\\nu_{it} = \\alpha_i + \\varepsilon_{it}\\). The RE estimator is a weighted average of the between and within estimators:\n\\[\\hat{\\beta}_{RE} = \\lambda \\hat{\\beta}_{W} + (1 - \\lambda) \\hat{\\beta}_{B}\\]\nwhere \\(\\lambda\\) depends on the ratio of within to between variance.\n\nre_fit &lt;- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n              data = Produc, index = c(\"state\", \"year\"),\n              model = \"random\")\nsummary(re_fit)\n\nOneway (individual) effect Random Effect Model \n   (Swamy-Arora's transformation)\n\nCall:\nplm(formula = log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp, \n    data = Produc, model = \"random\", index = c(\"state\", \"year\"))\n\nBalanced Panel: n = 48, T = 17, N = 816\n\nEffects:\n                   var  std.dev share\nidiosyncratic 0.001454 0.038137 0.175\nindividual    0.006838 0.082691 0.825\ntheta: 0.8888\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-0.1067230 -0.0245520 -0.0023694  0.0217333  0.1996307 \n\nCoefficients:\n               Estimate  Std. Error z-value  Pr(&gt;|z|)    \n(Intercept)  2.13541100  0.13346149 16.0002 &lt; 2.2e-16 ***\nlog(pcap)    0.00443859  0.02341732  0.1895    0.8497    \nlog(pc)      0.31054843  0.01980475 15.6805 &lt; 2.2e-16 ***\nlog(emp)     0.72967053  0.02492022 29.2803 &lt; 2.2e-16 ***\nunemp       -0.00617247  0.00090728 -6.8033 1.023e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    29.209\nResidual Sum of Squares: 1.1879\nR-Squared:      0.95933\nAdj. R-Squared: 0.95913\nChisq: 19131.1 on 4 DF, p-value: &lt; 2.22e-16\n\n\n\n8.1 Hausman test: FE vs RE\nThe Hausman test compares FE and RE. Under \\(H_0\\) (RE is consistent), both estimators are consistent but RE is efficient. Under \\(H_1\\), only FE is consistent.\n\nphtest(within_fit, re_fit)\n\n\n    Hausman Test\n\ndata:  log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp\nchisq = 9.5254, df = 4, p-value = 0.04923\nalternative hypothesis: one model is inconsistent\n\n\nA small p-value means we reject the RE assumption—the fixed effects are correlated with the regressors, so FE is preferred.\n\n\n8.2 Mundlak’s correlated random effects\nMundlak (1978) showed that we can relax the RE assumption by including the group means \\(\\bar{x}_i\\) as additional regressors. This “correlated random effects” (CRE) estimator gives the same slope coefficients as FE while also estimating effects of time-invariant variables:\n\n# Compute state means of the regressors\nProduc$lpcap_mean &lt;- ave(log(Produc$pcap), Produc$state)\nProduc$lpc_mean   &lt;- ave(log(Produc$pc), Produc$state)\nProduc$lemp_mean  &lt;- ave(log(Produc$emp), Produc$state)\nProduc$unemp_mean &lt;- ave(Produc$unemp, Produc$state)\n\n# CRE = RE + group means\ncre_fit &lt;- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp +\n                 lpcap_mean + lpc_mean + lemp_mean + unemp_mean,\n               data = Produc, index = c(\"state\", \"year\"),\n               model = \"random\")\n\n# Compare CRE slopes to FE slopes\ncat(\"CRE slopes: \", round(coef(cre_fit)[2:5], 4), \"\\n\")\n\nCRE slopes:  -0.0261 0.292 0.7682 -0.0053 \n\ncat(\"FE slopes:  \", round(coef(within_fit), 4), \"\\n\")\n\nFE slopes:   -0.0261 0.292 0.7682 -0.0053 \n\n\nThe slope coefficients on the time-varying regressors match the FE estimates, confirming the Mundlak equivalence. The coefficients on the group means capture the between-group relationships that FE removes."
  },
  {
    "objectID": "ch12-panel.html#dynamic-panels-and-arellanobond-gmm",
    "href": "ch12-panel.html#dynamic-panels-and-arellanobond-gmm",
    "title": "12. Panel Data",
    "section": "9 Dynamic panels and Arellano–Bond GMM",
    "text": "9 Dynamic panels and Arellano–Bond GMM\nWhen the model includes a lagged dependent variable, fixed effects estimation is inconsistent (Nickell bias, \\(O(1/T)\\)). Arellano and Bond (1991) proposed first-differencing the equation and using lagged levels as instruments:\n\\[\\Delta y_{it} = \\rho \\Delta y_{it-1} + \\Delta x_{it}'\\beta + \\Delta \\varepsilon_{it}\\]\nSince \\(\\text{Cov}(y_{is}, \\Delta\\varepsilon_{it}) = 0\\) for \\(s \\leq t-2\\), lagged levels are valid instruments. This creates a growing set of moment conditions—naturally suited to GMM.\n\n9.1 The EmplUK dataset\nWe use the EmplUK dataset: 140 UK firms observed over 1976–1984, with employment, wages, capital, and output.\n\ndata(\"EmplUK\", package = \"plm\")\ncat(\"Firms:\", length(unique(EmplUK$firm)),\n    \" Years:\", min(EmplUK$year), \"-\", max(EmplUK$year),\n    \" Obs:\", nrow(EmplUK), \"\\n\")\n\nFirms: 140  Years: 1976 - 1984  Obs: 1031 \n\nhead(EmplUK)\n\n  firm year sector   emp    wage capital   output\n1    1 1977      7 5.041 13.1516  0.5894  95.7072\n2    1 1978      7 5.600 12.3018  0.6318  97.3569\n3    1 1979      7 5.015 12.8395  0.6771  99.6083\n4    1 1980      7 4.715 13.8039  0.6171 100.5501\n5    1 1981      7 4.093 14.2897  0.5076  99.5581\n6    1 1982      7 3.166 14.8681  0.4229  98.6151\n\n\n\n# Visualize the panel structure\npanelview(emp ~ wage, data = EmplUK,\n          index = c(\"firm\", \"year\"),\n          type = \"miss\", axis.lab = \"off\",\n          main = \"EmplUK panel balance\")\n\nWarning in fortify(data, ...): Arguments in `...` must be used.\n✖ Problematic argument:\n• position = \"identity\"\nℹ Did you misspell an argument name?\n\n\nWarning: In `margin()`, the argument `t` should have length 1, not length 4.\nℹ Argument get(s) truncated to length 1.\n\n\n\n\n\n\n\n\n\n\n\n9.2 Nickell bias: why FE fails with lagged DV\n\nset.seed(123)\nN_nick &lt;- 100; T_nick &lt;- 8\nrho_true &lt;- 0.5\n\n# Simulate dynamic panel\nnick_data &lt;- data.frame()\nfor (i in 1:N_nick) {\n  alpha_i &lt;- rnorm(1, sd = 1)\n  y &lt;- numeric(T_nick)\n  y[1] &lt;- alpha_i / (1 - rho_true) + rnorm(1)\n  for (t in 2:T_nick) {\n    y[t] &lt;- rho_true * y[t-1] + alpha_i + rnorm(1)\n  }\n  nick_data &lt;- rbind(nick_data,\n    data.frame(id = i, time = 1:T_nick, y = y))\n}\n\n# FE estimate of rho (biased downward)\nnick_data$y_lag &lt;- ave(nick_data$y, nick_data$id,\n                       FUN = function(x) c(NA, x[-length(x)]))\nfe_nick &lt;- plm(y ~ y_lag, data = nick_data, index = c(\"id\", \"time\"),\n               model = \"within\", na.action = na.omit)\ncat(\"True rho:\", rho_true, \"\\n\")\n\nTrue rho: 0.5 \n\ncat(\"FE estimate:\", round(coef(fe_nick), 4), \" (Nickell bias)\\n\")\n\nFE estimate: 0.2179  (Nickell bias)\n\ncat(\"Expected bias ~ -1/T =\", round(-1/T_nick, 4), \"\\n\")\n\nExpected bias ~ -1/T = -0.125 \n\n\nThe FE estimator is biased downward by approximately \\(-1/T\\) when \\(T\\) is small.\n\nDefinition 2 (Nickell Bias) In a dynamic panel \\(y_{it} = \\rho y_{it-1} + \\alpha_i + \\varepsilon_{it}\\), fixed effects estimation is inconsistent with bias \\(\\approx -(\\rho + 1)/(T - 1)\\). This arises because demeaning creates mechanical correlation between \\(\\tilde{y}_{it-1}\\) and \\(\\tilde\\varepsilon_{it}\\). The bias is severe when \\(T\\) is small.\n\n\n\n9.3 Arellano–Bond difference GMM\nThe pgmm() function in plm implements Arellano–Bond (difference GMM) and Blundell–Bond (system GMM).\nThe formula syntax uses | to separate regressors from GMM instruments:\ny ~ regressors | GMM_instruments | standard_instruments\n\n# Arellano-Bond: Table 4, Column (b) from Arellano & Bond (1991)\nab_fit &lt;- pgmm(\n  log(emp) ~ lag(log(emp), 1:2) + lag(log(wage), 0:1)\n            + log(capital) + lag(log(output), 0:1)\n            | lag(log(emp), 2:99),\n  data = EmplUK,\n  effect = \"twoways\",\n  model = \"twosteps\",\n  transformation = \"d\"  # difference GMM\n)\n\nsummary(ab_fit, robust = TRUE)\n\nTwoways effects Two-steps model Difference GMM \n\nCall:\npgmm(formula = log(emp) ~ lag(log(emp), 1:2) + lag(log(wage), \n    0:1) + log(capital) + lag(log(output), 0:1) | lag(log(emp), \n    2:99), data = EmplUK, effect = \"twoways\", model = \"twosteps\", \n    transformation = \"d\")\n\nUnbalanced Panel: n = 140, T = 7-9, N = 1031\n\nNumber of Observations Used: 611\nResiduals:\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.6190677 -0.0255683  0.0000000 -0.0001339  0.0332013  0.6410272 \n\nCoefficients:\n                        Estimate Std. Error z-value  Pr(&gt;|z|)    \nlag(log(emp), 1:2)1     0.474151   0.185398  2.5575 0.0105437 *  \nlag(log(emp), 1:2)2    -0.052967   0.051749 -1.0235 0.3060506    \nlag(log(wage), 0:1)0   -0.513205   0.145565 -3.5256 0.0004225 ***\nlag(log(wage), 0:1)1    0.224640   0.141950  1.5825 0.1135279    \nlog(capital)            0.292723   0.062627  4.6741 2.953e-06 ***\nlag(log(output), 0:1)0  0.609775   0.156263  3.9022 9.530e-05 ***\nlag(log(output), 0:1)1 -0.446373   0.217302 -2.0542 0.0399605 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSargan test: chisq(25) = 30.11247 (p-value = 0.22011)\nAutocorrelation test (1): normal = -1.53845 (p-value = 0.12394)\nAutocorrelation test (2): normal = -0.2796829 (p-value = 0.77972)\nWald test for coefficients: chisq(7) = 142.0353 (p-value = &lt; 2.22e-16)\nWald test for time dummies: chisq(6) = 16.97046 (p-value = 0.0093924)\n\n\n\n\n9.4 Diagnostic tests\nTwo diagnostics are essential for dynamic panel GMM:\n\nSargan/Hansen J-test: Are the overidentifying restrictions satisfied?\nAR(2) test: Is there second-order serial correlation in the differenced residuals? (AR(1) is expected due to differencing; AR(2) would invalidate lag-2 instruments.)\n\n\n# Sargan test of overidentifying restrictions\nsargan(ab_fit)\n\n\n    Sargan test\n\ndata:  log(emp) ~ lag(log(emp), 1:2) + lag(log(wage), 0:1) + log(capital) +  ...\nchisq = 30.112, df = 25, p-value = 0.2201\nalternative hypothesis: overidentifying restrictions not valid\n\n\n\n# Serial correlation tests\nmtest(ab_fit, order = 1)  # AR(1): expected to reject\n\n\n    Arellano-Bond autocorrelation test of degree 1\n\ndata:  log(emp) ~ lag(log(emp), 1:2) + lag(log(wage), 0:1) + log(capital) +  ...\nnormal = -2.4278, p-value = 0.01519\nalternative hypothesis: autocorrelation present\n\nmtest(ab_fit, order = 2)  # AR(2): should not reject\n\n\n    Arellano-Bond autocorrelation test of degree 2\n\ndata:  log(emp) ~ lag(log(emp), 1:2) + lag(log(wage), 0:1) + log(capital) +  ...\nnormal = -0.33254, p-value = 0.7395\nalternative hypothesis: autocorrelation present\n\n\nAR(1) in the differenced residuals is expected and mechanical. The key test is AR(2): failing to reject means our instruments (lags 2+) are valid.\n\n\n9.5 Blundell–Bond system GMM\nWhen the dependent variable is highly persistent (\\(\\rho\\) close to 1), lagged levels become weak instruments for first differences. Blundell and Bond (1998) augment the moment conditions with level equations, using lagged differences as instruments:\n\n# System GMM: use transformation = \"ld\" (level + difference)\nbb_fit &lt;- pgmm(\n  log(emp) ~ lag(log(emp), 1) + lag(log(wage), 0:1)\n            + lag(log(capital), 0:1)\n            | lag(log(emp), 2:99) + lag(log(wage), 2:99)\n              + lag(log(capital), 2:99),\n  data = EmplUK,\n  effect = \"twoways\",\n  model = \"twosteps\",\n  transformation = \"ld\"  # system GMM\n)\n\nsummary(bb_fit, robust = TRUE)\n\nTwoways effects Two-steps model System GMM \n\nCall:\npgmm(formula = log(emp) ~ lag(log(emp), 1) + lag(log(wage), 0:1) + \n    lag(log(capital), 0:1) | lag(log(emp), 2:99) + lag(log(wage), \n    2:99) + lag(log(capital), 2:99), data = EmplUK, effect = \"twoways\", \n    model = \"twosteps\", transformation = \"ld\")\n\nUnbalanced Panel: n = 140, T = 7-9, N = 1031\n\nNumber of Observations Used: 1642\nResiduals:\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.7528842 -0.0361094  0.0000000  0.0006606  0.0469500  0.6008115 \n\nCoefficients:\n                         Estimate Std. Error z-value  Pr(&gt;|z|)    \nlag(log(emp), 1)         0.932214   0.026859 34.7072 &lt; 2.2e-16 ***\nlag(log(wage), 0:1)0    -0.634477   0.118758 -5.3426 9.163e-08 ***\nlag(log(wage), 0:1)1     0.494669   0.131783  3.7537 0.0001743 ***\nlag(log(capital), 0:1)0  0.485261   0.060427  8.0305 9.705e-16 ***\nlag(log(capital), 0:1)1 -0.423223   0.064445 -6.5672 5.127e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSargan test: chisq(100) = 110.7009 (p-value = 0.21828)\nAutocorrelation test (1): normal = -6.456154 (p-value = 1.074e-10)\nAutocorrelation test (2): normal = -0.259282 (p-value = 0.79542)\nWald test for coefficients: chisq(5) = 11221.9 (p-value = &lt; 2.22e-16)\nWald test for time dummies: chisq(7) = 13.73376 (p-value = 0.056124)\n\n\n\nsargan(bb_fit)\n\n\n    Sargan test\n\ndata:  log(emp) ~ lag(log(emp), 1) + lag(log(wage), 0:1) + lag(log(capital),  ...\nchisq = 110.7, df = 100, p-value = 0.2183\nalternative hypothesis: overidentifying restrictions not valid\n\nmtest(bb_fit, order = 2)\n\n\n    Arellano-Bond autocorrelation test of degree 2\n\ndata:  log(emp) ~ lag(log(emp), 1) + lag(log(wage), 0:1) + lag(log(capital),  ...\nnormal = -0.26286, p-value = 0.7927\nalternative hypothesis: autocorrelation present\n\n\n\n\n9.6 Instrument proliferation\nA practical concern with pgmm() is that the number of GMM instruments grows quadratically with \\(T\\). With \\(T = 9\\) periods, using lag(y, 2:99) generates up to \\((T-1)(T-2)/2 = 28\\) instruments from a single variable. Too many instruments can overfit the endogenous variable.\nThe collapse = TRUE option reduces the instrument count:\n\n# Collapsed instrument matrix\nab_collapse &lt;- pgmm(\n  log(emp) ~ lag(log(emp), 1:2) + lag(log(wage), 0:1)\n            + log(capital) + lag(log(output), 0:1)\n            | lag(log(emp), 2:99),\n  data = EmplUK,\n  effect = \"twoways\",\n  model = \"twosteps\",\n  transformation = \"d\",\n  collapse = TRUE\n)\n\nsummary(ab_collapse, robust = TRUE)\n\nTwoways effects Two-steps model Difference GMM \n\nCall:\npgmm(formula = log(emp) ~ lag(log(emp), 1:2) + lag(log(wage), \n    0:1) + log(capital) + lag(log(output), 0:1) | lag(log(emp), \n    2:99), data = EmplUK, effect = \"twoways\", model = \"twosteps\", \n    collapse = TRUE, transformation = \"d\")\n\nUnbalanced Panel: n = 140, T = 7-9, N = 1031\n\nNumber of Observations Used: 611\nResiduals:\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.8455637 -0.0326605  0.0000000 -0.0003799  0.0312841  0.7010278 \n\nCoefficients:\n                        Estimate Std. Error z-value Pr(&gt;|z|)   \nlag(log(emp), 1:2)1     0.853895   0.562348  1.5184 0.128902   \nlag(log(emp), 1:2)2    -0.169886   0.123293 -1.3779 0.168232   \nlag(log(wage), 0:1)0   -0.533119   0.245948 -2.1676 0.030189 * \nlag(log(wage), 0:1)1    0.352516   0.432846  0.8144 0.415408   \nlog(capital)            0.271707   0.089921  3.0216 0.002514 **\nlag(log(output), 0:1)0  0.612855   0.242289  2.5294 0.011424 * \nlag(log(output), 0:1)1 -0.682550   0.612311 -1.1147 0.264974   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSargan test: chisq(5) = 11.62681 (p-value = 0.040275)\nAutocorrelation test (1): normal = -1.290551 (p-value = 0.19686)\nAutocorrelation test (2): normal = 0.4482577 (p-value = 0.65397)\nWald test for coefficients: chisq(7) = 134.788 (p-value = &lt; 2.22e-16)\nWald test for time dummies: chisq(6) = 11.91947 (p-value = 0.06379)\n\n\n\n# Compare coefficient on lag(log(emp), 1) across specifications\ncat(\"Full instruments:     \", round(coef(ab_fit)[\"lag(log(emp), 1:2)1\"], 4), \"\\n\")\n\nFull instruments:      0.4742 \n\ncat(\"Collapsed instruments:\", round(coef(ab_collapse)[\"lag(log(emp), 1:2)1\"], 4), \"\\n\")\n\nCollapsed instruments: 0.8539"
  },
  {
    "objectID": "ch12-panel.html#panel-estimators-as-moment-conditions",
    "href": "ch12-panel.html#panel-estimators-as-moment-conditions",
    "title": "12. Panel Data",
    "section": "10 Panel estimators as moment conditions",
    "text": "10 Panel estimators as moment conditions\nEvery panel estimator we’ve covered is a GMM estimator with specific moment conditions:\n\n\n\n\n\n\n\n\nEstimator\nMoment condition\nInstruments\n\n\n\n\nPooled OLS\n\\(\\mathbb{E}[X_{it} \\varepsilon_{it}] = 0\\)\n\\(X_{it}\\)\n\n\nWithin (FE)\n\\(\\mathbb{E}[\\tilde{X}_{it} \\tilde{\\varepsilon}_{it}] = 0\\)\nDemeaned \\(\\tilde{X}_{it}\\)\n\n\nBetween\n\\(\\mathbb{E}[\\bar{X}_i \\bar{\\varepsilon}_i] = 0\\)\nGroup means \\(\\bar{X}_i\\)\n\n\nRandom effects\n\\(\\mathbb{E}[X_{it} \\nu_{it}] = 0\\), \\(\\mathbb{E}[\\bar{X}_i \\nu_{it}] = 0\\)\nBoth \\(X_{it}\\) and \\(\\bar{X}_i\\)\n\n\nArellano–Bond\n\\(\\mathbb{E}[y_{is} \\Delta\\varepsilon_{it}] = 0,\\ s \\leq t-2\\)\nLagged levels\n\n\nBlundell–Bond\nAbove + \\(\\mathbb{E}[\\Delta y_{is} \\varepsilon_{it}] = 0,\\ s \\leq t-1\\)\n+ Lagged differences\n\n\n\nThe progression from OLS to system GMM mirrors the course arc: each step adds moment conditions and addresses a new identification challenge."
  },
  {
    "objectID": "ch12-panel.html#choosing-a-panel-estimator",
    "href": "ch12-panel.html#choosing-a-panel-estimator",
    "title": "12. Panel Data",
    "section": "11 Choosing a panel estimator",
    "text": "11 Choosing a panel estimator\n\n\n\nPanel Estimator Decision Guide:\n================================\n1. Is alpha_i correlated with X_it?\n   - No  --&gt; Random Effects (more efficient)\n   - Yes --&gt; Fixed Effects\n   - Unsure --&gt; Run Hausman test\n\n2. Do you have a lagged dependent variable?\n   - No  --&gt; Standard FE is fine\n   - Yes --&gt; FE is biased (Nickell bias)\n           --&gt; Use Arellano-Bond or System GMM\n\n3. Is the DV highly persistent (rho near 1)?\n   - No  --&gt; Arellano-Bond (difference GMM)\n   - Yes --&gt; Blundell-Bond (system GMM)\n\n4. Diagnostics:\n   - Always report cluster-robust SEs\n   - For GMM: check Sargan test + AR(2) test\n   - Watch for instrument proliferation"
  },
  {
    "objectID": "ch12-panel.html#summary",
    "href": "ch12-panel.html#summary",
    "title": "12. Panel Data",
    "section": "12 Summary",
    "text": "12 Summary\n\npanelView visualizes treatment adoption, missingness, and outcome dynamics before estimation.\nPooled OLS ignores unit effects; between uses only cross-sectional variation; within (FE) uses only time variation within units.\nThe within estimator removes time-invariant confounders but cannot estimate effects of time-invariant regressors.\nCluster-robust SEs (Arellano, Newey–West, Driscoll–Kraay) account for within-cluster error dependence. The Moulton factor quantifies the standard error inflation from ignoring clustering.\nMundlak’s CRE gives FE slopes while retaining the ability to estimate effects of time-invariant variables.\nHausman test compares FE and RE; rejection means \\(\\alpha_i\\) is correlated with \\(X_{it}\\).\nArellano–Bond and Blundell–Bond use GMM for dynamic panels with lagged dependent variables, overcoming Nickell bias.\nDiagnostic tests: Sargan/Hansen J-test for overidentifying restrictions; AR(2) test for instrument validity.\nAll panel estimators are special cases of GMM with different moment conditions."
  },
  {
    "objectID": "ch13-fixed-effects.html",
    "href": "ch13-fixed-effects.html",
    "title": "13. Fixed Effects and Modern DiD",
    "section": "",
    "text": "This chapter covers the fixed vs. random effects choice, correlated random effects (Mundlak), dynamic panel GMM, and modern difference-in-differences methods that address the problems with two-way fixed effects under staggered treatment adoption. We use plm for panel estimation, did for Callaway–Sant’Anna group-time ATTs, and fect for counterfactual imputation estimators.\nQuestions this chapter answers:\nlibrary(ggplot2)\nlibrary(plm)\nlibrary(fixest)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(did)\nlibrary(fect)"
  },
  {
    "objectID": "ch13-fixed-effects.html#random-effects-as-gls",
    "href": "ch13-fixed-effects.html#random-effects-as-gls",
    "title": "13. Fixed Effects and Modern DiD",
    "section": "1 Random effects as GLS",
    "text": "1 Random effects as GLS\nIn Chapter 12 we estimated fixed effects by demeaning within each unit. The random effects (RE) estimator instead treats \\(\\alpha_i\\) as part of a composite error \\(\\nu_{it} = \\alpha_i + \\varepsilon_{it}\\) and applies GLS to exploit the known correlation structure:\n\\[\\text{Var}(\\nu_i) = \\sigma_\\varepsilon^2 I_T + \\sigma_\\alpha^2 \\iota\\iota' = \\Omega \\tag{1}\\]\n\ndata(\"Produc\", package = \"plm\")\npdata &lt;- pdata.frame(Produc, index = c(\"state\", \"year\"))\n\n\n# Fixed effects\nfe_fit &lt;- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n              data = pdata, model = \"within\")\n\n# Random effects (GLS with error components)\nre_fit &lt;- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n              data = pdata, model = \"random\")\n\n# Compare slope coefficients\ncoef_comp &lt;- data.frame(\n  Variable = names(coef(fe_fit)),\n  FE = round(coef(fe_fit), 4),\n  RE = round(coef(re_fit)[-1], 4)\n)\nrownames(coef_comp) &lt;- NULL\ncoef_comp\n\n   Variable      FE      RE\n1 log(pcap) -0.0261  0.0044\n2   log(pc)  0.2920  0.3105\n3  log(emp)  0.7682  0.7297\n4     unemp -0.0053 -0.0062\n\n\nRE is more efficient than FE when the random effects assumption (\\(\\alpha_i \\perp X_{it}\\)) holds, because it uses both within-unit and between-unit variation. But if \\(\\alpha_i\\) is correlated with the regressors, RE is inconsistent.\n\n1.1 Variance components\nThe RE estimator decomposes total variance into unit-level (\\(\\sigma_\\alpha^2\\)) and idiosyncratic (\\(\\sigma_\\varepsilon^2\\)) components:\n\n# Extract variance components from plm\nercomp &lt;- ercomp(re_fit)\ncat(\"sigma_alpha (unit):\", round(sqrt(ercomp$sigma2[\"id\"]), 4), \"\\n\")\n\nsigma_alpha (unit): 0.0827 \n\ncat(\"sigma_eps (idios):\", round(sqrt(ercomp$sigma2[\"idios\"]), 4), \"\\n\")\n\nsigma_eps (idios): 0.0381 \n\ncat(\"theta (shrinkage):\", round(ercomp$theta, 4), \"\\n\")\n\ntheta (shrinkage): 0.8888 \n\n\nThe shrinkage parameter \\(\\theta\\) controls how much RE pulls toward the within estimator versus the between estimator. When \\(\\theta\\) is close to 1, RE is close to FE; when \\(\\theta\\) is close to 0, RE is close to pooled OLS.\n\n\n1.2 Partial pooling: RE as a weighted average\n\n# Between estimator\nbe_fit &lt;- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp,\n              data = pdata, model = \"between\")\n\ncat(\"Between: \", round(coef(be_fit)[\"log(pcap)\"], 4), \"\\n\")\n\nBetween:  0.1794 \n\ncat(\"Within:  \", round(coef(fe_fit)[\"log(pcap)\"], 4), \"\\n\")\n\nWithin:   -0.0261 \n\ncat(\"RE:      \", round(coef(re_fit)[\"log(pcap)\"], 4), \"\\n\")\n\nRE:       0.0044 \n\ncat(\"(RE is between the within and between estimates)\\n\")\n\n(RE is between the within and between estimates)"
  },
  {
    "objectID": "ch13-fixed-effects.html#the-hausman-test",
    "href": "ch13-fixed-effects.html#the-hausman-test",
    "title": "13. Fixed Effects and Modern DiD",
    "section": "2 The Hausman test",
    "text": "2 The Hausman test\nThe Hausman test compares FE (consistent under both \\(H_0\\) and \\(H_1\\)) with RE (efficient under \\(H_0\\) but inconsistent under \\(H_1\\)):\n\\[H = (\\hat\\beta_{FE} - \\hat\\beta_{RE})' [\\text{Var}(\\hat\\beta_{FE}) - \\text{Var}(\\hat\\beta_{RE})]^{-1} (\\hat\\beta_{FE} - \\hat\\beta_{RE}) \\xrightarrow{d} \\chi^2_K \\tag{2}\\]\n\nTheorem 1 (Hausman Test) The Hausman statistic \\(H = (\\hat\\beta_{FE} - \\hat\\beta_{RE})'[\\text{Var}(\\hat\\beta_{FE}) - \\text{Var}(\\hat\\beta_{RE})]^{-1}(\\hat\\beta_{FE} - \\hat\\beta_{RE}) \\xrightarrow{d} \\chi^2_K\\) tests whether FE and RE estimates differ systematically. Rejection means \\(\\alpha_i\\) is correlated with \\(X_{it}\\), favoring FE.\n\n\nphtest(fe_fit, re_fit)\n\n\n    Hausman Test\n\ndata:  log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp\nchisq = 9.5254, df = 4, p-value = 0.04923\nalternative hypothesis: one model is inconsistent\n\n\nA small p-value rejects the RE assumption—state fixed effects are correlated with the regressors, so FE is preferred.\nCaveat: Using the Hausman test to choose between FE and RE creates a pretest estimator with distorted coverage. The research design should determine the estimator; report the Hausman test as a diagnostic. The Hausman test can also be viewed as a GMM overidentification test."
  },
  {
    "objectID": "ch13-fixed-effects.html#sec-mundlak",
    "href": "ch13-fixed-effects.html#sec-mundlak",
    "title": "13. Fixed Effects and Modern DiD",
    "section": "3 Correlated random effects (Mundlak)",
    "text": "3 Correlated random effects (Mundlak)\nMundlak (1978) proposed a middle ground: include the group means \\(\\bar{x}_i\\) as additional regressors in the RE model. This “soaks up” the correlation between \\(\\alpha_i\\) and \\(x_{it}\\):\n\\[\\alpha_i = \\delta_0 + \\bar{x}_i' \\delta + \\zeta_i, \\quad \\mathbb{E}[\\zeta_i \\mid x_{it}, z_i] = 0\\]\n\n# Add group means of time-varying regressors\npdata$lpcap_bar &lt;- Between(log(pdata$pcap))\npdata$lpc_bar   &lt;- Between(log(pdata$pc))\npdata$lemp_bar  &lt;- Between(log(pdata$emp))\npdata$unemp_bar &lt;- Between(pdata$unemp)\n\n# CRE = RE + group means\ncre_fit &lt;- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp +\n                 lpcap_bar + lpc_bar + lemp_bar + unemp_bar,\n               data = pdata, model = \"random\")\n\n# CRE slopes = FE slopes (by FWL)\ncat(\"CRE slopes: \", round(coef(cre_fit)[2:5], 4), \"\\n\")\n\nCRE slopes:  -0.0261 0.292 0.7682 -0.0053 \n\ncat(\"FE slopes:  \", round(coef(fe_fit), 4), \"\\n\")\n\nFE slopes:   -0.0261 0.292 0.7682 -0.0053 \n\n\nThe CRE slopes on time-varying regressors match FE exactly. This follows from the FWL theorem: partialing out \\(\\bar{x}_i\\) from \\(x_{it}\\) gives the within-demeaned data.\n\nTheorem 2 (Correlated Random Effects (Mundlak)) Adding group means \\(\\bar{x}_i\\) to the RE model gives CRE: \\(y_{it} = x_{it}'\\beta + \\bar{x}_i'\\delta + \\alpha_i + \\varepsilon_{it}\\). The slope \\(\\hat\\beta_{CRE}\\) equals \\(\\hat\\beta_{FE}\\) by FWL. Testing \\(\\delta = 0\\) is equivalent to the Hausman test but works with robust SEs and allows estimation of time-invariant variable effects.\n\n\n\n\n\n\n\nNoteCRE = FE + Time-Invariant Variables\n\n\n\nThe Mundlak/CRE approach gives identical slope coefficients to FE while also allowing estimation of effects of time-invariant regressors (region, sex, ethnicity). It also provides a robust version of the Hausman test through an F-test on the group means.\n\n\n\n3.1 The Mundlak test = Hausman test\nTesting \\(H_0: \\delta = 0\\) (the group means have no additional explanatory power) is equivalent to the Hausman test:\n\n# F-test on the group means\nlibrary(car)\n\nLoading required package: carData\n\nlinearHypothesis(cre_fit,\n  c(\"lpcap_bar = 0\", \"lpc_bar = 0\", \"lemp_bar = 0\", \"unemp_bar = 0\"))\n\n\nLinear hypothesis test:\nlpcap_bar = 0\nlpc_bar = 0\nlemp_bar = 0\nunemp_bar = 0\n\nModel 1: restricted model\nModel 2: log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp + lpcap_bar + \n    lpc_bar + lemp_bar + unemp_bar\n\n  Res.Df Df  Chisq Pr(&gt;Chisq)  \n1    811                       \n2    807  4 9.7181    0.04545 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n3.2 CRE identifies effects of time-invariant variables\nA key advantage of CRE over FE: it can estimate effects of time-invariant regressors. FE sweeps them out along with \\(\\alpha_i\\).\n\n# Add a time-invariant variable: region\npdata$region_ne &lt;- as.numeric(pdata$region == \"1\")\npdata$region_s  &lt;- as.numeric(pdata$region == \"3\")\npdata$region_w  &lt;- as.numeric(pdata$region == \"4\")\n\ncre_region &lt;- plm(log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp +\n                    lpcap_bar + lpc_bar + lemp_bar + unemp_bar +\n                    region_ne + region_s + region_w,\n                  data = pdata, model = \"random\")\n\n# Region coefficients (relative to North Central)\ncat(\"Region effects (CRE):\\n\")\n\nRegion effects (CRE):\n\ncat(\"  Northeast:\", round(coef(cre_region)[\"region_ne\"], 4), \"\\n\")\n\n  Northeast: 0.1225 \n\ncat(\"  South:    \", round(coef(cre_region)[\"region_s\"], 4), \"\\n\")\n\n  South:     -0.0089 \n\ncat(\"  West:     \", round(coef(cre_region)[\"region_w\"], 4), \"\\n\")\n\n  West:      -0.102"
  },
  {
    "objectID": "ch13-fixed-effects.html#sec-twfe-bias",
    "href": "ch13-fixed-effects.html#sec-twfe-bias",
    "title": "13. Fixed Effects and Modern DiD",
    "section": "4 The problem with TWFE under staggered treatment",
    "text": "4 The problem with TWFE under staggered treatment\nStandard two-way fixed effects (TWFE) with staggered treatment can produce severely biased estimates when treatment effects are heterogeneous. TWFE implicitly compares newly-treated units to already-treated units, creating negative weights on some group-time ATTs.\n\nTheorem 3 (TWFE Bias Under Staggered Treatment) With staggered adoption and heterogeneous treatment effects, TWFE assigns negative weights to some group-time ATTs — using already-treated units as controls for newly-treated units. The resulting estimate can be far from any meaningful causal parameter, including the wrong sign.\n\n\n\n\n\n\n\nWarningTWFE Can Produce Negative Weights\n\n\n\nUnder staggered treatment with dynamic or heterogeneous effects, TWFE regression assigns negative weights to some group-time treatment effects. This means the TWFE coefficient is not a convex average of individual treatment effects — it can even have the wrong sign. Use Callaway-Sant’Anna, Sun-Abraham, or fect instead.\n\n\n\n4.1 Simulation: TWFE bias\n\nset.seed(42)\nN &lt;- 200; T_max &lt;- 10\n\n# Staggered adoption: 3 cohorts + never-treated\ncohorts &lt;- c(4, 6, 8)  # treatment start times\nsim_panel &lt;- data.frame()\n\nfor (i in 1:N) {\n  # Assign to cohort (25% each)\n  g &lt;- sample(c(cohorts, 0), 1)  # 0 = never treated\n  alpha_i &lt;- rnorm(1, sd = 2)\n\n  for (t in 1:T_max) {\n    treated &lt;- (g &gt; 0) & (t &gt;= g)\n    # Treatment effect grows with exposure AND varies by cohort\n    te &lt;- if (treated) 1.0 + 0.3 * (t - g) + 0.5 * (g == 4) else 0\n    y &lt;- 2 + alpha_i + 0.2 * t + te + rnorm(1)\n\n    sim_panel &lt;- rbind(sim_panel,\n      data.frame(id = i, time = t, y = y,\n                 treat = as.integer(treated),\n                 cohort = g,\n                 first_treat = ifelse(g == 0, 0, g)))\n  }\n}\n\n# TWFE regression\ntwfe &lt;- feols(y ~ treat | id + time, data = sim_panel)\ncat(\"TWFE estimate:\", round(coef(twfe)[\"treat\"], 3), \"\\n\")\n\nTWFE estimate: 1.417 \n\n# True average ATT among treated observations\ntrue_att &lt;- mean(sim_panel$y[sim_panel$treat == 1]) -\n  mean(2 + ave(rnorm(N), rep(1:N, each = T_max)) + 0.2 * sim_panel$time[sim_panel$treat == 1])\n\nWarning in split.default(x, g): data length is not a multiple of split variable\n\n\nWarning in split.default(seq_along(x), f, drop = drop, ...): data length is not\na multiple of split variable\n\n\nWarning in 2 + ave(rnorm(N), rep(1:N, each = T_max)) + 0.2 *\nsim_panel$time[sim_panel$treat == : longer object length is not a multiple of\nshorter object length\n\ncat(\"(The true ATT varies by cohort and exposure time -- TWFE averages with potentially negative weights)\\n\")\n\n(The true ATT varies by cohort and exposure time -- TWFE averages with potentially negative weights)"
  },
  {
    "objectID": "ch13-fixed-effects.html#callawaysantanna-group-time-atts",
    "href": "ch13-fixed-effects.html#callawaysantanna-group-time-atts",
    "title": "13. Fixed Effects and Modern DiD",
    "section": "5 Callaway–Sant’Anna: group-time ATTs",
    "text": "5 Callaway–Sant’Anna: group-time ATTs\nThe did package (Callaway & Sant’Anna, 2021) avoids the TWFE problem by estimating separate ATTs for each group-time pair, then aggregating. The mpdta dataset tracks teen employment across 500 U.S. counties, with staggered minimum wage increases.\n\ndata(mpdta)\ncat(\"Counties:\", length(unique(mpdta$countyreal)),\n    \" Years:\", paste(range(mpdta$year), collapse = \"-\"),\n    \" Treatment cohorts:\", paste(sort(unique(mpdta$first.treat[mpdta$first.treat &gt; 0])),\n                                  collapse = \", \"), \"\\n\")\n\nCounties: 500  Years: 2003-2007  Treatment cohorts: 2004, 2006, 2007 \n\nhead(mpdta)\n\n    year countyreal     lpop     lemp first.treat treat\n866 2003       8001 5.896761 8.461469        2007     1\n841 2004       8001 5.896761 8.336870        2007     1\n842 2005       8001 5.896761 8.340217        2007     1\n819 2006       8001 5.896761 8.378161        2007     1\n827 2007       8001 5.896761 8.487352        2007     1\n937 2003       8019 2.232377 4.997212        2007     1\n\n\n\n5.1 Estimating group-time ATTs\n\n# Estimate all group-time ATTs\ncs_out &lt;- att_gt(\n  yname = \"lemp\",\n  gname = \"first.treat\",\n  idname = \"countyreal\",\n  tname = \"year\",\n  xformla = ~1,              # unconditional\n  data = mpdta,\n  control_group = \"nevertreated\",\n  est_method = \"dr\"          # doubly robust (default)\n)\n\nsummary(cs_out)\n\n\nCall:\natt_gt(yname = \"lemp\", tname = \"year\", idname = \"countyreal\", \n    gname = \"first.treat\", xformla = ~1, data = mpdta, control_group = \"nevertreated\", \n    est_method = \"dr\")\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. &lt;https://doi.org/10.1016/j.jeconom.2020.12.001&gt;, &lt;https://arxiv.org/abs/1803.09015&gt; \n\nGroup-Time Average Treatment Effects:\n Group Time ATT(g,t) Std. Error [95% Simult.  Conf. Band]  \n  2004 2004  -0.0105     0.0237       -0.0747      0.0537  \n  2004 2005  -0.0704     0.0313       -0.1553      0.0145  \n  2004 2006  -0.1373     0.0362       -0.2354     -0.0391 *\n  2004 2007  -0.1008     0.0379       -0.2036      0.0020  \n  2006 2004   0.0065     0.0235       -0.0571      0.0702  \n  2006 2005  -0.0028     0.0193       -0.0552      0.0497  \n  2006 2006  -0.0046     0.0167       -0.0499      0.0407  \n  2006 2007  -0.0412     0.0195       -0.0940      0.0116  \n  2007 2004   0.0305     0.0144       -0.0084      0.0695  \n  2007 2005  -0.0027     0.0165       -0.0474      0.0419  \n  2007 2006  -0.0311     0.0181       -0.0801      0.0179  \n  2007 2007  -0.0261     0.0161       -0.0698      0.0177  \n---\nSignif. codes: `*' confidence band does not cover 0\n\nP-value for pre-test of parallel trends assumption:  0.16812\nControl Group:  Never Treated,  Anticipation Periods:  0\nEstimation Method:  Doubly Robust\n\n\nEach row is an ATT(g, t): the average treatment effect for group \\(g\\) (defined by when they were first treated) at time \\(t\\).\n\nggdid(cs_out, ylim = c(-0.3, 0.3))\n\n\n\n\n\n\n\n\n\n\n5.2 Aggregation: event study\nThe many group-time ATTs can be aggregated into an event study (dynamic effects by exposure time):\n\ncs_dyn &lt;- aggte(cs_out, type = \"dynamic\")\nsummary(cs_dyn)\n\n\nCall:\naggte(MP = cs_out, type = \"dynamic\")\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. &lt;https://doi.org/10.1016/j.jeconom.2020.12.001&gt;, &lt;https://arxiv.org/abs/1803.09015&gt; \n\n\nOverall summary of ATT's based on event-study/dynamic aggregation:  \n     ATT    Std. Error     [ 95%  Conf. Int.]  \n -0.0772        0.0221    -0.1206     -0.0338 *\n\n\nDynamic Effects:\n Event time Estimate Std. Error [95% Simult.  Conf. Band]  \n         -3   0.0305     0.0150       -0.0069      0.0679  \n         -2  -0.0006     0.0138       -0.0350      0.0338  \n         -1  -0.0245     0.0132       -0.0574      0.0085  \n          0  -0.0199     0.0115       -0.0486      0.0088  \n          1  -0.0510     0.0170       -0.0934     -0.0085 *\n          2  -0.1373     0.0375       -0.2310     -0.0435 *\n          3  -0.1008     0.0345       -0.1871     -0.0145 *\n---\nSignif. codes: `*' confidence band does not cover 0\n\nControl Group:  Never Treated,  Anticipation Periods:  0\nEstimation Method:  Doubly Robust\n\n\n\nggdid(cs_dyn, ylim = c(-0.3, 0.3))\n\n\n\n\nCallaway-Sant’Anna event study: pre-treatment estimates near zero support parallel trends\n\n\n\n\nPre-treatment estimates near zero support the parallel trends assumption. Post-treatment estimates show the dynamic treatment effect.\n\n\n5.3 Simple aggregation: overall ATT\n\ncs_simple &lt;- aggte(cs_out, type = \"simple\")\nsummary(cs_simple)\n\n\nCall:\naggte(MP = cs_out, type = \"simple\")\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. &lt;https://doi.org/10.1016/j.jeconom.2020.12.001&gt;, &lt;https://arxiv.org/abs/1803.09015&gt; \n\n\n   ATT    Std. Error     [ 95%  Conf. Int.]  \n -0.04        0.0118    -0.0632     -0.0167 *\n\n\n---\nSignif. codes: `*' confidence band does not cover 0\n\nControl Group:  Never Treated,  Anticipation Periods:  0\nEstimation Method:  Doubly Robust\n\n\n\n\n5.4 Group-specific effects\n\ncs_group &lt;- aggte(cs_out, type = \"group\")\nsummary(cs_group)\n\n\nCall:\naggte(MP = cs_out, type = \"group\")\n\nReference: Callaway, Brantly and Pedro H.C. Sant'Anna.  \"Difference-in-Differences with Multiple Time Periods.\" Journal of Econometrics, Vol. 225, No. 2, pp. 200-230, 2021. &lt;https://doi.org/10.1016/j.jeconom.2020.12.001&gt;, &lt;https://arxiv.org/abs/1803.09015&gt; \n\n\nOverall summary of ATT's based on group/cohort aggregation:  \n    ATT    Std. Error     [ 95%  Conf. Int.]  \n -0.031        0.0127    -0.0559     -0.0062 *\n\n\nGroup Effects:\n Group Estimate Std. Error [95% Simult.  Conf. Band]  \n  2004  -0.0797     0.0299       -0.1447     -0.0148 *\n  2006  -0.0229     0.0164       -0.0586      0.0128  \n  2007  -0.0261     0.0167       -0.0623      0.0101  \n---\nSignif. codes: `*' confidence band does not cover 0\n\nControl Group:  Never Treated,  Anticipation Periods:  0\nEstimation Method:  Doubly Robust\n\n\nDifferent cohorts may experience different effects—early adopters vs. late adopters of the minimum wage increase.\n\n\n5.5 With covariates\nParallel trends can be conditioned on covariates. Here we control for log population:\n\ncs_cov &lt;- att_gt(\n  yname = \"lemp\",\n  gname = \"first.treat\",\n  idname = \"countyreal\",\n  tname = \"year\",\n  xformla = ~lpop,           # conditional on log population\n  data = mpdta,\n  control_group = \"nevertreated\"\n)\n\ncs_cov_dyn &lt;- aggte(cs_cov, type = \"dynamic\")\nggdid(cs_cov_dyn, ylim = c(-0.3, 0.3))"
  },
  {
    "objectID": "ch13-fixed-effects.html#counterfactual-estimators-with-fect",
    "href": "ch13-fixed-effects.html#counterfactual-estimators-with-fect",
    "title": "13. Fixed Effects and Modern DiD",
    "section": "6 Counterfactual estimators with fect",
    "text": "6 Counterfactual estimators with fect\nThe fect package (Liu, Wang & Xu, 2024) takes a different approach: it imputes counterfactual outcomes for treated observations by fitting a model on untreated observations only, then computes treatment effects as the difference between observed and imputed outcomes.\nThe simdata dataset has 200 units over 35 periods with treatment that can switch on and off:\n\ndata(fect)\ncat(\"Units:\", length(unique(simdata$id)),\n    \" Periods:\", paste(range(simdata$time), collapse = \"-\"),\n    \" Treated obs:\", sum(simdata$D), \"\\n\")\n\nUnits: 200  Periods: 1-35  Treated obs: 1503 \n\n\n\n6.1 Two-way FE counterfactual\nThe simplest fect estimator uses two-way fixed effects to impute counterfactuals:\n\nout_fe &lt;- fect(Y ~ D + X1 + X2, data = simdata,\n               index = c(\"id\", \"time\"),\n               method = \"fe\", force = \"two-way\",\n               se = TRUE, nboots = 200,\n               parallel = FALSE)\n\n\ncat(\"ATT estimate:\", round(out_fe$est.avg[1, 1], 3), \"\\n\")\n\nATT estimate: 5.121 \n\ncat(\"SE:          \", round(out_fe$est.avg[1, 2], 3), \"\\n\")\n\nSE:           0.3 \n\ncat(\"95% CI:      [\", round(out_fe$est.avg[1, 3], 3), \",\",\n    round(out_fe$est.avg[1, 4], 3), \"]\\n\")\n\n95% CI:      [ 4.532 , 5.709 ]\n\n\n\n\n6.2 Event study (gap) plot\nThe default plot shows period-by-period ATTs relative to treatment onset:\n\nplot(out_fe, main = \"FE counterfactual: ATT by periods since treatment\")\n\n\n\n\n\n\n\n\n\n\n6.3 Interactive fixed effects (IFE)\nWhen parallel trends may not hold, interactive fixed effects allow for unit-specific responses to common latent factors:\n\\[Y_{it}(0) = \\alpha_i + \\xi_t + f_t' \\lambda_i + X_{it}'\\beta + \\varepsilon_{it}\\]\nThe number of factors \\(r\\) is selected by cross-validation:\n\nout_ife &lt;- fect(Y ~ D + X1 + X2, data = simdata,\n                index = c(\"id\", \"time\"),\n                method = \"ife\", force = \"two-way\",\n                CV = TRUE, r = c(0, 5),\n                se = TRUE, nboots = 200,\n                parallel = FALSE)\n\nCross-validating ...\n\n\nCriterion: Mean Squared Prediction Error\n\n\nInteractive fixed effects model...\n\n\nr = 0; sigma2 = 6.35460; IC = 2.21891; PC = 6.08178; MSPE = 6.89894\n\n\nr = 1; sigma2 = 4.52698; IC = 2.24325; PC = 5.26760; MSPE = 5.05627\n\n\nr = 2; sigma2 = 3.89603; IC = 2.45349; PC = 5.33953; MSPE = 4.62321\n\n\n*\n\n\nr = 3; sigma2 = 3.79056; IC = 2.78325; PC = 5.98062; MSPE = 4.96778\n\n\nr = 4; sigma2 = 3.67967; IC = 3.10762; PC = 6.56967; MSPE = 5.65206\n\n\nr = 5; sigma2 = 3.57625; IC = 3.43005; PC = 7.12886; MSPE = 6.07420\n\n\n\n r* = 2\n\n\n\ncat(\"IFE ATT:\", round(out_ife$est.avg[1, 1], 3),\n    \" (selected r =\", out_ife$r.cv, \")\\n\")\n\nIFE ATT: 3.073  (selected r = 2 )\n\n\n\nplot(out_ife, main = \"IFE counterfactual: ATT by periods since treatment\")\n\n\n\n\n\n\n\n\n\n\n6.4 Matrix completion\nMatrix completion treats the \\(N \\times T\\) matrix of untreated potential outcomes as approximately low-rank and uses nuclear norm regularization to impute missing entries—no need to specify the number of factors:\n\nout_mc &lt;- fect(Y ~ D + X1 + X2, data = simdata,\n               index = c(\"id\", \"time\"),\n               method = \"mc\", force = \"two-way\",\n               CV = TRUE,\n               se = TRUE, nboots = 200,\n               parallel = FALSE)\n\nCross-validating ...\n\n\nCriterion: Mean Squared Prediction Error\n\n\nMatrix completion method...\n\n\nlambda.norm = 1.00000; MSPE = 7.10032; MSPTATT = 0.28561; MSE = 5.80999\n\n\nlambda.norm = 0.42170; MSPE = 5.52641; MSPTATT = 0.12218; MSE = 4.15356\n\n\nlambda.norm = 0.17783; MSPE = 5.25167; MSPTATT = 0.04007; MSE = 1.57548\n\n\n*\n\n\nlambda.norm = 0.07499; MSPE = 5.47297; MSPTATT = 0.00815; MSE = 0.28590\n\n\nlambda.norm = 0.03162; MSPE = 5.76475; MSPTATT = 0.00170; MSE = 0.05133\n\n\nlambda.norm = 0.01334; MSPE = 6.68770; MSPTATT = 0.00034; MSE = 0.00914\n\n\n\n lambda.norm* = 0.177827941003892\n\n\n\n\n\n\ncat(\"MC ATT:\", round(out_mc$est.avg[1, 1], 3), \"\\n\")\n\nMC ATT: 4.262 \n\n\n\n\n6.5 Comparing methods\n\nmethods_comp &lt;- data.frame(\n  Method = c(\"FE\", \"IFE\", \"MC\"),\n  ATT = round(c(out_fe$est.avg[1, 1], out_ife$est.avg[1, 1], out_mc$est.avg[1, 1]), 3),\n  SE = round(c(out_fe$est.avg[1, 2], out_ife$est.avg[1, 2], out_mc$est.avg[1, 2]), 3),\n  CI_lower = round(c(out_fe$est.avg[1, 3], out_ife$est.avg[1, 3], out_mc$est.avg[1, 3]), 3),\n  CI_upper = round(c(out_fe$est.avg[1, 4], out_ife$est.avg[1, 4], out_mc$est.avg[1, 4]), 3)\n)\nmethods_comp\n\n  Method   ATT    SE CI_lower CI_upper\n1     FE 5.121 0.300    4.532    5.709\n2    IFE 3.073 0.282    2.521    3.626\n3     MC 4.262 0.327    3.622    4.902\n\n\n\n\n6.6 Placebo test for pre-trends\nThe placebo test checks whether there are “treatment effects” in the pre-treatment period—evidence against parallel trends:\n\nout_placebo &lt;- fect(Y ~ D + X1 + X2, data = simdata,\n                    index = c(\"id\", \"time\"),\n                    method = \"fe\", force = \"two-way\",\n                    placeboTest = TRUE,\n                    placebo.period = c(-2, 0),\n                    se = TRUE, nboots = 200,\n                    parallel = FALSE)\nplot(out_placebo, main = \"Placebo test: pre-treatment effects should be zero\")\n\n\n\n\n\n\n\n\n\n\n6.7 Equivalence test\nThe equivalence test provides a more formal assessment: are pre-treatment ATTs close enough to zero?\n\nplot(out_fe, type = \"equiv\",\n     main = \"Equivalence test for pre-treatment periods\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the fect package.\n  Please report the issue at &lt;https://github.com/xuyiqing/fect/issues&gt;."
  },
  {
    "objectID": "ch13-fixed-effects.html#matrix-completion-for-panel-data-conceptual",
    "href": "ch13-fixed-effects.html#matrix-completion-for-panel-data-conceptual",
    "title": "13. Fixed Effects and Modern DiD",
    "section": "7 Matrix completion for panel data (conceptual)",
    "text": "7 Matrix completion for panel data (conceptual)\nAthey, Bayati, Doudchenko, Imbens & Khosravi (2021) formalize the connection between DID, synthetic control, and matrix completion. The key insight is that all three are special cases of the same framework, differing in what structure they impose on the matrix of untreated potential outcomes \\(Y(0)\\):\n\n\n\n\n\n\n\n\nMethod\nAssumption on \\(Y(0)\\)\nBest when\n\n\n\n\nDID\nAdditive: \\(Y_{it}(0) = \\alpha_i + \\xi_t\\) (rank 1)\nStrong parallel trends\n\n\nSynthetic control\nHard rank constraint on columns\nFew treated units, many controls\n\n\nMatrix completion\nSoft low-rank via nuclear norm penalty\nGeneral (robust across settings)\n\n\n\nThe MCPanel package (available from GitHub: susanathey/MCPanel) implements nuclear norm minimized matrix completion. The idea is to find a low-rank matrix \\(L\\) plus unit and time effects that best approximate the observed (untreated) entries:\n\\[\\min_{L, \\gamma, \\delta} \\frac{1}{|\\mathcal{O}|} \\sum_{(i,t) \\in \\mathcal{O}} (Y_{it} - L_{it} - \\gamma_i - \\delta_t)^2 + \\lambda \\|L\\|_*\\]\nwhere \\(\\|L\\|_*\\) is the nuclear norm (sum of singular values) and \\(\\lambda\\) is chosen by cross-validation. When \\(\\lambda\\) is very large, \\(L \\to 0\\) and the method reduces to DID. When \\(\\lambda\\) is small, it approximates interactive fixed effects.\nWe can demonstrate the idea with a simple simulation:\n\nset.seed(99)\nN_mc &lt;- 30; T_mc &lt;- 20; R_true &lt;- 2\n\n# Low-rank structure\nA &lt;- matrix(rnorm(N_mc * R_true), N_mc, R_true)\nB &lt;- matrix(rnorm(T_mc * R_true), T_mc, R_true)\nL_true &lt;- A %*% t(B)\n\n# Unit and time effects\ngamma &lt;- rnorm(N_mc, sd = 0.5)\ndelta &lt;- rnorm(T_mc, sd = 0.5)\nY0 &lt;- L_true + outer(gamma, rep(1, T_mc)) + outer(rep(1, N_mc), delta) +\n  matrix(rnorm(N_mc * T_mc, sd = 0.3), N_mc, T_mc)\n\n# Treatment: units 1-10 treated from period 15 onward\ntau_true &lt;- 2.0  # constant treatment effect\nY_obs &lt;- Y0\nY_obs[1:10, 15:T_mc] &lt;- Y_obs[1:10, 15:T_mc] + tau_true\n\n# What DID would estimate\ndid_control_pre &lt;- mean(Y_obs[11:N_mc, 1:14])\ndid_control_post &lt;- mean(Y_obs[11:N_mc, 15:T_mc])\ndid_treat_pre &lt;- mean(Y_obs[1:10, 1:14])\ndid_treat_post &lt;- mean(Y_obs[1:10, 15:T_mc])\ndid_att &lt;- (did_treat_post - did_treat_pre) - (did_control_post - did_control_pre)\n\n# SVD-based imputation (simplified matrix completion)\n# Use control rows + pre-treatment treated rows to impute\nmask &lt;- matrix(1, N_mc, T_mc)\nmask[1:10, 15:T_mc] &lt;- 0  # missing = treated\n\n# Impute using low-rank SVD of observed entries\nY_masked &lt;- Y_obs * mask\nY_masked[1:10, 15:T_mc] &lt;- NA\n\n# Simple imputation: use SVD on complete rows to predict treated post\nY_control &lt;- Y_obs[11:N_mc, ]\nsvd_control &lt;- svd(Y_control)\n# Project treated pre-period onto control space\nY_treat_pre &lt;- Y_obs[1:10, 1:14]\n# Use first R_true factors\nV_pre &lt;- svd_control$v[1:14, 1:R_true]\nV_post &lt;- svd_control$v[15:T_mc, 1:R_true]\nU_hat &lt;- Y_treat_pre %*% V_pre %*% solve(t(V_pre) %*% V_pre)\nY0_hat_post &lt;- U_hat %*% t(V_post)\n\n# Adjust for level (add mean)\nmc_att &lt;- mean(Y_obs[1:10, 15:T_mc] - Y0_hat_post) -\n  mean(Y_obs[1:10, 1:14] - U_hat %*% t(V_pre))\n\ncat(\"True ATT:     \", tau_true, \"\\n\")\n\nTrue ATT:      2 \n\ncat(\"DID estimate: \", round(did_att, 3), \"\\n\")\n\nDID estimate:  1.914 \n\ncat(\"MC estimate:  \", round(mc_att, 3), \"\\n\")\n\nMC estimate:   1.733 \n\n\nWhen the data have a low-rank structure that violates simple parallel trends, matrix completion can outperform DID by exploiting the factor structure."
  },
  {
    "objectID": "ch13-fixed-effects.html#when-to-use-which-method",
    "href": "ch13-fixed-effects.html#when-to-use-which-method",
    "title": "13. Fixed Effects and Modern DiD",
    "section": "8 When to use which method",
    "text": "8 When to use which method\nThe landscape of DiD and panel methods has expanded. Here is a practical guide:\n\n\n\nModern DiD / Panel Method Guide:\n==================================\n\nSTANDARD PANEL (no staggered treatment):\n  - alpha_i correlated with X?\n    Yes --&gt; Fixed Effects or CRE (Mundlak)\n    No  --&gt; Random Effects (rare in observational data)\n  - Lagged dependent variable?\n    Yes --&gt; Arellano-Bond or System GMM (see Ch. 12)\n\nSTAGGERED DiD (treatment adopted at different times):\n  - Homogeneous treatment effects?\n    Yes --&gt; Standard TWFE is fine\n    No  --&gt; Use modern methods:\n      * Callaway-Sant'Anna (did package):\n        - Group-time ATTs, flexible aggregation\n        - Supports covariates, doubly robust estimation\n        - Requires staggered adoption (no treatment reversal)\n      * fect package:\n        - FE, IFE, or matrix completion counterfactuals\n        - Handles treatment reversal\n        - Built-in placebo and equivalence tests\n        - Also wraps other methods (CS, Sun-Abraham, etc.)\n      * MCPanel:\n        - Matrix completion (nuclear norm)\n        - Unifies DID, synthetic control, and IFE\n        - Best when outcome matrix is approximately low-rank\n\nDIAGNOSTICS:\n  - Always check pre-trends (event study / placebo test)\n  - For GMM: Sargan/J-test + AR(2) test\n  - For RE: Hausman test (as diagnostic, not decision rule)\n  - For CRE: F-test on group means = robust Hausman test"
  },
  {
    "objectID": "ch13-fixed-effects.html#summary",
    "href": "ch13-fixed-effects.html#summary",
    "title": "13. Fixed Effects and Modern DiD",
    "section": "9 Summary",
    "text": "9 Summary\n\nRandom effects assumes \\(\\alpha_i \\perp X_{it}\\) and applies GLS with the composite error structure. It is more efficient than FE when valid, but inconsistent when violated.\nThe Hausman test compares FE and RE; use it as a diagnostic, not a model-selection tool.\nCorrelated random effects (Mundlak) nests both FE and RE: it recovers FE slopes while also estimating effects of time-invariant variables. The Mundlak test (\\(\\delta = 0\\)) equals the Hausman test but works with robust SEs.\nTWFE fails under staggered treatment with heterogeneous effects due to negative weighting.\nCallaway–Sant’Anna (did package) estimates group-time ATTs and aggregates them into event studies, group-specific effects, or an overall ATT—avoiding the negative weighting problem.\nfect provides counterfactual imputation estimators: FE, interactive fixed effects, and matrix completion. It handles treatment reversal and includes built-in diagnostics.\nMatrix completion (Athey et al.) unifies DID and synthetic control as special cases of nuclear norm minimization, providing a flexible middle ground.\nAll panel estimators connect to the GMM framework: FE uses within-group moments, RE adds between-group moments, CRE augments RE to nest FE, and dynamic panels use lagged instruments."
  }
]